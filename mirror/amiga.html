<!DOCTYPE html>
<html lang="en">
	<head>
		<title>A history of the Amiga</title>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1"/>
		<link rel="icon" href="https://arstechnica.com/favicon.ico">
		<style>
			body {
			background-color: #5f6265;
			text-align: center;
			font-family: sans-serif;
			margin-top: 0;
			margin-bottom: 0;
			padding-top: 0;
			padding-bottom: 0;
			}
			.content {
			text-align: left;
			display: inline-block;
			background-color: #f0f1f2;
			padding-left: 4%;
			padding-right: 4%;
			padding-top: 2%;
			padding-bottom: 2%;
			margin-bottom: 3%;
			}
			.content * {
			padding-left: 4%;
			padding-left: 4%;
			}
			#top * {
			vertical-align: middle;
			}
			pre {
			white-space: pre-line;
			font-family: sans-serif;
			max-width: 40em;
			border-left: 4px double black;
			}
			h1 {
			max-width: 20em;
			font-family: "Georgia", "Bitstream Vera Serif", serif;
			text-decoration: underline;
			}
			h2 {
			font-family: "Georgia", "Bitstream Vera Serif", serif;
			}
			h3 {
			font-family: "Georgia", "Bitstream Vera Serif", serif;
			}
			.content img {
			max-width: 90%;
			max-height: 30em;
			}
			hr {
			border: none;
			border-top: 3px double #333;
			color: #333;
			overflow: visible;
			text-align: center;
			height: 5px;
			}
			hr:after {
			background: #f0f1f2;
			content: '§';
			padding: 0 4px;
			position: relative;
			top: -13px;
			}
			.slideshow {
			display: block;
			padding: 1%;
			margin-left: auto;
			margin-right: auto;
			}
			.slideshow img {
			display: block;
			width: 60%;
			}
			.slideshow span {
			vertical-align: middle;
			display: inline-block;
			max-width: 30%;
			}
			.slideshow i {
			font-size: 0.8em;
			max-width: 50%;
			}
			.chapter-btn {
			border: 2px solid white;
			border-radius: 10em;
			background-color: black;
			color: white;
			display: inline-block;
			padding: 1%;
			cursor: pointer;
			font-family: monospace;
			}
			.chapter-btn:hover {
			background-color: darkblue;
			}
			.chapter-btn a {
			color: white;
			text-decoration: none;
			}
			#chapter-btns-container {
			margin-top: 1%;
			text-align: center;
			}
			@media screen and (min-width: 800px) {
			.content {
			max-width: 50%;
			}
			}
		</style>
	</head>
	<body>
		<div class="content" id="1">
			<span id="top"><img src="https://arstechnica.com/favicon.ico" alt="arstechnica">&nbsp;JEREMY REIMER, 2007</span>
			<div id="chapter-btns-container">
				<div class="chapter-btn"><a href="#1">1</a></div>
				<div class="chapter-btn"><a href="#2">2</a></div>
				<div class="chapter-btn"><a href="#3">3</a></div>
				<div class="chapter-btn"><a href="#4">4</a></div>
				<div class="chapter-btn"><a href="#5">5</a></div>
				<div class="chapter-btn"><a href="#6">6</a></div>
				<div class="chapter-btn"><a href="#7">7</a></div>
				<div class="chapter-btn"><a href="#8">8</a></div>
				<div class="chapter-btn"><a href="#9">9</a></div>
				<div class="chapter-btn"><a href="#10">10</a></div>
				<div class="chapter-btn"><a href="#11">11</a></div>
				<div class="chapter-btn"><a href="#12">12</a></div>
			</div>
			<h1>A history of the Amiga, part 1: Genesis</h1>
			<p class="series-subtitle">When it first arrived, the Amiga was a dream machine...</p>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2015/07/Screen-Shot-2015-07-23-at-10.29.33-AM.png" alt="">
			<h2>Prologue: the last day</h2>
			<i>April 24, 1994</i>
			<pre>
The flag was flying at half-mast when Dave Haynie drove up to the headquarters of Commodore International for what would be the last time.

Dave had worked for Commodore at its West Chester, Pennsylvania, headquarters for eleven years as a hardware engineer. His job was to work on advanced products, like the revolutionary AAA chipset that would have again made the Amiga computer the fastest and most powerful multimedia machine available. But AAA, like most of the projects underway at Commodore, had been canceled in a series of cost-cutting measures, the most recent of which had reduced the staff of over one thousand people at the factory to less than thirty.

"Bringing your camera on the last day, eh Dave?" the receptionist asked in a resigned voice.

"Yeah, well, they can't yell at me for spreading secrets any more, can they?" he replied.

Dave took his camera on a tour of the factory, his low voice echoing through the empty hallways. "I just thought about it this morning," he said, referring to his idea to film the last moments of the company for which he had given so much of his life. "I didn't plan this."

The air conditioners droned loudly as he passed warehouse after warehouse. Two years ago these giant rooms had been filled with products. Commodore had sold $1 billion worth of computers and computer accessories that year. Today, the warehouses stood completely empty.

Dave walked upstairs and continued the tour. "This is where the chip guys worked," he said as the camera panned over empty desks. The "chip guys" were engineers designing VLSI (Very Large Scale Integration) custom microchips on advanced CAD workstations. These chips had always formed the heart of the Amiga computer. Five years later, most personal computers would include custom chips to speed up the delivery of graphics, sound, and video, but the Amiga had done so since its introduction in 1985.

"Wow, one guy is still here!" Dave said, zooming in on the workstation of Brian Rosier. "And he's actually working!" The workstation screen showed a complex line graph, the result of a simulation of a new chip design. "This is for my next job," the engineer said, smiling. Most of the technical people would not be out of work for very long.

Dave passed his own office. The camera zoomed up to an empty bottle of ale displayed proudly on a shelf. "This was for the birth of my son," he said, then panned around the rest of the desk, filled with papers and technical manuals. "I felt I had to do something," he said before he left.

"This was my workbench," he explained as the tour continued. On the desk were various Amiga computers, a Macintosh IIsi, tons of test equipment, and a large prototype circuit board.

"And this... this is Triple-A," he said, with a mixture of pride and bitterness. "I read on the 'Net that AAA didn't exist. Well, here it is!" He pointed out the memory slots, the expansion bus, and various other features.

Many of the Commodore engineers were on the Internet, back before the World Wide Web existed, when the 'Net was just text and was the exclusive domain of academics, researchers, and a few dedicated hobbyists. AAA had been the subject of hundreds of rumors, from its announcement to a series of delays and its final cancellation. While there were those who believed it had never existed, there were also others who went the other way, who endowed AAA with mythical properties, perpetually waiting in the wings for its revival and subsequent domination of the computer industry. These people would keep the faith for years, in the subsequent trying times for the Amiga after the death of its parent company. They refused to let go of the dream.

Others were more pragmatic. "Here's Dr. Mo!" Dave exclaimed, finding Greg Berlin, manager of high-end systems at Commodore International, crouched down on the floor, pulling chips out of a personal computer and placing them, one at a time, on top of the large tower case.

"Dr. Mo in pilfer mode," he said, looking up from his task. His face registered laughter, guilt, sadness, and resignation all at the same time. He sighed. "Well, I've been waiting all these years, I finally broke down and I'm doing it. I finally decided, I've been here long enough that I deserved something." He looked at the tiny, pathetic little pile, as if the supreme inequity of this trade was suddenly hitting him. "So I'm taking a couple of RAM chips," he said.
</pre>
			<h2>Introduction</h2>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amiga-1.jpg" alt="">
			<pre>
The Amiga computer was a dream given form: an inexpensive, fast, flexible multimedia computer that could do virtually anything. It handled graphics, sound, and video as easily as other computers of its time manipulated plain text. It was easily ten years ahead of its time. It was everything its designers imagined it could be, except for one crucial problem: the world was essentially unaware of its existence.

With personal computers now playing such a prominent role in modern society, it's surprising to discover that a machine with most of the features of modern PCs actually first came to light back in 1985. Almost without exception, the people who bought and used Amigas became diehard fans. Many of these people would later look back fondly on their Amiga days and lament the loss of the platform. Some would even state categorically that despite all the speed and power of modern PCs, the new machines have yet to capture the fun and the spirit of their Amiga predecessors. A few still use their Amigas, long after the equivalent mainstream personal computers of the same vintage have been relegated to the recycling bin. Amiga users, far more than any other group, were and are extremely passionate about their platform.

So if the Amiga was so great, why did so few people hear about it? The world has plenty of books about the IBM PC and its numerous clones, and even a large library about Apple Computer and the Macintosh platform. There are also many books and documentaries about the early days of the personal computing industry. A few well-known examples are the excellent book Accidental Empires (which became a PBS documentary called Triumph of the Nerds) and the seminal work Fire in the Valley (which became a TV movie on HBO entitled Pirates of Silicon Valley.)

These works tell an exciting tale about the early days of personal computing, and show us characters such as Bill Gates and Steve Jobs battling each other while they were still struggling to establish their new industry and be taken seriously by the rest of the world. They do a great job telling the story of Microsoft, IBM, and Apple, and other companies that did not survive as they did. But they mention Commodore and the Amiga rarely and in passing, if at all. Why?

When I first went looking for the corresponding story of the Amiga computer, I came up empty-handed. An exhaustive search for Amiga books came up with only a handful of old technical manuals, software how-to guides, and programming references. I couldn't believe it. Was the story so uninteresting? Was the Amiga really just a footnote in computing history, contributing nothing new and different from the other platforms?

As I began researching, I discovered the answer, and it surprised me even more than the existence of the computer itself. The story of Commodore and the Amiga was, by far, even more interesting than that of Apple or Microsoft. It is a tale of vision, of technical brilliance, dedication, and camaraderie. It is also a tale of deceit, of treachery, and of betrayal. It is a tale that has largely remained untold.

This series of articles attempts to explain what the Amiga was, what it meant to its designers and users, and why, despite its relative obscurity and early demise, it mattered so much to the computer industry. It follows some of the people whose lives were changed by their contact with the Amiga and shows what they are doing today. Finally, it looks at the small but dedicated group of people who have done what many thought was impossible and developed a new Amiga computer and operating system, ten years after the bankruptcy of Commodore. Long after most people had given up the Amiga for dead, these people have given their time, expertise and money in pursuit of this goal.

To many people, these efforts seem futile, even foolish. But to those who understand, who were there and lived through the Amiga at the height of its powers, they do not seem foolish at all.

But the story is about something else as well. More than a tale about a computer maker, this is the story about the age-old battle between mediocrity and excellence, the struggle between merely existing and trying to go beyond expectations. At many points in the story, the struggle is manifested by two sides: the hard-working, idealistic engineers driven to the bursting point and beyond to create something new and wonderful, and the incompetent and often avaricious managers and executives who end up destroying that dream. But the story goes beyond that. At its core, it is about people, not just the designers and programmers, but the users and enthusiasts, everyone whose lives were touched by the Amiga. And it is about me, because I count myself among those people, despite being over a decade too late to the party.

All these people have one thing in common. They understand the power of the dream.
</pre>
			<h2>The dream (1977-1984)</h2>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/jay-miner-1.jpg" alt="Jay Miner and his dog, Mitchy.">
			<br>
			<i>Jay Miner and his Dog, Mitchy.</i>
			<pre>
There were many people who helped to create the Amiga, but the dream itself was the creation of one man,  known as the father of the Amiga. His name was Jay Miner.

Jay was born in Prescott, Arizona on May 31, 1932. A child of the Depression, he was interested in electronics from an early age. He started university at San Diego State. By this time, the Korean War was in full swing, and Jay opted to join the Coast Guard. His education and interest worked in his favor, landing him in electronics school in Groton, Connecticut. It was here that he met his future wife, Caroline Poplawski. They were married in a quiet ceremony in 1952.

Jay's interest in electronics continued to grow, and he brought his new bride with him to California where he enrolled at the University of California-Berkeley. He completed his degree in electrical engineering in 1958. Berkeley would later become a hotbed of computer science, contributing, among other things, the TCP/IP communications protocol that would later become the standard for the entire Internet.

For the next ten years, Jay moved around from company to company, many of them startups. His desire to be involved at a fundamental level in the design process was far greater than his need for steady employment. At startups, all the traditional rules about management and procedure are typically thrown out the window. People don't worry about sticking to their job descriptions; employees on every level from intern to CEO simply do whatever work needs to be done. This type of environment suited Jay well.

Jay then landed a position at a hot young company called Atari, which had gone from nothing to worldwide success overnight with the invention of the first computerized arcade games, including the blockbuster PONG. Atari was by no means a typical company. Its founder, Nolan Bushnell, was a child of the 1960s and believed that corporations could be more than emotionless profit machines: they should be like families, helping each other to prosper in more ways than just financially. There were few rules at Atari, and it didn't matter how weird a person you were if you could do the work. (One such Atari hire was Steve Jobs, who later moved on to bigger and better things.)

The man at Atari who hired Jay Miner in the mid-1970s was Harold Lee, who became a lifelong colleague and friend. Harold once said of Jay that "he was always designing. He never stopped designing." That kind of attitude could get you far at a company like Atari. Jay wound up being the lead chip designer for a revolutionary product that would create a multibillion dollar industry: the Atari 2600, otherwise known as the Video Computer System or VCS.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/atari-2600.jpg" alt="The Atari 2600 and the game that made it famous.">
			<br>
			<i>The Atari 2600 and the game that made it famous.</i>
			<h2>Atari days</h2>
			<pre>
The generation of gamers who have been raised on Sony and Nintendo may not remember Atari, which today exists only as a logo and a brand used by a video game software company, but Atari essentially created the home video game industry as it stands today. The VCS was the first massively popular game console, and despite having incredibly primitive hardware inside, it managed to have a commercial life span far greater than any of its competitors. Much of this longevity was due to Jay Miner's brilliant design, which allowed third-party programmers to coax the underpowered machine to achieve things never dreamed of by its creators.

An example of this was Atari's Chess game. The original packaging for the VCS showed a screenshot of the machine playing chess, although its designers knew that there was no way it was powerful enough to do so. However, when someone sued Atari for misleading advertising, the programmers at Atari realized they had better try and program such a game. Clever programming made the impossible possible, something that would be seen many times on the Amiga later on in our story.

Having achieved such great success with the VCS game console, Jay's next assignment was designing Atari's first personal computer system. In 1978, personal computers had barely been invented, and the few companies that had developed them were often small, quirky organizations, barely moved out of their founder's garages. Apple (started by the aforementioned Steve Jobs and Steve Wozniak) was one of the major players, as was Tandy Radio Shack and even Commodore (we will get to the full Commodore story in a future installment).
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/atari-800.jpg" alt="An early ad for the Atari 400/800. Note the years!">
			<br>
			<i>An early ad for the Atari 400/800. Note the years!</i>
			<pre>
The computer Jay designed was released in 1979 as the Atari 400. A more powerful version, the 800, was also released with a better keyboard. At the time, most of its competitors were awkward, clunky machines, often large, heavy and temperamental, and if they created any graphics at all they were either in monochrome or, in the case of the Apple ][, limited to a palette of only eight colors. The Atari 400/800 machines had a maximum of 40 simultaneous colors, and featured custom chips to accelerate sound and graphics to the point that accurate conversions of popular arcade games became possible. Compared to an Apple ][ or a TRS-80, the Atari machine seemed to come from the future. The same thing would happen with the Amiga a few years later.

However, Atari management undermined the success of the 400/800 in several ways. Firstly, to avoid competition with the VCS, they downplayed the importance or even the existence of games for the platform, insisting that it be considered a "serious" machine. Ironically, when the company was struggling to produce a successor to the 2600, they ended up simply putting an Atari 400 in a smaller, keyboard-less case. Even worse, Atari was reticent about giving out information about how the hardware worked, thinking that such data was to be kept a trade secret, known only to internal Atari programmers. Some individuals, such as the superstar game programmer John Harris, considered this a challenge, and they managed to unlock most of the Atari's secrets by a process similar to reverse engineering. But the lack of strong third-party development for the computer doomed it to an also-ran status in the nascent industry.

After the 400 and 800 had shipped, Atari management wanted Jay to continue developing new computers. However, they insisted that he work with the same central processing unit, or CPU, that had powered the VCS and the 400/800 series. That chip, the 6502, was at the heart of many of the computers of the day. But Jay wanted to use a brand new chip that had come out of Motorola's labs, called the 68000.
</pre>
			<h3>The 68000</h3>
			<pre>
The 68000 was an engineer's dream: fast, years ahead of its time, and easy to program. But it was also expensive and required more memory chips to operate, and Atari management didn't think that expensive computers constituted a viable market. Anyone who had studied the history of electronics knew that in this industry, what was expensive now would gradually become cheaper over time, and Jay pleaded with his bosses to reconsider. They steadfastly refused.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/moto-68000.jpg" alt="The dream chip: Motorola's 68000.">
			<br>
			<i>The dream chip: Motorola's 68000.</i>
			<pre>
Atari at this time was changing, and not necessarily for the better. The company's rapid growth had resulted in a cash flow crunch, and in response Nolan Bushnell had sold the company to Warner Communications in 1978. The early spirit of family and cooperation was rapidly vanishing. The new CEO, Ray Kassar, had come from a background in clothing manufacturing and had little knowledge of the electronics industry. He managed to alienate all of Atari's VCS programmers, refusing their demands for royalty payments on the games they designed (which were at the time selling in incredible numbers) and even referred to them at one point as "prima donna towel designers." His attitude led to a large number of Atari programmers quitting the company and forming their own startups, such as the very successful Activision, started by Larry Kaplan. Larry had been Atari's very first VCS programmer.

Jay had incredible visions of the kind of computer he could create around the 68000 chip, but Atari management simply wasn't interested, so finally he gave up in disgust and left the company in early 1982. He joined Zimast, a small electronics company that made chips for pacemakers. It seemed like his dream was dead.

However, as would happen many times in the short history of this industry, forces would align to make a previously impossible dream possible. While technology was advancing rapidly, the number of people who really understood the technology remained small. These people would not be limited by the short-sighted management of large companies. They would find each other, and together, they would find a way.

It was this feeling that caused Larry Kaplan to pick up the phone and make the fateful call to Jay Miner in the middle of 1982.

Larry was enjoying the fruits of his success with Activision, yet still felt the limitations of being primarily a developer for the Atari VCS. Video games were a hot property at this time, and there was no shortage of investment money that people were willing to put into new gaming startups. A consortium out of Texas, which included an oil baron (who had also made money from sales of pacemaker chips, which was how Jay knew him) and three dentists, had approached Larry about investing seven million dollars in a new video game company.

Larry immediately phoned Jay at Zimast to ask if he would like to be involved in this new venture. The idea was to spread the development around: Larry and Activision would develop the games, Jay and Zimast would design and build the new hardware to run them, and everybody would make money. They had to quickly decide on a name for the new venture, and "Hi-Toro" was chosen because it sounded both high-tech and Texan. The company needed a management person to oversee all this development, so David Morse was recruited from his position of vice president of marketing at Tonka Toys. A small office was located in Santa Clara, California, and the three co-founders got down to the business of designing the ultimate games machine.

It was around this time that Larry Kaplan began to get cold feet about the whole idea. Jay speculated that perhaps things weren't moving fast enough for him, or maybe he was worried that the games industry was becoming too crowded, but he suddenly decided to quit the company in late 1982. It turned out that Kaplan had been given a very generous offer from Nolan Bushnell to come back to Atari, an offer that later turned out to be less than expected.

In any case, Kaplan's departure presented the fledgling venture with a problem: they had no chief engineer. While Larry was a software developer and not a true hardware engineer, he had still been in charge of engineering management for the company. The next logical choice for this position was Jay Miner.

Jay knew this was his chance. He agreed to take over the position of chief of engineering at Hi-Toro under two conditions: He had to be able to make the new video game machine use the 68000 chip, and also make it work as a computer.
</pre>
		</div>
		<div class="content" id="2">
			<h1>A history of the Amiga, part 2: The birth of Amiga</h1>
			<p class="series-subtitle">The second installment of our series on the history of the Amiga picks up with …</p>
			<h2>Born as a console, but with the heart of a computer</h2>
			<pre>
Game consoles and personal computers are not all that different on the inside. Both use a central processing unit as their main engine (the Apple ][, Commodore 64, and the Atari 400/800 all used the same 6502 CPU that powered the original Nintendo and Sega consoles). Both allow user input (keyboards and mice on computers, joysticks and game pads on consoles) and both output to a graphical display device (either a monitor or a TV). The main difference is in user interaction. Gaming consoles do one thing only—play games—whereas personal computers also allow users to write letters, balance finances and even enter their own customized programs. Computers cost more, but they also do more. It was not too much of a stretch to imagine the new Hi-Toro console being optionally expandable to a full computer.

However, the investors weren't likely to see things that way. They wanted to make money, and at the time the money in video games dwarfed the money in personal computers. Jay and his colleagues agreed that they would design the new piece of hardware to look like a games unit, with the option of expansion into a full computer cleverly hidden.

This was one of those decisions that, in retrospect, seems incredibly prescient. At the time, it was merely practical—the investors wanted a game console, the new company needed Jay Miner, and Jay wanted to design a new computer. This compromise allowed everyone to get what they wanted. But events were transpiring that would make this decision not only beneficial, but necessary for the survival of the company.
</pre>
			<h3>The video game crash</h3>
			<pre>
The great video game crash of 1983, was, like all great crashes, easy to predict after it had already happened. With sales of home consoles and video games rising exponentially, companies started to think that the potential for earning money was unlimited. Marketing executives at Atari bragged that they could "shit in a box and sell it." And inevitably, that's exactly what happened.

There were too many software companies producing too many games for the Atari VCS and other competing consoles. The quality of games began to suffer, and the technological limitations of the first generation of video game machines were starting to become insurmountable. Clever programming could only take you so far. Today, it is understood that each new generation of game consoles has a limited lifecycle, and new hardware platforms are scheduled for release just as the old ones are starting to wane. Back then, however, the industry was so new that the sinusoidal-like demand for a game platform was not understood at all. People just expected sales to keep going up forever.

Just like the dotcom bubble in the late 1990s, a point was reached where the initial enthusiasm was left behind and replaced with sheer insanity. This point can be traced precisely to the release of a new game for the Atari VCS in late 1982, timed to coincide with the release of a new blockbuster movie: E.T. The Extra Terrestrial.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/et2600.jpg" alt="The game that ended it all.">
			<br>
			<i>The game that ended it all.</i>
			<pre>
Atari paid millions of dollars for the license to make the game, but marketing executives demanded that it be developed and sent to manufacturing in six weeks. Good software is like good wine—it cannot be rushed. The game that Atari programmers managed to produce turned out to be a very nasty bottle of vinegar. It was repetitive, frustrating, and not much fun. Atari executives, however, did not realize this. They compounded their mistake by ordering the manufacture of five million cartridges, which was nearly the number of VCS consoles existing at the time. But the insanity didn't stop there. For the release of the game Pac-Man, Atari actually manufactured more cartridges than there were VCS consoles to run them!

An Atari marketing manager was actually asked about this disparity, and his response clearly expressed his total disconnect from reality. He said that people might like to buy two copies: one for home, and one for a vacation cottage!

Instead of two copies, most people decided to buy zero. Atari (and thus Warner) posted huge losses for the year and were forced to write off most of its unsold inventory of VCS cartridges. In a famous ceremony, tens of thousands of E.T., Pac-Man, and other carts were buried and bulldozed in an industrial waste dump.

The E.T. debacle was the exact moment when the bubble burst. Millions of kids around the world decided that Atari and, by extension, all console video games weren't "cool" anymore. Sales of all game systems and software plummeted. Suddenly, venture funding for new game companies vanished.

Personal computer sales, however, were still climbing steadily. Systems like the Apple ][, the Commodore 64, and even the new IBM PC were becoming more popular in the home. Parents could justify paying a little more money for a system that was educational, while the kids rejoiced in the fact that these little computers could also play games.

This set the stage for a fateful meeting. The nervous Hi-Toro investors, watching the video game market crumble before their eyes, anxiously asked Jay Miner if it might be possible to convert the new console into a full-blown personal computer. Imagine their relief as he told them he had been planning this all along!

There was only one problem remaining: the company's name. Someone had done a cursory check and found out that the name Hi-Toro was already owned by a Japanese lawnmower company. Jay wanted his new computer to both friendly and sexy. He suggested "Amiga," the Spanish word for female friend. Perhaps not coincidentally, Amiga would also come before Atari in the phone book! Jay wasn't terribly pleased with the name initially. However, as none of the other employees could think of anything better, the name stuck.

Now everything was in place. The players were set; the game was under way.

The dream was becoming a reality.
</pre>
			<h2>Early days at Amiga</h2>
			<pre>
Jay Miner once described the feeling of being involved in the young Amiga company as being like Mickey Mouse in the movie Fantasia, creating magical broomsticks to help carry buckets of water, then being unable to stop his runaway creations as they multiplied beyond control. He immediately hired four engineers to help him with the hardware design, and a chief of software design, Bob Pariseau. Bob then quickly hired four more software engineers to help him. The young company quickly became an unruly beast, devouring money at an insatiable pace. But it was necessary.

In high technology, even more so than in other industries, speed is always important, and there is never enough time. Things change so quickly that this year's hot new design looks stale and dated next year. The only way to overcome this problem is to apply massive amounts of concentrated brainpower and come up with a very clever design, then rush as quickly as possible to get the design through the initial prototype and into an actual product. Even the inelegant, unimaginative and graphically inept IBM PC, introduced in 1981, was the result of an unprecedented one-year crash building program. Not even the mighty IBM, with resources greater than those of small nations, was immune to the pressures of time.

A tiny company like Amiga had even greater problems. On top of the maddening rush to ramp up staffing and develop a new product, Jay and his team had to worry about much larger corporations and their industrial espionage teams stealing their new ideas and applying much greater resources to beat them to market. Nobody knew what Amiga, Inc. was up to, and the company's founders liked it that way. So an elaborate two-pronged attack was devised to ensure that nobody got wise to Amiga's ambitions before they were ready to show them to the world.

Firstly, the company would create a deceptive business front. This had to be something simple enough that it would not take away too many resources from the actual work, yet still deliver actual products and generate some revenue. The company decided to stick to its videogame roots and produce hardware and software add-ons for the Atari VCS. One of the first products, a collector's item today, was the Amiga Joyboard, a kind of joystick that was used by sitting or standing on top of it and leaning back and forth, left and right. The company also wrote some simple games for it that involved skiing and skateboarding. While income from these games and peripherals helped sustain the company in its early days, it was also affected by the video game crash of '83 and sales quickly dwindled.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/joyboard.jpg" alt="The Amiga Joyboard. Note the small Amiga logo at bottom.">
			<br>
			<i>The Amiga Joyboard. Note the small Amiga logo at bottom.</i>
			<pre>
This short-lived era of the young company's history had one long-lasting impact on the Amiga computer. RJ Mical, a programmer writing some of the complicated routines that would bring the Amiga to life, developed a simple game that used the Joyboard and was designed to try and help him to relax. The game was called "Zen Meditation" and the object was to try and sit absolutely still. The game was a kind of running joke in the Amiga offices, and when the time came to write the text for a serious error message for the Amiga operating system, a programmer came up with the term "Guru Meditation Error." This would remain in the operating system for years to come, until a nameless and unimaginative Commodore executive insisted on removing the Guru and making the message into "Software Failure."

The second front of deception against industrial espionage involved codenames for the powerful new custom chips the team was designing for the Amiga computer. Dave Morse decided that henceforth all these chips would be referred to by women's names. The idea was that if anyone intercepted telephone conversations between Amiga people, they would be unable to figure out that they were discussing parts of a computer. The idea of "Agnes" being temperamental or "Denise" not living up to expectations also appealed to the engineers' sense of humor. The computer itself was codenamed "Lorraine," the name of Dave's wife.

Jay Miner may have been leading the team, but the details of the new computer were hammered out at team design meetings, held in a seminar-like room that had whiteboards covering the walls. Everyone could pitch for inclusion in the machine, and the small group would have to come to a consensus about which features to include and which to leave out. Engineering is all about tradeoffs, and you can't just decide to include "the best of everything" and have it all work. Cost, speed, time to develop, and complexity are just some of the factors that must be taken into account at this crucial stage of a new computer. The way the Amiga team came to a consensus was with foam rubber baseball bats.

It isn't known who first came up with the idea, but the foam bats became an essential part of all design meetings. A person would pitch an idea, and if other engineers felt they were stupid or unnecessary, they would hit the person over the head with a bat. As Jay said, "it didn't hurt, but the humiliation of being beaten with the bat was unbearable." It was a lighthearted yet still serious approach, and it worked. Slowly the Amiga design began to take shape.
</pre>
			<h2>Hold and modify</h2>
			<pre>
Jay had always had a passion for flight simulators, and it was something that would stay with him for the rest of his life. A friend of his took him on a field trip to Link, a company that made multimillion-dollar flight simulators for the military. Jay was enthralled by the realistic sights and sounds and vowed that he would make the Amiga computer capable of playing the best flight simulators possible.

Two major design decisions came out of this trip: the blitter and HAM mode. Jay had already read about blitters in electronic design magazines and had taken a course at Stanford on their use, so they were not a new idea for him. However, the flight simulator experience had made him determined to create the best possible blitter for the Amiga.

A blitter is a dedicated chip that can move large chunks of graphics around on the screen around very quickly without having to involve the CPU. All modern video cards have what is essentially an advanced descendant of a blitter inside them. Again, Jay was ahead of his time.

HAM mode, which stood for Hold And Modify, was a way of getting more colors to display on the screen than could normally fit into the display memory. At the time, memory chips were very expensive, and the cost for displaying millions of colors at once was too high even for military applications like the Link simulator. So instead of storing all the color information for each dot (or pixel) on the display, the hardware could be programmed to start with one color and then change only one component of it (Hue, Saturation or Luminosity) for each subsequent pixel along each line. Jay decided to put this into the Amiga.

Later on in the design process, Jay would become concerned that HAM mode was too slow and even asked his chip layout artist if he could take it out. The chip designer replied that it would take many months and leave an aesthetically unappealing "hole" in the middle of the chip. Jay decided to keep the feature in, and later admitted that this was a good decision. The Amiga shipped with the ability to display 4096 colors in this mode, far more than any of its competitors, with clever programmers squeezing even more colors out of future Amiga chipset revisions. Despite HAM being suitable only for displaying pre-calculated images, a software company would even develop a graphics editor that operated in HAM mode. Like the chess game on the Atari 2600 before it, programmers would make the impossible possible on the Amiga.
</pre>
			<h3>Screens like no other</h3>
			<pre>
Another new invention for the Amiga computer was the "copper" chip. This was essentially a special-purpose CPU designed specifically for direct manipulation of the display. It had only three instructions, but it could directly access any part of the other display chips at any time. What's more, it could turn amazing tricks in the fraction of a second that it took for the monitor to refresh the display. This allowed a trick that no other computer has ever reproduced: the ability to view multiple different screens, opened at different resolutions, at the same time. These "pull-down" screens would amaze anyone who saw them. Modern computers can open different screens at different resolutions (say, for example, to open a full-screen game at a lower resolution than the desktop is displaying, in order to play the game faster or at a higher frame rate) but they can only switch between these modes, not display multiple modes at once.

The design eventually coalesced down to three chips named Agnes, Denise, and Paula. Agnes handled direct access to memory and contained both the blitter and copper chips. Denise ran the display and supported "sprites," or graphical objects that could be displayed and moved over a complex background without having to redraw it. Finally, Paula handled sound generation using digitally-sampled waveforms and was capable of playing back four channels at once: two on the left stereo channel and two on the right. It would be years before competing computer sound capabilities came anywhere close to this ability. Paula also controlled the Amiga's floppy disk drive.

These chips formed the core of what would be referred to as the Amiga's "custom chipset." However, they did not yet exist, except on paper. While the software development team was able to get started planning and writing programs that would support the chipset's features, the hardware team needed some way to test that their chips would actually work before committing to the expense of manufacturing them. In addition, the operating software could not be fully tested without having real Amiga hardware to run it on.
</pre>
		</div>
		<div class="content" id="3">
			<h1>A history of the Amiga, part 3: The first prototype</h1>
			<i>The third installment of our series on the history of the Amiga begins with …</i>
			<h2>Prototyping the hardware</h2>
			<pre>
Modern chips are designed using high-powered workstations that run very expensive chip simulation software. However, the fledgling Amiga company could not afford such luxuries. It would instead build, by hand, giant replicas of the silicon circuitry on honeycomb-like plastic sheets known as breadboards.

Breadboards are still used by hobbyists today to rapidly build and test simple circuits. The way they work is fairly simple. The breadboard consists of a grid of tiny metal sockets arranged in a large plastic mesh. Short vertical strips of these sockets are connected together on the underside of the board so that they can serve as junctions for multiple connectors. Small lengths of wire are cut precisely to length and bent into a staple-like shape, with the exposed wire ends just long enough to drop neatly into the socket. Small chips that perform simple logic functions (such as adding or comparing two small numbers in binary code) straddle the junctions, their centipede-like rows of metal pins precisely matching the spacing of the grid.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/lorrainechips.jpg" alt="The Lorraine prototype, with three custom chips. Image courtesy of Secret Weapons of Commodore">
			<br>
			<i>The Lorraine prototype, with three custom "chips." Image courtesy of <a href="http://www.floodgap.com/retrobits/ckb/secret/">Secret Weapons of Commodore</a></i>
			<pre>
At the time, nobody had ever designed a personal computer this way. Most personal computers, such as the IBM PC and the Apple ][, had no custom chips inside them. All they consisted of was a simple motherboard that defined the connections between the CPU, the memory chips, the input/output bus, and the display. Such motherboards could be designed on paper and printed directly to a circuit board, ready to be filled with off-the-shelf chips. Some, like the prototypes for the Apple ][, were designed by a single person (in this case, Steve Wozniak) and manufactured by hand.

The Amiga was nothing like this. Its closest comparison would be to the minicomputers of the day—giant, refrigerator-sized machines like the DEC PDP-11 and VAX or the Data General Eagle. These machines were designed and prototyped on giant breadboards by a team of skilled engineers. Each one was different and had to be designed from scratch—although to be fair, the minicomputer engineers had to design the CPU as well, a considerable effort all by itself! These minicomputers sold for hundreds of thousands of dollars each, which paid for the salaries of all the engineers required to construct them. The Amiga team had to do the same thing, but for a computer that would ultimately be sold for under $2,000.

So there were three chips, and each chip took eight breadboards to simulate, about three feet by one and a half feet in size, arranged in a circular, spindle-like fashion so that all the ground wires could run down the center. Each board was populated with about 300 MSI logic chips, giving the entire unit about 7200 chips and an ungodly number of wires connecting them all. Constructing and debugging this maze of wires and chips was a painstaking and often stressful task. Wires could wiggle and lose their connections. A slip of a screwdriver could pull out dozens of wires, losing days of work. Or worse, a snippet of cut wire could fall inside the maze, causing random and inexplicable errors.

However, Jay never let the mounting stress get to him or to his coworkers. The Amiga offices were a relaxed and casual place to work. As long as the work got done, Jay and Dave Morse didn't care how people dressed or how they behaved on the job. Jay was allowed to bring his beloved dog, Mitchy, into work. He let him sit by his desk and had a separate nameplate manufactured for him.

Jay even let Mitchy help in the design process. Sometimes, when designing a complex logic circuit, one comes to a choice of layout that could go either way. The choice may be an aesthetic one, or merely an intuitive guess, but one can't help but feel that it should not be left merely to random chance. On these occasions Jay would look at Mitchy, and his reaction would determine the choice Jay would make.

Slowly, the Amiga's custom chips began to take shape. Connected to a Motorola 68000 CPU, they could accurately simulate the workings of the final Amiga, albeit more slowly than the final product would run. But a computer, no matter how advanced, is nothing more than a big, dumb pile of chips without software to run on it.
</pre>
			<h2>Raising the bar for operating systems</h2>
			<pre>
All computers since the very first electronic calculators required some kind of "master control program" to handle basic housekeeping tasks such as running application programs, managing the user environment, talking to peripherals such as floppy and hard disks, and controlling the display. This master program is called the operating system, and for most personal computers of the day, it was a very simple program that was only capable of doing one thing at a time.

Jay's specialty was designing hardware, not software, so he had little input on the design of the Amiga's operating system. But he did know that he wanted his computer to be more advanced than the typical personal computers of the time running such primitive operating systems as AppleDOS and MS-DOS. His hire for chief of software engineering, Bob Pariseau, did not come from a background in microcomputers. He worked for the mainframe computer company Tandem, which made massive computers that were (and are still today) used by the banking industry.

Bob was used to his powerful computers that could handle many tasks and transactions at one time. He saw no reason why microcomputers should not be capable of the same thing. At the time, there were no personal computers that could multitask, and it was generally felt that the small memory capacities and slow CPU speeds of these machines made multitasking impossible. But Bob went ahead and hired people who shared his vision.

The four people he hired initially would later become legends of software development in their own right. They were RJ Mical, Carl Sassenrath, Dale Luck, and Dave Needle. Carl's interview was the simplest of all: Bob asked him what his ultimate dream job would be, and he replied, "To design a multitasking operating system." Bob hired him on the spot.

Carl Sassenrath had been hired from Hewlett-Packard where he had been working on the next big release of a multitasking operating system for HP's high-end server division. According to Carl:

"What I liked about HP was that they really believed in innovation. They would let me buy any books or publications I wanted... so I basically studied everything ever published about operating systems. I also communicated with folks at Xerox PARC, UC Berkeley, MIT, and Stanford to find out what they were doing.

In 1981-82 I got to know CPM and MSDOS, and I concluded that they were poor designs. So, I started creating my own OS design, even before the Amiga came along."

So the Amiga operating system would be a multitasking design, based on some of Carl's ideas that would later be called a "microkernel" by OS researchers in academia. Carl had invented the idea before it even had a name; the kernel, or core of the operating system, would be small, fast, and capable of doing many things at once, attributes that would then pervade the rest of the operating system.

The decision to make a multitasking kernel would have a huge impact on the way the Amiga computer would perform, and even today the effects can still be felt. Because the mainstream PC market did not gain true multitasking until 1995 (with Windows 95) and the Macintosh until 2001 (with OSX), an entire generation of software developers grew up on these platforms without knowing or understanding its effects, whereas the Amiga, which had this feature since its inception, immediately gave its developers and users a different mindset: the user should never have to wait for the computer. As a result, programs developed for the Amiga tend to have a different, more responsive feel than those developed for other platforms.
</pre>
			<h3>Adding a GUI</h3>
			<pre>
There was one more significant design decision that was made about the Amiga at this time: to design it with a graphical user interface. Most personal computers at the time were controlled by a command line interface; the user had to type in the name of a program to run it and enter a long series of commands to move files or perform maintenance tasks on the computer.

The idea of a graphical user interface was not new. Douglas Engelbart had demonstrated most of its features along with the world's first computer mouse in 1968, and researchers at Xerox PARC had created working models in the mid-70's. At the beginning of the 1980's, it seemed everyone was trying to cash in on the graphical interface idea, although developing it on the primitive computers of the day was problematic. Xerox itself released the Star computer in 1981, but it cost $17,000 and sold poorly, serving mostly as an inspiration for other companies. Apple's version, the Lisa, came out in 1983. It cost $10,000 and also sold poorly. Clearly, personal computers were price-sensitive, even if they had advanced new features.

Apple solved the price issue by creating a stripped-down version of the Lisa. It took away the large screen, replacing it with a tiny 9 inch monochrome monitor. Instead of two floppy drives, the new machine would come with only one. There were no custom chips to accelerate sound or graphics. And as much hardware as possible was removed from the base model, including the memory—the operating system was completely rewritten to squeeze into 128 kilobytes of RAM. The stripped-down operating system was only capable of running one application at a time—it couldn't even switch between paused tasks.

This was the Macintosh, which was introduced to the world in dramatic fashion by Steve Jobs in January of 1984. What most people don't remember about the Macintosh was that initially it was not a success—it sold reasonably well in 1984, but the following year sales actually went down. The Mac in its original incarnation was actually not very useful. The built-in word processor that came with the machine was limited to only eight pages, and because of the low memory and single floppy drive, making a backup copy of a disk took dozens of painful, manual swaps.

The Amiga operating system team wasn't thinking like this. The hardware design group wasn't compromising and stripping things down to the bare minimum to save money, so why should they?
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/workbench.jpg" alt="The Amiga user interface, Workbench.">
			<br>
			<i>The Amiga user interface, Workbench.</i>
			<pre>
One of the more difficult parts of writing a graphical user interface is doing the low-level plumbing, called an API, or Application Programming Interface, that other programmers will use to create new windows, menus, and other objects on the system. An API needs to be done right the first time, because once it is released to the world and becomes popular, it can't easily be changed without breaking everyone's programs. Mistakes and bad design choices in the original API will haunt programmers for years to come.

RJ Mical, the programmer who had come up with the "Zen Meditation" game, took this task upon himself. According to Jay Miner, he sequestered himself in his office for three weeks, only coming out once to ask Carl Sassenrath a question about message ports. The resulting API was called Intuition, an appropriate name given its development. It wound up being a very clean, easily-understandable API that programmers loved. In contrast, the API for Windows, called Win16 (later updated to Win32) was constructed by a whole team of people and ended up as a mishmash that programmers hated.
</pre>
			<h2>Working 90-hour weeks</h2>
			<pre>
RJ Mical recalled what life was like back in those busy early days:

"We worked with a great passion... my most cherished memory is how much we cared about what we were doing. We had something to prove... a real love for it. We created our own sense of family out there."

Like the early days at Atari, people were judged not on their appearance or their unusual behavior but merely on how well they did their jobs. Dale Luck, one of the core OS engineers, looked a bit like a stereotypical hippie, and there were even male employees who would come to work in purple tights and pink fuzzy slippers. "As long as the work got done, I didn't mind what people looked like," was Jay Miner's philosophy. Not only was it a family, but it was a happy one: everyone was united by their desire to build the best machine possible.

Why was everybody willing to work so hard, to put in tons of late (and sometimes sleepless) nights just to build a new computer? The above and beyond dedication of high-tech workers has been a constant ever since Silicon Valley became Silicon Valley. Companies have often reaped the rewards from workers who were willing to put in hundreds of hours of unpaid overtime each month. Managers in other industries must look at these computer companies and wonder why they can't get their workers to put in that kind of effort.

Part of the answer lies with the extreme, nearly autistic levels of concentration that are achieved by hardware and software engineers when they are working at peak efficiency. Everyday concerns like eating, sleeping, and personal hygiene often fade into the background when an engineer is in "the zone." However, I think it goes beyond that simple explanation. Employees at small computer companies have a special position that even other engineers can't hope to achieve. They get to make important technical decisions that have far-reaching effects on the entire industry. Often, they invent new techniques or ideas that significantly change the way people interact with their computers. Giving this kind of power and authority to ordinary employees is intoxicating; it makes people excited about the work that they do, and this excitement then propels them to achieve more and work faster than they ever thought they could. RJ Mical's three-week marathon to invent Intuition was one such example, but in the story of the Amiga there were many others.

The employees of Amiga, Inc. needed this energy and passion, because there was a hard deadline coming up fast. The Consumer Electronics Show, or CES, was scheduled for January 1984.
</pre>
			<h2>The January CES and the buyout of Amiga</h2>
			<pre>
CES had expanded significantly since its inception in 1967. The first CES was held in New York City, drawing 200 exhibitors and 17,500 attendees. Among the products that had already debuted at CES were the VCR (1970), the camcorder (1981), and the compact disc player (also 1981). CES was also home to the entire nascent video game industry, which would not get its own expo (E3) until 1995.

Amiga, Inc. didn't have a lot of money left over for shipping its prototype to the show, and the engineers were understandably nervous about putting such a delicate device through the rigors of commercial package transport. Instead, RJ Mical and Dale Luck purchased an extra airline seat between the two of them and wrapped the fledgling Amiga in pillows for extra security. According to airline regulations, the extra "passenger" required a name on the ticket, so the Lorraine became "Joe Pillow," and the engineers drew a happy face on the front pillowcase and added a tie! They even tried to get an extra meal for Joe, but the flight attendants refused to feed the already-stuffed passenger.

The January 1984 CES show was an exciting and exhausting time for the Amiga engineers. Amiga rented a small booth in the West Hall at CES, with an enclosed space behind the public display to showcase their "secret weapon," the Lorraine computer. A guarded door led into the inner sanctum, and once inside people could finally see the massive breadboarded chips, sitting on a small table with a skirt around the edges. Skeptical customers would often lift the skirt after seeing a demonstration, looking for the "real" computer underneath.

The operating system and other software were nowhere near ready, so RJ Mical and Dale Luck worked all night to create software that would demonstrate the incredible power of the chips. The first demo they created was called Boing and featured a large, rotating checkered ball bouncing up and down, casting a shadow on a grid in the background, and creating a booming noise in stereo every time it hit the edge of the screen. The noise was sampled from Bob Pariseau hitting the garage door with one of the team's celebrated foam baseball bats. The Boing Ball would wind up becoming an iconic image and became a symbol for the Amiga itself.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/boingballdemo.jpg" alt="The famous Amiga Boing Ball demo.">
			<br>
			<i>The famous Amiga Boing Ball demo.</i>
			<pre>
The January CES was a big success for the Amiga team, and the company followed it up by demonstrating actual prototype silicon chips at the June CES in Chicago, but the fledgling company was rapidly running out of money. CEO Dave Morse gave presentations to a number of companies, including Sony, Hewlett-Packard, Philips, Apple, and Silicon Graphics, but the only interested suitor was Atari, who lent the struggling company $500,000 as part of a set of painful buyout negotiations. According to the contract, Amiga had to pay back the $500,000 by the end of June or Atari would own all of their technology. "This was a dumb thing to agree to but there was no choice," said Jay Miner, who had already taken a second mortgage out on his house to keep the company going.

Fortunately for Amiga (or unfortunately, depending on how you imagine your alternate histories) Commodore came calling at the last minute with a buyout plan of its own. It gave Amiga the $500,000 to pay back Atari, briefly thought about paying $4 million for the rights to use the custom chips, and then finally went all in and paid $24 million to purchase the entire company. The Amiga had been saved, but it now belonged to Commodore.
</pre>
		</div>
		<div class="content" id="4">
			<h1>A history of the Amiga, part 4: Enter Commodore</h1>
			<i>Part four of our series on the history of the Amiga covers Amiga's rescue by …</i>
			<h2>Deus ex machina</h2>
			<h3>Commodore</h3>
			<pre>
The company that rescued Amiga in 1984 was the creation of a single man. Born in Poland in 1928 as Idek Tramielski, he was imprisoned in the Nazi work camps after his country was invaded in World War II. Rescued from the camps by the US Army, he married a fellow concentration camp survivor named Helen Goldgrub, and the two emigrated to the United States. Upon arrival, he changed his name to Jack Tramiel.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/200/tramiel.jpg" alt="Jack Tramiel">
			<br>
			<i>Jack Tramiel</i>
			<pre>
Jack enlisted in the US Army in 1948 and served in the Equipment Repair Office. He served two tours of duty in Korea, then left the Army to work at a small typewriter repair company. In 1955, Jack and his wife left for Canada to start their own typewriter manufacturing firm. Jack wanted a military-sounding name for the company, but General and Admiral were already taken, so he settled on Commodore after seeing a car on the street with that name.

The little firm grew quickly, going public in 1962, but it became enveloped in a financial scandal that threatened to consume the company. Jack was a survivor, however, and would not give up. He found a financier named Irving Gould who purchased a large chunk of Commodore and moved it into new directions. Inexpensive Japanese typewriters were eating into Commodore's profits, so the company got into selling calculators instead. Then cheap calculators from Japan and from US firms like Texas Instruments threatened to take that business away as well. Jack realized that in order to survive the price wars, he needed to control the chips that went into the calculators. In 1976 he bought MOS Technologies, the same company that split off from Motorola to produce the legendary 6502 chip that ended up in the Apple ][, various game consoles, and the Atari 400/800 series.

The MOS purchase got Commodore into the computer business, starting with the PET, then the low-cost VIC 20, and finally in 1982 the company released the best-selling personal computer model of all time: the Commodore 64.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/commodore_logo.jpg" alt="">
			<pre>
The 64 was a huge hit, selling over 22 million machines over its life span and firmly cementing Commodore as one of the major players in the burgeoning personal computer industry. However, things were not all rosy at the company.

Jack was determined not just to compete with other computer companies, but to destroy them. "Business is war," was his motto, and while the price war he initiated did take out some competitors—including getting revenge against TI, which withdrew from the computer business in October 1983—it also strained Commodore's profits. Tramiel often fought with Gould over matters of money: the financier wanted Jack to grow the business without any extra capital, but Jack wanted more cash in order to lower costs and thus wipe out the rest of his competitors. "We sell computers to the masses, not the classes," he once said, reflecting the price difference between a $199 Commodore 64 and machines from Apple and IBM that cost thousands of dollars.

In the end, as is often the case when battling your financiers, the money people won. Jack Tramiel was forced out of his own company by the board of directors in late 1983.

This ouster would have a huge effect on the fledgling Amiga company, because Tramiel did not go quietly.

Jack Tramiel was a study in conflicts and contradictions, like any human being, but more so. His hardheaded management style made him enemies, but also made him steadfast friends—many key employees quit Commodore when he left to join him in his new ventures. His tendency to jump from project to project paid huge dividends when the company moved from the PET to the VIC-20 to the Commodore 64, but that same line of thinking hurt the company when ill-conceived successors such as the Plus/4 failed in the marketplace.

So it should come as no surprise that Tramiel's departure from Commodore both saved and doomed the Amiga at the same time.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/atari_logo.png"  alt="">
			<pre>
Before Tramiel had left, Commodore had already engaged in halfhearted talks to purchase the struggling Amiga, Inc., but nothing had come from them. Atari was developing a new personal computer and game console and wanted access to the Amiga chipset. The initial offer was for $3 a share and kept getting lower. When it hit 98¢ per share, both sides walked away from the table. It was at this point that Atari "loaned" Amiga $500,000 to continue operations for a few more months.

This poisonous deal was put together by none other than Jack Tramiel, who had managed to purchase Atari's computer division after being kicked out of Commodore. Due to the video game crash of 1983, Atari's parent company Warner Communications had been looking to dump the computer and home console video game portions of Atari (they would retain the arcade division, which was still doing well), and Tramiel managed to work out a spectacular deal that gave him ownership of Atari's computer division for no money down.

When Jack left Commodore for Atari, the former company's stock fell while the latter's rose, as public opinion still considered (and rightly so) Jack to have been the driving force that built Commodore's success. A steady flow of engineers followed Tramiel to Atari, which prompted Commodore to sue Atari for theft of trade secrets. (Tramiel, in his inimitable style, would later countersue; both lawsuits were eventually settled out of court.) To compete with Tramiel and regain the engineering talent they had lost, Commodore decided to purchase Amiga wholesale. Keeping the original Amiga team intact saved the computer as Jay Miner and the others had originally envisioned it.

However, it also made Tramiel more determined than ever to get his revenge on Commodore. That revenge would come in the form of the Atari ST—sometimes called the Jackintosh—which was rushed into production to compete against the Amiga. Had Jack never been kicked out of Commodore, the Atari line of computers might have just faded into oblivion after Warner had dumped the company. The competition between Amiga and Atari would wind up hurting both platforms as they focused their resources on fighting each other rather than making sure they had a place in a world increasingly dominated by the IBM PC.

Still, all that was in the future, and the Amiga team, now a fully-owned subsidiary of Commodore, had but one thing on their minds: finishing the computer.
</pre>
			<h2>Finalizing the design</h2>
			<pre>
One hugely positive benefit about being owned by a large computer company was that the Amiga team no longer (for the moment, anyway) had to worry about money. The team was moved 10 miles to a spacious, rented facility in Los Gatos, California. They could afford to hire more engineers, and the software development team went from having 10 people sharing a single Sage workstation to everyone having their own SUN on their desk.

The influx of resources made the release of the Amiga computer possible, but it was still a race against time to get the computer finished before the competition took away the market.

While the hardware was mostly done, pending a few adjustments by Jay Miner and his team, the software (as is usually the case in high-tech development) was falling behind schedule. The microkernel, known as Exec, was mostly complete, thanks to the brilliant work by Carl Sassenrath, and the GUI was coming together as well, building on RJ Mical's solid framework (for a short time, his new Commodore business card read "Director of Intuition").

However, there was a third layer necessary to complete the picture. Exec, like modern microkernels, handled basic memory and task management, but there was still a need for another component to handle mundane tasks such as the file system and other operating system duties.
</pre>
			<h3>The CAOS debacle</h3>
			<pre>
Originally, that third layer was known as CAOS, which stood for the Commodore Amiga Operating System. Exec programmer Carl Sassenrath wrote up the design spec for CAOS, which had all sorts of neat features such as an advanced file system and resource tracking. The latter was a method of keeping track of such things as file control blocks, I/O blocks, message ports, libraries, memory usage, shared data, and overlays, and freeing them up if a program quit unexpectedly. As the Amiga software engineers were already behind schedule, they had contracted out parts of CAOS development to a third party. Still, as is often the case in software, the development hit some unforeseen roadblocks.

According to Commodore engineer Andy Finkel, the management team "decided that it wouldn't be possible to complete [CAOS] and still launch the Amiga on time, especially since the software guys had already given up weekends at home. And going home. And sleeping."

Lack of time wasn't the only problem. The third-party development house learned that Amiga, Inc., had been bought out by Commodore, and they suddenly demanded significantly more money than had originally been agreed upon. "Commodore tried to negotiate with them in good faith, but the whole thing fell apart in the end," recalled RJ Mical, who was upset by the whole event. "It was a jerk-butt thing that they did there."
</pre>
			<h3>TripOS to the rescue</h3>
			<pre>
When the CAOS deal fell apart, the Amiga team suddenly needed a replacement operating system. Relief came in the form of TripOS, written by Dr. Tim King at the University of Cambridge in the 1970s and 80s, and later ported to the PDP-11. Dr. King formed a small company called MetaComCo to quickly rewrite TripOS for the Amiga, where it became known as AmigaDOS.

AmigaDOS handled many of the same tasks as CAOS, but it was an inferior replacement. "Their code was university-quality code," said Mical, "where optimized performance was not important, but where theoretical purity was important." The operating system also lacked resource tracking, which hurt the overall stability of the system. This oversight had repercussions that remain to this day: the very latest PowerPC-compiled version of AmigaOS will still sometimes fail to free up all resources when a program crashes.

Interestingly, TripOS (and thus AmigaOS) was written in the BCPL language, a predecessor to C. Later versions of the operating system would replace this with a combination of C and Assembler.

With the kernel, OS, and GUI ready, and with last-minute adjustments to the custom chips, all that remained was designing a case for the system, which had been dubbed the Amiga 1000. Jay Miner felt it would be appropriate to have the signature from all 53 Amiga team members—both Amiga, Inc. employees and Commodore engineers who later joined the project—to be preserved on the inside of the computer's case. Both Joe Pillow and Jay's dog Mitchy got to sign the case in their own unique way.

Dave Morse, who was still nominally in charge of Commodore Amiga, added his own idea for the case: a raised "garage" on the bottom that users could slide their keyboards into when not in use.

There was only one potential stumbling block preventing the release of the Amiga 1000: the decision about how much RAM to put in the system. Cost-conscious Commodore wanted to ship with only 256KB. Knowing that the operating system and GUI needed more memory, Jay insisted on shipping with 512KB. The two sides were unable to come to an agreement, so a compromise was reached: the Amiga would ship with 256KB but come with an easily-accessible expansion cage on the front of the case that could accommodate more memory. Jay would later say that he had to "put his job on the line" just to get Commodore to put the expansion port in.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amiga1000.jpg" alt="The final Amiga 1000 design">
			<br>
			<i>The final Amiga 1000 design</i>
			<pre>
Now that all the pieces were in place, Commodore decided to announce the Amiga to the world. For the first time in the company's history, management decided to pull out all the stops. The Amiga announcement would be the most lavish and expensive new product showcase in the history of personal computers.
</pre>
			<h2>The announcement</h2>
			<pre>
Commodore rented the Lincoln Center and hired a full orchestra for the Amiga announcement ceremony, which was videotaped for posterity. All Commodore employees were given tuxedoes to wear for the event: RJ Mical one-upped the rest by finding a pair of white gloves to complete his ensemble. The band played a jaunty little number with tubas and xylophones as a brilliant laser display revealed the Amiga name in its new font.

The master of ceremonies was Commodore marketing vice president Bob Truckenbrode, but he soon gave way to the real star of the show: the head of software engineering, Bob Pariseau. With his long hair elegantly tied back in a ponytail, Pariseau directed the demonstration like a maestro conducting a symphony. With each wave of his hand, he would signal his counterpart, sitting at a real Amiga 1000, to demonstrate each new feature.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/bobpariseau.jpg" alt="Robert Pariseau">
			<br>
			<i>Robert Pariseau</i>
			<pre>
"At Amiga, the user controls how he uses his time, not the computer," Pariseau said, as his assistant showed the flexibility of the then-new graphical user interface. He then brought up a graphical word processor called TextCraft to show how a GUI could be applied to everyday work: the word processor featured menus, toolbar buttons, and an on-screen ruler for setting margins and tab stops. Pedestrian stuff for 1995, but astounding for a decade earlier!

Then he moved on to showing off the Amiga's graphics capabilities, showing all 4,096 colors at once on the same screen, followed by a close-up photo of a baboon's face in 640 by 400 resolution: an image that many people might remember gazing at in VGA monitor advertisements from the early 1990s.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/mandrell.jpg" alt="It's looking at you!">
			<br>
			<i>It's looking at you!</i>
			<pre>
From static images, he moved on to the Amiga's strong suit: animation. The custom chips included hardware commands to flood fill arbitrary areas: those who remember using flood fill in Photoshop on older computers will remember how slow it was when it had to rely on the CPU. The Amiga's hardware-accelerated version filled up multiple rotating and intersecting triangles with different colors as they spun across the screen, all at a constant 30 frames per second. Another animation demo, Robot City, showed the Amiga's built-in sprite and collision detection features, allowing large animated characters to move over complex backgrounds and interact with each other.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/triangles.jpg" alt="Hardware flood fills">
			<br>
			<i>Hardware flood fills</i>
			<pre>
None of the demos were taking over the entire computer to do their magic. Each full-screen demo could be smoothly slid down to reveal other running applications beneath.

The concept of multitasking was virtually unknown for personal computer users in 1985, and Bob went through several examples of how this feature could be used not just for entertainment but for business applications as well. A bar chart and pie chart were built simultaneously from the same numerical data, and the user could quickly switch from one window to another to see the results in either format.

Moving on from graphics to sound, Bob demonstrated the four-channel synthesized sound hardware by using the keyboard as a virtual piano playing various different sampled instruments. "With all four channels going simultaneously, the 68000 [CPU] is idle," Pariseau commented, something that would not be true for many years in other computers until sampled waveform sound cards became available for PCs. A close-up of the Amiga operator at the keyboard showed his fingers shaking slightly—there was a lot riding on these demos, and the software was brand new and still largely untested. Yet the Amiga performed masterfully in its first time on stage, without crashing once.

The next demonstration was of computer-generated speech: the Amiga spoke in a male voice, a female voice, a fast and a slow voice, and all were pitch-modulated to sound more like a real person; the last voice was spoken in a monotone, "just like a real computer." This line got a good laugh from the audience.

Even back in 1985, the market was already showing signs of standardizing on the IBM PC platform, and Bob acknowledged this fact in his speech. "You know, it's hard," he said, "it's hard to be innovative in an industry that has been dominated by one technology for so long. We at Commodore Amiga knew that to do this [introduce a new platform] we had to be at least an order of magnitude better than anything anyone had ever seen.

"We've done that," he continued, "and then we decided: why stop there? Why not include that older technology in what we had already done?" Thus was set the stage for the very first IBM PC emulator on the Amiga, called Amiga Transformer. The program was started up, then a PC-DOS installation disk was placed into an attached 5.25 inch floppy drive, and this was replaced with a Lotus 1-2-3 disk. "Standard, vanilla, IBM DOS," Bob said with a sigh, and the crowd laughed again. Compared to the exciting graphics and sound demos of a few minutes earlier, it was a bit of a letdown seeing the industry standard spreadsheet take over the screen.

To lighten the mood, Bob finished off with a replay of the original Boing Ball demo that was first shown at CES only a year earlier. "We've lived our dream," he said, "and seen it come to life. Now it's your turn. What will you do with the Amiga Computer?"
</pre>
			<h2>Andy and Debbie</h2>
			<pre>
Two unlikely celebrities were then invited on stage to show what creative folks might do with their Amigas. Deborah Harry, the lead singer of Blondie, walked on stage along with counterculture art icon Andy Warhol, who took a quick appreciative glance at her red dress as they sat down. "Are you ready to paint me now?" Debbie asked, her voice slightly nervous.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/warhol.jpg" alt="">
			<pre>
Andy sat down in front of the Amiga 1000, looking at it like it was some kind of alien technology from another world. "What other computers have you worked with?" asked resident Amiga artist Jack Hager. "I haven't worked on anything," Andy replied truthfully. "I've been waiting for this one." A nearby video camera was attached to a digitizer, and from this setup a monochrome snapshot of Debbie's face appeared on the Amiga screen, ready for Andy to add a splash of color.

It is a cardinal rule in doing computer demos in public that you never let anyone else take control of the machine, lest they do something off-script that winds up crashing the computer. The paint program (ProPaint) being used was a very early alpha, and the software engineers knew that it had bugs in it. One of the known bugs was that the flood fill algorithm—the paint program didn't use the hardware fills that were demonstrated earlier—would usually crash the program every second time it was used. Yet there was Andy clicking here, there, and everywhere with the flood fill. Somehow, the demo gods were smiling on Amiga that day, and the program didn't crash. "This is kind of pretty," Andy said, admiring his work. "I think I'll keep that."
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/blondie.jpg" alt="The finished product">
			<pre>
The show ended with a short video—powered by the Amiga—of a wireframe ballerina, who then turned into a solid-shaded figure, and finally a fully rotoscoped animated image. A real ballerina then came out on stage and danced in sync with her animated counterpart.
</pre>
			<h3>Reactions to the show</h3>
			<pre>
While the crowd attending the show went away extremely impressed with what they had seen, the reaction from the rest of the world was mixed. Articles about the demo were published in magazines such as Popular Computing, Fortune, Byte, and Compute. The Fortune article both praised and dismissed the Amiga at the same time: "While initial reviews praised the technical capabilities of the Amiga, a shell-shocked PC industry has learned to resist the seductive glitter of advanced technology for its own sake."

Think about that last line for a few moments. Can any computer user today honestly say that color, animation, multichannel sound, and multitasking are merely seductive glitter that exists only for its own sake? Like Doug Engelbart's revolutionary demonstration of the first mouse-driven graphical user interface back in 1968, many of the ideas shown in the Amiga unveiling were a little too far ahead of their time, at least for some people.

Nevertheless, Commodore had some great buzz leading up to the introduction of the Amiga 1000. The machine had great hardware and software. It had features that no other computer could even hope to emulate. Freelance writer Louis Wallace described it thusly: "To give you an idea of its capabilities, imagine taking all that is good about the Macintosh, combine it with the power of the IBM PC-AT, improve it, and then cut the price by 75 percent." This last part was a bit of an exaggeration, but not by much: the final price of the Amiga 1000 was set at $1,295 for the 256KB version and $1,495 for the 512KB one. This compared favorably to the Macintosh, which had only 128KB and sold for $2,495.

Commodore looked like it had everything going for it. The new Amiga computer was years ahead of the competition, and many people in the company—including Jay Miner—felt that they had a real chance to significantly impact the industry. Sitting in the crowd during the Amiga's unveiling was Thomas Rattigan, an enthusiastic executive who had come from Pepsi and was being groomed for the position of CEO at Commodore. He had big plans for the Amiga. The original designers had achieved their dream by creating the Amiga from nothing, but now bigger dreams were being imagined for the little computer.

Unbeknownst to him, however, larger forces were at work that would turn these dreams into nightmares.
</pre>
		</div>
		<div class="content" id="5">
			<h1>A history of the Amiga, part 5: postlaunch blues</h1>
			<h2>On the cusp of greatness</h2>
			<pre>
By July 1985, Commodore had everything going for it. The Amiga computer had been demonstrated in public to rave reviews, and everyone was excited at the potential of this great technology.

That's when the problems started.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/plus_4.jpg" alt="The Commodore Plus/4">
			<br>
			<i>The Commodore Plus/4</i>
			<pre>
Commodore's primary woes were always about money, and 1985 was no exception. Sales of the Commodore 64 were still going strong, but the price wars had slashed the profits on the little computer. The company had invested millions of dollars creating new and bizarre 8-bit computers that competed directly against the venerable C-64, such as the wholly incompatible Plus/4, that had no chance in the marketplace. To make things worse, the company had to deal with lawsuits from its ousted founder, Jack Tramiel. Finally, Commodore had invested $24 million to purchase Amiga outright, but as the computer had not gone on sale yet, there was no return on this investment.

All these financial problems put a strain on the company's ability to get the Amiga ready to sell to the public. Without a lot of spare cash, it was difficult to rush the production of the computer. Further software delays pushed back the launch as well. The end result was that the Amiga did not go on sale until August of 1985.

This wouldn't have been a huge problem, had Commodore been able to gather enough resources to ship the machine in quantity. Instead, production delays meant that the computers trickled off the assembly lines, and by October there were only 50 Amiga 1000 units in existence, all used by Commodore for demos and internal software development.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/atari-st.jpg" alt="Jack Tramiel's Atari ST">
			<br>
			<i>Jack Tramiel's Atari ST</i>
			<pre>
This delay was doubly crippling because Jack Tramiel had managed to rush the development of the Atari ST, using off-the-shelf chips and an operating system and GUI purchased from Digital Research. Tramiel was able to show the ST off at the January CES and started taking orders for the computer shortly thereafter. This sudden competition from Commodore's former CEO took everyone by surprise.
</pre>
			<h3>Missing Christmas</h3>
			<pre>
Amiga 1000 computers did not start to appear in quantity on retail shelves until mid-November 1985. This was too late to make a significant impact on the crucial holiday buying season. Most retailers make 40 percent or more of their yearly sales over the holidays, and Commodore had missed the boat.

To make matters worse, the company was not really clear about how it was going to sell the computer. The Commodore 64 had been sold at big retail chains like Sears and K-Mart, but marketing executives felt that the Amiga was better positioned as a serious business computer. Astoundingly, Commodore actually turned down Sears' offer to sell Amigas. Back in the 1980s, Sears was a major player in computer sales; I personally used to cherish parental shopping visits so that I could get my hands on the latest in computer technology. The Atari ST was sold there, but the Amiga was not.

Even these blunders might have been mitigated had Commodore come up with some truly amazing advertising campaigns to drum up interest in the new computer. The delays gave the company extra time to do this, but what Commodore came up with was so awful that it sickened many of its own employees.
</pre>
			<h2>Bad advertising</h2>
			<pre>
Because the Amiga was years ahead of its time compared to the competition, many Commodore executives believed that the computer would sell itself. This was not—and has never been—true of any technology. When personal computers first came on the scene in the late 1970s, most people had no idea what they would be useful for. As a result, the only people who bought them initially were enthusiastic and technically skilled hobbyists—a limited market at best. It took a few killer applications, such as the spreadsheet, combined with an all-out marketing assault, to drive sales to new levels.

The Amiga was in the same position in 1985. It was a multimedia computer before the term had been invented, but there were no killer applications yet. What it needed was a stellar advertising campaign, one that would drive enough sales to get software companies interested in supporting the new platform. Instead, what it got was a half-hearted series of television ads that ran over Christmas and were never seen again. The first commercial had a bunch of zombie-like people shuffling up stairs towards a pedestal, from which a computer monitor emanated a blinding light. It was a poor copy of Apple's famous 1984 advertisement, and failed to generate even a tiny amount of buzz in the industry.

From there, things got worse. The next ad was a rip-off of the ending of 2001: A Space Odyssey and featured an old man turning into a fetus. Some pictures of the commercial's production made their way to the Commodore engineers, and soon the "fetus on a stick" became a standard joke about their company's marketing efforts.

Further advertising used black-and-white and sepia-toned footage of typical family home movies, with some vague narration: "When you were growing up, you learned you faced a world full of competition." Amiga did indeed face a world full of competition, but this kind of lifestyle avant-garde advertising was already being done—and being done much better—by Apple.

What Commodore really needed at that time was some simple comparative advertising. A picture of an IBM PC running in text mode on a green monochrome screen, then a Macintosh with its tiny 9-inch monochrome monitor, then the Amiga with full color, multitasking, animation, and sound. For extra marks, you could even put prices under all three.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amiga-ad-alternate.jpg" alt="An Amiga ad from an alternate history">
			<br>
			<i>An Amiga ad from an alternate history</i>
			<pre>
As a result of Commodore dropping the ball on production and marketing, the firm sold only 35,000 Amigas in 1985. This didn't help with the balance sheet, which was getting grim.
</pre>
			<h3>Missing CES</h3>
			<pre>
Commodore had experienced a financial crunch at the worst possible time. In the six quarters between September 1984 and March 1986, Commodore Business Machines International lost over $300 million. Money was tight, and the bean-counters were in charge.

As a result, Commodore was a no-show for the January 1986 Consumer Electronics Show (CES). Ahoy! Magazine reflected on this conspicuous absence:

> "Understand that the last four CES shows in a row, dating back to January 1984, Commodore's exhibit had been the focal point of the home computer segment of CES, the most visited computer booth at the show—as befitted the industry's leading hardware manufacturer. Their pulling out of CES seemed like Russia resigning from the Soviet Bloc."

Commodore also missed the following computer dealer exhibition, COMDEX, as well as the June 1986 CES. The company had defaulted on its bank loans and could not get the bankers to lend any more money for trade shows.

The company's advertising also slowed to a trickle. Thomas Rattigan, who was being groomed for the position as Commodore's CEO, recalled those troubling times. "Basically, the company was living hand to mouth," he said. "When I was there, they weren't doing very much advertising because they couldn't afford it."

This strategic retreat from the market had a hugely negative impact on Amiga sales. In February 1986, Commodore revealed that it was moving between 10,000 and 15,000 Amiga 1000 computers a month. Jack Tramiel's Atari ST was beating the Amiga in sales figures, in signing up dealers, and worse still, in application support.
</pre>
			<h2>"They f***** it up"</h2>
			<pre>
Many Amiga engineers felt betrayed by Commodore's financial ineptitude and pathetic marketing efforts. They were disgusted that their company could take such an advanced and powerful computer and fail to capitalize on it. Most of these bad feelings were confined to grumblings in the hallways, but some of them wound up hurting the Amiga directly.

One of the software engineers working on upgrades to Workbench, the Amiga's graphical desktop environment, decided he would "get back" at Commodore for its failure to properly market the Amiga. He programmed in a hidden message, commonly known as an "Easter Egg" in the software industry, that would only appear only when the user pressed a certain combination of keys simultaneously. The message was "We made the Amiga, they fucked it up."

RJ Mical got a slight chuckle out of the message, but told the engineer (who remains nameless to this day) that it was unacceptable, and he would have to take it out. The engineer relented, and when Mical checked the final code, the offending text had been replaced with the message "Amiga: born a champion." He thought that was the end of it.

Little did he know that the engineer had added a second Easter Egg with the original message encrypted inside. To get to the message, you had to hold down eight separate keys, which would pop up the text "We made the Amiga" on the screen. If you kept the keys held down, and were very dexterous or had a friend to help you, inserting a floppy in the drive would flash the latter part of the message for 1/60th of a second. The engineer thought that nobody would ever see this last part, but because the Amiga could output its graphics directly to video, you could just tape the whole experience and press pause on the VCR to see it.

The message was discovered embedded in the ROMs for the European PAL version of the Amiga 1000, just after the computer had gone on sale in the United Kingdom. Managers at Commodore UK pulled tens of thousands of Amigas off the shelves and refused to sell the machines until replacement ROM chips were sent out that excised the offending message. The little joke by a single software engineer cost the Amiga over three months of sales in a major market and had ramifications that shook the whole company.
</pre>
			<h3>Leaving Los Gatos</h3>
			<pre>
After the Easter Egg fiasco, Commodore management decided that they should move the Amiga team closer to headquarters so that they could keep a closer eye on their activities. The Amiga engineers were asked to move across the country, from their offices in Los Gatos, California, to West Chester, Pennsylvania.

Many of the engineers shrugged their shoulders and started packing, but for some this was the last straw. RJ Mical, the software guru who had written the Intuition programming interface and designed much of the Amiga's GUI, decided that his future would lie elsewhere. He wound up working as an independent contractor on Amiga peripherals and software, including an early video capture device called a frame grabber.

Despite his issues with Commodore, Mical still was proud of the role he played in developing the Amiga. "Those were such cool days, you just couldn't believe it," he would later tell Commodore documentary author Brian Bagnall. "It was one of the most magical periods of my entire life working at Amiga. God, what an incredible thing we did."

The father of the Amiga, Jay Miner, also refused to switch coasts. While he left Commodore as an official employee, he continued to work as a consultant for them for many years. He also donated much of his time giving talks to Amiga user groups around North America, telling the story of how he brought his dream computer to life.
</pre>
			<h3>Searching for stability</h3>
			<pre>
The trials and tribulations of Commodore Business Machines International weren't the only problems that dogged the young Amiga computer. The initial release of the operating system was rushed, and as a result the first Amiga 1000 machines shipped with many bugs in the OS. The "Guru Meditation" error that started as a joke in the Amiga offices would come to haunt many early Amiga users.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/540/Guru_meditation.gif" alt="The infamous Guru meditation error">
			<br>
			<i>The infamous Guru meditation error</i>
			<pre>
Because the OS lacked memory protection, a fatal error in the OS or even in an application could lock up the system completely, forcing a reboot. Users might be taking advantage of the multitasking abilities of the Amiga to run many programs at once, only to lose work in all of them when the machine went down. The PC, Macintosh, and Atari ST, which had much simpler operating systems that could only run one application at a time, did not suffer from this problem.

As a result, the Amiga gained a reputation for instability that would stay with the machine for many years to come. The lack of memory protection wasn't the real problem—an operating system with full memory protection can still be brought down by a bug in the OS itself, and an application that crashes all the time isn't useful even if the OS keeps running. The software engineers at Commodore worked tirelessly to track down these bugs and eliminate them, as did the application developers. Years later, most Amiga users would run many applications at once and keep their machines operating for weeks and even months without crashing or requiring a reboot. However, the initial stability problems hurt the reputation of the Amiga—and it carried this reputation for the rest of its life.
</pre>
			<h2>Rattigan takes the reins</h2>
			<pre>
What had seemed like such a promising start for the Amiga Computer had turned, at least early on, into something resembling a disaster. Yet all was not lost. There was still hope that the problems that plagued the platform and its owner could be addressed, and the Amiga given a chance to thrive. Doing so, however, would necessitate a change in Commodore management.

The company, which had been thrown into such disarray when founder Jack Tramiel was unceremoniously booted out by the jet-setting financier Irving Gould, was currently being run by an uninspiring man named Marshall Smith. Smith had come from the steel industry, where nothing much changes across decades, and was thoroughly unprepared for the task of running a computer company.

An indication of what kind of man Marshall Smith was came at Commodore's 1985 Christmas party held at the Sunnybrook Ballroom in Pottstown, Pennsylvania. Drinking heavily, Smith started slam-dancing with a bunch of the engineers, including Greg Berlin, Bil Herd, and Bob Russell. For some reason, unknown to anyone but Smith, he playfully slapped Herd in the face. Herd, who had also been drinking, replied with a slap of his own, but his right hand was in a plaster cast and his slap carried significantly more impact. As Smith staggered back under the weight of the blow, Herd simply said "Don't do that again." Smith said nothing, and Herd was never disciplined for hitting his boss. "Drinking and slam-dancing, that's about the only thing I think [Smith] was qualified to do," recalled Russell.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/cbm_lcd.jpg" alt="The Commodore laptop that never was. Image courtesy old-computers.com">
			<br>
			<i>The Commodore laptop that never was. Image courtesy old-computers.com</i>
			<pre>
Smith certainly wasn't a good businessman. At the January 1985 CES, Commodore had shown off an innovative portable computer using an LCD screen. The laptop computer had a display that could show 16 lines of 80-column text, which compared favorably to the then-popular Tandy Model 100's 8 lines and 30 columns. Commodore took orders for 15,000 units of the machine just at the show itself, and it looked like it would be a smash success. That was when the CEO of Tandy/Radio Shack took Marshall Smith aside and told him that there was no money in LCD computers. Smith not only canceled the machine, but sold off Commodore's entire LCD development and manufacturing division, based solely on this dubious "advice" from his competition! Commodore had a chance to take an early lead in the emerging market of portable computers. Instead, the company would never produce a laptop again.

The man intended to replace Marshall Smith was Thomas Rattigan, an executive from Pepsi who had once worked with then-Apple CEO John Sculley. Like Sculley, he knew little about computers when he arrived at his new company, but he was a good listener and learned quickly. In late 1985, Rattigan was given the title of Chief Operating Officer (COO), reporting to Marshall Smith. Smith continued to bungle almost everything, and finally in February 1986 he was let go and Rattigan became President and CEO of Commodore International. He was given a five-year contract that was set to expire on July 1, 1991.

At last Rattigan could take on the task of righting the sinking ship that was Commodore. He had an ambitious plan that involved tackling every problem that plagued the beleaguered computer company. Firstly, to stop the bleeding and restore the company to profitability, he would cancel irrelevant projects, sell off unimportant divisions, and be brutal about laying off employees. Secondly, he would push for a redesigned and cost-reduced Amiga that could be sold at a lower price and allow Commodore to reenter the home consumer market that it once dominated with the C-64. Lastly, he would make a serious attempt to capture the more profitable high-end market by making a new Amiga that was more powerful and expandable than the 1000.

Rattigan would end up succeeding in every part of his plan. He would bring Commodore back from the brink of bankruptcy and back into profitability. He would reinvigorate the Amiga platform by splitting it into low-end and high-end models, each with different market possibilities. He would even preside over a new set of advertisements that, for the first time, properly showcased the power of the Amiga.

For all this effort, which Rattigan would achieve in a little under a year and half, he would be rewarded not with a pay raise, a promotion, or even a pat on the back. Instead, Rattigan would be kicked to the curb, fired before he had even run out his contract. In his place would come vampires, creatures dedicated not to the success of Commodore or the Amiga, but in sucking them both dry until they turned into dust.
</pre>
		</div>
		<div class="content" id="6">
			<h1>A history of the Amiga, part 6: stopping the bleeding</h1>
			<h2>Chopping heads</h2>
			<pre>
When a corporation is bleeding money, often the only way to save it is to drastically lower fixed expenses by firing staff. Commodore had lost over $300 million between September 1985 and March 1986, and over $21 million in March alone. Commodore's new CEO, Thomas Rattigan, was determined to stop the bleeding.

Rattigan began three separate rounds of layoffs. The first to go were the layabouts, people who hadn't proven their worth to the company and were never likely to. The second round coincided with the cancellation of many internal projects. The last round was necessary for the company to regain profitability, but affected many good people and ultimately may have hurt the company in the long run. Engineer Dave Haynie recalled that the first round was actually a good thing, the second was of debatable value, and the last was "hitting bone."

Under Jack Tramiel, Commodore had embarked on a whole host of projects: some practical, some far-sighted and visionary, and others just plain crazy. To try and figure out which was which, Rattigan looked for an experienced opinion. He found one in Charles Winterble, a former Commodore engineer turned consultant who at the time was still under the lawsuit Commodore had going against Atari!

First to the chopping block were Commodore's aging line of PET computers, which had been the first fully-assembled computers to hit the market (they predated both the Apple ][ and TRS-80). The VIC-20, once promoted by William Shatner, was also axed. Next up were the innovative but ultimately pointless collection of small 8-bit computers that were incompatible with the blockbuster C-64: the Plus/4 and Commodore 16, and various other machines that had never left the prototype stage. The Commodore 900, an innovative Unix workstation with a 1024 by 800 bitmapped display, was also canceled.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/c-filing.jpg" alt="How many megabytes does that cabinet hold?">
			<br>
			<i>How many megabytes does that cabinet hold?</i>
			<pre>
Computers weren't the only thing that Tramiel had a hand in. At the time, the company still owned an office supply manufacturing firm in Canada. I personally ran into Commodore-branded filing cabinets far more often than I ever ran into Amigas. Rattigan got rid of these and other distractions.

Rattigan also cleaned up the sloppy accounting processes that had been allowed to fester under his predecessor, Marshall Smith. Three redundant manufacturing plants were closed, and new financial controls were put into place to keep a tight check on spending.

In all, the cuts did their job. Commodore paid off their debts and even posted a modest $22 million profit in the last quarter of 1986.

In the meantime, the Amiga still needed applications if it was to become anything more than a curiosity. One of the first companies to publicly pledge support for the platform was none other than Electronic Arts.
</pre>
			<h3>Electronic Arts and Deluxe Paint</h3>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/trip-a1000.jpg" alt="EA founder Trip Hawkins poses with an Amiga 1000">
			<br>
			<i>EA founder Trip Hawkins poses with an Amiga 1000</i>
			<pre>
Those who have firsthand experience with the modern Electronic Arts typically know it as a faceless corporate behemoth, infamous for absorbing, then strangling independent development teams, eliminating competition by paying for exclusive rights to major sports leagues, and working its employees beyond the breaking point. They may be surprised to find out that EA originally had quite a different mission and philosophy.

EA's founder, Trip Hawkins, was actually fighting against the poor treatment of programmers that he witnessed elsewhere in the industry. When he launched Electronic Arts in 1982, he envisioned an environment where developers and game designers would be treated like rock stars: promoted in major media, given generous royalties, and allowed to explore wherever their imagination and talent led them.

Hawkins saw the Amiga as a groundbreaking platform, a brand new canvas that would let his developers create great new works of art. In November 1985, he took out a two-page ad in Compute! magazine that extolled the Amiga's virtues and promised that Electronic Arts would be supporting the platform for a whole new generation of games. "I believe this machine, marketed and supported properly, should have a very significant impact on the personal computer industry," Hawkins said prophetically in an earlier interview in the same magazine.

EA's first Amiga product, however, wasn't a game at all, but a game development tool. Programmer Dan Silva had been working on an internal graphics editor that was code-named Prism. When the Amiga was released, he quickly reworked the program to take advantage of the new computer's stunning graphics capabilities. Even before it shipped, Silva was already working on the next version, which would contain many more advanced features.

This program was Deluxe Paint, and it launched the careers of thousands of computer graphic artists. With a simple interface featuring a toolbar on the right-hand side of the screen, Deluxe Paint was a powerful tool that could create not only static graphics, but also animation. This made it perfect for creating images for computer and video games, and for a long time Deluxe Paint was the industry standard for creating art for this medium, much like tools such as 3D Studio Max are today.

Even years later, as the PC gaming market began to eclipse the Amiga in terms of sheer size and number of titles, many game development studios still made their art using Deluxe Paint. Its native format, IFF, and animation format, IFF ANIM, are still supported by many graphics packages today. IFF ANIM files were compressed using delta encoding, resulting in smaller files. This was nearly 10 years before animation compression standards such as MPEG were released.

But back in 1986, the combination of an Amiga and Deluxe Paint was unbeatable. While Adobe's Photoshop on the Macintosh platform would eventually become the standard tool for creating two-dimensional graphic images, the Mac was still a monochrome-only computer at this point, and the PC could barely manage four colors even with a CGA graphics card. Again, the Amiga was ahead of its time.

The cover art for the Deluxe Paint II box featured an image of Tutankhamen that had been created inside the program itself. This image quickly became an iconic picture in the computer graphics industry. Even Commodore recognized the power of Deluxe Paint, using the Tutankhamen image on a new full-page ad that—finally!—stated the Amiga's advantages outright.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amiga_ad_tut.jpg" alt="Commodore Amiga ad, circa 1986. Image courtesy The Commodore Billboard">
			<br>
			<i>Commodore Amiga ad, circa 1986. Image courtesy The Commodore Billboard</i>
			<h2>Magazines</h2>
			<pre>
Around this time, the first print magazines covering the Amiga platform were starting to appear. The first such magazine was called Amiga World, started by publisher IDG. The premiere issue of the bimonthly magazine reached store shelves in late 1985, and featured the new Amiga 1000 on the cover.

For the second issue, Amiga World tracked down Andy Warhol, who had been one of the stars of the Amiga unveiling. Warhol was an enigmatic personality who ran a magazine called Interview, yet refused to give interviews himself. After brusquely turning down the Amiga World reporter's request for an interview, Warhol retreated to his office upstairs. The undaunted reporter followed Warhol into his office, and while the iconic artist began painting pictures on his Amiga 1000, the journalist started asking him questions anyway.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amigaworld1.jpg" alt="Andy Warhol on the cover of Amiga World">
			<br>
			<i>Andy Warhol on the cover of Amiga World. Image courtesy Amiga History Guide</i>
			<pre>
> "Do you like the Amiga? What do you like about it?" the reporter asked.

> "I love it. I like it because it looks like my work."

> "Do you think it will push the artists?

> "That's the best part about it. I guess you can... An artist can really do the whole thing. Actually, he can make a film with everything on it, music and sound and art... everything."

> "Why haven't you used computers before?"

> "Oh, I don't know, MIT called me for about ten years or so, but I just never went up... maybe it was Yale."

> "You just never thought it was interesting enough?"

> "Oh no, I did, uh, it's just that, well, this one was so much more advanced than the others."

Warhol was a genius at self-promotion, but his "interview" showed genuine enthusiasm for the Amiga computer. He expressed frustration at not having a color printer yet and talked about how cool it would be to have a graphics tablet and stylus to replace the mouse. These products were all in development, but Warhol wanted them now.

Celebrity endorsements were hardly new in the computer field, but here was something different: a celebrity artist who was a genuine user and enthusiast for the platform. Here was a market—albeit a small one—that could potentially be nurtured.
</pre>
			<h2>Repositioning the Amiga</h2>
			<pre>
Commodore marketing had positioned the Amiga 1000 as a business machine to compete directly with the IBM PC and its countless clones. This was probably not the best idea.

The average businessman is—let's face it—slow, stodgy, and a bit boring. They are often the last to adopt any new technology unless it can make a clear case for increasing the bottom line. A computer that could print dynamic 3D charts and graphs in color was not going to be useful to a businessman unless there was a whole supporting infrastructure around it: color printers, color overhead display panels, business presentation software, and so forth. This was not the case in 1986.

Thomas Rattigan didn't believe that the business market was the best place to try and sell the Amiga. "I think the price confused a lot of people," he said in a 1987 interview. "People seem to think that home systems are under $1,000 and business systems are over $1,000. I don't think the higher-end Amiga is going to go into accounting departments, but I do think it is going to go into areas where there is a degree of creativity, if you will." In this prediction Rattigan was bang-on.

Rattigan believed that the best strategy was to split the Amiga 1000 into two products: a low-end model to take on the huge home market that had been dominated by the Commodore 64, and a high-end computer that would appeal to graphic artists—like Andy Warhol—who were interested in expanding their system.
</pre>
			<h3>The low-end: The Amiga 500</h3>
			<pre>
The Commodore CEO wasn't the first one to make the case for a cheaper Amiga. Hardware engineer George Robbins felt that a lower-end Amiga was a better idea right from the start, and Bob Russell said he had been fighting for such a product before the Amiga 1000 was even released. Still, it took someone higher up in the management chain to make the new machine—dubbed the Amiga 500—a reality. Rattigan had to choose between the remnants of the original Los Gatos crew who had designed the Amiga 1000 and Commodore's core group of engineers in West Chester, Pennsylvania. He chose the latter group because he felt they would be "more bloodthirsty" and thus likely to deliver the machine faster.

He assigned Jeff Porter, the engineer who had developed the innovative (but canceled by Rattigan's predecessor) LCD computer, to be the director of new product development. The lead engineers for the 500 were George Robbins and Bob Welland, who had previously worked on the also-canceled Commodore 900 Unix workstation. They were an odd bunch to be tasked with coming up with the computer that had to save the company, but in many respects they echoed the rogue team of misfits that had come up with the Amiga in the first place. George Robbins, a gentle and kind man with long hair and a walrus mustache, practically lived at work and often forgot to do his laundry. His coworkers, who loved Robbins, but worried about his personal hygiene, would constantly buy him new shirts to wear and quietly dispose of the old ones.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amiga500.jpg" alt="The Amiga 500">
			<br>
			<i>The Amiga 500</i>
			<pre>
Robbins needed to avoid the distraction of laundry duties, as he was intensely focused on cutting costs on the Amiga. Welland was the ideas man, while Robbins was the practical engineer who could take great ideas and turn them into working electronics. One of the ideas Welland had was to increase the RAM on the "Agnes" custom chip to 1MB so that the Amiga could support higher graphics resolutions. The original Los Gatos team was a bit miffed at the proposed changes to their design, which they felt were not revolutionary enough, and made it known that they didn't think the changes would work. This motivated the Amiga 500 team even harder.

"Fat Agnes" did end up working, and the original Amiga engineers admitted that the design was probably a good idea. The modest change increased the Amiga's capabilities while also keeping a high level of backwards compatibility with existing software. "It was a step in the right direction, but it violated the [original] idea of the bus architecture and actually slowed the machine down," RJ Mical said later.

Meanwhile, the pragmatic Robbins was finding ways to redesign the Amiga's motherboard to reduce costs. He took out the ability to connect directly to a television set and replaced it with a separate adapter, the A520. This turned out to be a good idea because most users weren't using a TV set anyway—the fuzzy image quality of TV sets caused text to "bleed" and made it hard to read. He took the power supply out of the main machine and integrated the keyboard into the case, which was inspired by the design of the Commodore 128. The 3.5 inch floppy drive was fitted in on the right-hand side of the machine. An thin expansion slot was placed on the other side. Devices could be plugged into this slot directly without removing the Amiga's case.
</pre>
			<h3>The high-end: The Amiga 2000</h3>
			<pre>
While the 500 project continued, Commodore needed people to work on the high-end Amiga 2000 design. Unfortunately, Rattigan's massive personnel cuts had left few engineers available for the project. The task was farmed off to Commodore's German subsidiary, but the engineers there simply took the original Amiga 1000 design, added a hardware interface for adding expansion cards, and put the whole thing in a standard PC desktop case. This wasn't quite what Rattigan was looking for.

The task of redesigning the Amiga 2000 fell on one Dave Haynie, whose broad shoulders and ever-broadening ego were large enough to carry this burden. "I was the design team for the A2000," Haynie said. "That's kind of the way things were there because we had a lot of layoffs. I was working day and night and there still wasn't enough time to do everything." Haynie would work through the week, then let off steam on Friday by retiring to Margaritas, a local dive where the beer was cheap and plentiful.

Haynie was inspired by the designs made by the Los Gatos team and determined to improve on their elegant architecture. He designed a new custom chip, called Buster, to handle the expansion bus. The bus design, which was called "Zorro" in reference to one of the original Amiga prototypes, was also ahead of its time. Unlike the ISA slots in the IBM PC, the Zorro slots had "autoconfig" built-in and would allow expansion cards to be work instantly without any manual configuration of jumpers or resolving device conflicts.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/amiga2000.jpg" alt="Built like a tank: the Amiga 2000">
			<br>
			<i>Built like a tank: the Amiga 2000</i>
			<pre>
Haynie wanted to design a machine that would be easy—for both end users and Commodore itself—to upgrade to more powerful processors that were coming out of Motorola's design labs. He put the CPU on a separate board that could be swapped out later. From the German designers he got the idea of a genlock—a way to directly output computer images on top of video with no loss of image stability. He turned this idea into a separate dedicated video slot, which could be fitted with a genlock card or other types of video processing cards. This idea would later turn the Amiga—already a multimedia powerhouse—into the standard computer for the video industry.

The Amiga 2000 would have unprecedented expandability, with five open Amiga Zorro expansion slots, four IBM PC ISA slots, and the aforementioned CPU and video slots. This was to be a serious machine, for serious users. The case was recycled from the canceled Commodore 900 workstation project.

Not everybody liked the idea of the 2000. Amiga creator Jay Miner, when asked about the machine at an Amiga user group meeting, recommended that Amiga 1000 owners hang on to their existing computer instead of upgrading. Jay's feelings weren't all about sour grapes. He felt that the computer had not been improved enough, given the advances in technology that had occurred since he first started designing the original Amiga back in 1982.
</pre>
			<h2>Rattigan's fall</h2>
			<pre>
Jay Miner had a point. Time had been passing swiftly since the Amiga launch in 1985, and he wasn't the only one getting frustrated. Irving Gould, the enigmatic financier who controlled Commodore at a distance, started voicing concerns that the Amiga 500 and 2000 were taking too long to arrive.

Gould, like many bosses before and after, was asking for the impossible. Making Commodore profitable was the first priority, and Rattigan had done that by slashing the payroll. Creating a more popular successor to the Amiga 1000 was the next priority, and the few remaining engineers were doing what they could with very limited resources.

The Amiga 500 and 2000 delays weren't the only fault that Gould could find with Rattigan. He accused his CEO of behaving "in a high-profile manner" in an interview with the Philadelphia Inquirer, a spurious charge if there ever was one. Rattigan's "high profile" consisted of doing a couple of magazine interviews. In one of these, the reporter asked him how he felt about being so little-known compared to other computer industry CEOs like John Sculley at Apple. He replied that he didn't think it was important to be well-known when your company was losing money.

Rattigan knew that he could not win in a battle with Gould, who owned six million of Commodore's 30 million shares. For his part, Gould was a slippery opponent. He rarely came into the Commodore offices, preferring to spend his time phoning various employees, trying to dig up dirt on his own CEO.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/irving_gould.jpg" alt="Irving Gould
				">
			<br>
			<i>Irving Gould</i>
			<pre>
In April 1987, Gould hired the management consulting firm Dillon-Read to prepare a report on Commodore. Consulting firms have a long and inglorious history of charging outrageous fees just to have their junior-level employees issue urgent recommendations for more consulting, all billed by the hour. This particular firm was no different, but the Dillon-Read consultant who prepared the report had an even less altruistic purpose in mind.

His name was Mehdi Ali, and legions of Commodore employees and Amiga owners would one day learn to rue his name.

The report suggested that Rattigan be immediately replaced, something Gould was more than happy to carry out. He called a board meeting, specifically excluding his CEO from attending. Rattigan knew that the game was up, but decided to stick it out to the end, and showed up for work the next morning.

The guards had been ordered not to let him on the premises, but pretended that they hadn't heard these instructions. "What the hell am I going to do?" one of them said. "The guy is running the company and turned it around, and I'm going to stop him from entering? Are you crazy?"

The locks on his office door had been changed. Rattigan was met in the hallway by an army of lawyers, who informed him that he was no longer employed at Commodore. He asked what the basis was for his termination, but the lawyers could give him nothing but meaningless gibberish. Resigned to his fate, Rattigan allowed himself to be escorted out of the office. Standing in the parking lot, he took a look back at the company that he had saved, and wondered where it had all gone wrong.

Gould had won, but it was a pyrrhic victory at best. He had lost the best CEO he ever had, and worse still, had broken a legally binding contract to do so. Rattigan sued for breach of contract and $9 million of unpaid wages. Commodore immediately countersued for $24 million. The case wasn't settled until 1991, which was ironically the expiry date of Rattigan's original five-year contract. Rattigan won, and Commodore's countersuit was dismissed.
</pre>
			<h3>The Amiga strikes back</h3>
			<pre>
So what had Rattigan accomplished? He had stopped the bleeding, made Commodore profitable, and made possible the projects that would bring the Amiga into its golden age: the A500 and A2000. Both models were released within a couple of months of Rattigan's termination.

What the Amiga could have done had Rattigan been allowed to stay is another of the many "what if" stories that pepper the Amiga tale, but it is what he did while he was there that mattered. By saving Commodore, he allowed the Amiga to survive, and in its new high-end and low-end forms it would find sales successes that the Amiga 1000 could only dream of.

And because of these new models, the story of the Amiga split also. No longer was it just about the original creators, or the struggles of a company trying to introduce a revolutionary new technology. From now on, the Amiga tale would be about its users: a diverse group of people who found the platform in different ways and took it in different directions.

Amiga was now about the gamers, about the bulletin board users, about the demo coders, about the hackers, and about the graphic artists, the animators, and the movie and television creators. It was now about the Amigans.
</pre>
		</div>
		<div class="content" id="7">
			<h1>A history of the Amiga, part 7: Game on!</h1>
			<h2>The most powerful gaming platform</h2>
			<pre>
The Amiga started out its life as a dedicated games machine, and even though it grew into a full computer very quickly, it never lost its gaming side. The machine's 4096-color palette, stereo sampled sound, and graphics acceleration chips made it a perfect gaming platform, and it didn't take long for game companies to start taking advantage of this power.

While the slow sales of the Amiga 1000 limited the number of games that developers were willing to make for the platform, when Commodore released the low-cost Amiga 500 in 1987, everything changed. Now the most powerful gaming computer was also one of the cheapest, and game companies jumped at the chance to showcase their talents on the Amiga.
</pre>
			<h3>Mind Walker (1986)</h3>
			<pre>
One of the first games ever released for the Amiga was a quirky gem called Mind Walker, written by Bill Williams and published by Commodore itself. Williams started his game design career on the Atari 800, writing classics like Necromancer and Alley Cat. His games were always unique, combining off-the-wall situations with innovative game play.

Mind Walker puts the gamer in the role of a physicist who has lost his mind. Instead of resorting to drugs or therapy, the protagonist of the game decided to send his split ego into the depths of his own brain. Your job is to navigate this surreal landscape and uncover paths leading to deeper and deeper levels, with the ultimate goal of finding the hidden key to save your sanity.

Your alter ego jumps around on brightly colored square platforms of varying height—fortunately, you can't fall off. If you reach the end of the screen it instantly loads the adjacent area. Floating gold balls try to zap you with deadly searchlights, but you can zap most of them with bolts of electricity that you direct with the deft skill of a Sith Lord. Over some squares hover strange pyramids that transform your avatar from a man into a red wizard, a flying bug-like alien, or a sexy seductress. Different forms are required to find different parts of the path, and keeping track of the whole operation requires careful consultation of the overhead map. If the character becomes hidden behind an overly-tall platform, the player can switch to one of four different views by hitting the letters N, S, E or W.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/Mindwalker.gif" alt="Mindwalker by Commodore.">
			<pre>
Once the path is complete, the game shifts to a three-dimensional view of a psychedelic tunnel. The player has to grab a translucent green door with his disembodied hands, which leads to the final level in which the character fends off fuzzy-looking "bad thoughts" to find the next piece of his sanity.

The game has simple but evocative graphics that make good use of the Amiga's built-in hardware polygon drawing and area fills. Bill Williams had been a composer before he became a game designer, and the music he created for Mind Walker has an eerie, lyrical quality to it that fits perfectly with the game's theme. The game uses stereo pan effects to let the lightning bolts seem to sear across the room. Like the Amiga itself, Mind Walker was unusual and thought-provoking.

Another unusual thing about the game was that it not only fit neatly on a single floppy disk, but it also had no copy protection and could be run directly from the Amiga's Workbench GUI. Furthermore, the game was multitasking-friendly, so you could easily run other applications in the background. Few Amiga games in the future would retain these qualities. Game developers, eager to squeeze out every last bit of power from the computer, would bypass the operating system and access the hardware directly. This allowed later titles to be much more graphically impressive, but at the cost of multitasking capabilities.

Bill Williams would continue writing games up until 1992, when corporate interference on the Super Nintendo title Bart's Nightmare (he referred to it as "Bill's Nightmare") caused him to leave the industry altogether and pursue a second career as a Lutheran pastor, picking up a master's degree in theology along the way.
</pre>
			<h3>Defender of the Crown (1986)</h3>
			<pre>
Cinemaware was started by Robert and Phyllis Jacob in 1985. Their goal was to create games that had style and presentation that were evocative of movies. This was an ambitious goal back when most video games were simple shoot-em-up or maze-chasing affairs, but the advent of the Amiga gave the small company a chance to realize their dreams.

Defender of the Crown was their first title, and it showcased the power of the new platform. The scene: you are a Saxon baron of an English fiefdom in the Middle Ages, and the king has just been killed without a clear successor. You must fight other Saxons and Norman invaders to conquer England and become the new king.

The game was one of the first to feature gorgeous hand-painted loading screens to set up the action, and the game itself was just as beautiful. Each turn begins with a stylized birds-eye view of Britain. From this menu, the player can choose to attack a neighboring county, stage a raid on an enemy castle, stage a jousting tournament, or occasionally stage a daring rescue of a beautiful maiden. Robin Hood pops up from time to time as a non-player character who can be either an enemy or an ally.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/Defender_of_the_crown.png" alt="Defender of the Crown by Cinemaware.">
			<br>
			<i>Defender of the Crown by Cinemaware.</i>
			<pre>
As in many games of its era, winning can be frustratingly difficult. In the raid screen, for example, you control a single fighter who must cut down enemy after enemy while his compatriots merely keep the rest of them away. Jousting is only slightly less difficult than the real thing, requiring a steady hand on the mouse to position your lance in the right position at the right moment. Winning a joust can gain your side honor points or even territory, depending on the initial stakes.

Defender of the Crown was an Amiga-only game at the outset, and was often used by dealers to showcase the platform to eager young gamers. Much of the credit for the game's success has to go to the game's artist, Jim Sachs. RJ Mical, who did some consulting work for the game, recalled his talent.

"Jim Sachs, what a god he is," marveled Mical. "Jim Sachs is amazing. These days everyone sees graphics like that because there are a lot of really good computer graphics artists now, but back then, 20 years ago, it was astonishing to have someone that good."

Because Cinemaware was a startup company running low on cash, and Defender was the first product, it was forced to release the game before it was completely finished. Later, ports of the game to the Nintendo Entertainment System, Commodore 64, IBM PC, and the Atari ST would fill out the missing parts, including a more substantial army attacking screen. The ports could not deliver the same sound and graphics quality of the Amiga version, however.

Cinemaware continued to publish innovative games until 1991, when over-extension and feature creep on the Cold War title SSI caused the company to declare bankruptcy. Two early employees of the company, Lars Fuhrken-Batista and Sean Vesce, got back together to create an updated version of the game called Robin Hood: Defender of the Crown for the PlayStation 2, Xbox, and Windows PCs in 2003.
</pre>
			<h2>RPGs come to the Amiga</h2>
			<h3>Faery Tale (1986)</h3>
			<pre>
Faery Tale is one of those games that everyone who played it remembers. An fantasy role-playing game by MicroIllusions that featured a top-down view, Faery Tale resembles classics like the original Legend of Zelda and the Ultima series, and contains a surprising amount of depth.

The game starts out by introducing the main characters via a virtual story book that slowly flips its pages. Three brothers, Julian, Philip, and Kevin have grown up in the small hamlet of Tambry in the land of Holm and are eager to explore the wider world. Julian, the bravest of the three, sets out first. The world, as in many RPGs, is inexplicably full of bandits, monsters, and undead creatures like skeletons. They often attack in groups that can easily overwhelm the player's character, especially with his initial armament of a small dagger. The action takes place in real-time, without turns or pauses, and surviving the game's early stages can be difficult.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/faery_tale.png" alt="Faery Tale Adventure by MicroIllusions.">
			<pre>
If Julian dies, a small fairy will resurrect him, but after a number of deaths he becomes a ghost. The player is now transferred to Philip, who can talk to Julian's ghost and recover items from his body. If Philip fails, the quest is taken up by Kevin, who is the player's last hope. Fortunately, the game can be saved at any time.

Characters have various statistics that can be improved with time and training, as in many RPGs. Bravery reflects the player's strength, and Vitality his hit points (which go up at a slower pace as Bravery rises). There are also Kindness points that are required to talk with certain non-player characters. Unlike many role-playing games, Faery Tale lets the player attack innocent non-player characters, although because they then stay dead this is rarely a good idea. The player must also make sure he has packed enough food for his long journey, as hunger will slowly drain his Vitality.

Objects on the ground can be picked up, and treasure obtained can be traded in for better weapons at the local shop. When the weapon is equipped, it is immediately visible on the player's character. Some items are magical, such as the Bird Totem that gives the player a birds-eye view of the map. There are colored keys to open certain locked doors, potions to restore health, and even trinkets to momentarily stop time in the heat of battle.

The game world is staggeringly large, and contains many surprises and twists, such as a giant turtle that the player can use to transport himself across the water. Later, to save the king's daughter from a horrible fate, the player must tame and fly a golden swan across an otherwise-impassable mountain range.

Despite having a fairly pedestrian fantasy plot (the player must accumulate five golden statues in order to open a portal to the Astral World and defeat the evil Necromancer) the game is still memorable more than twenty years later. I talked to my friend Domenico DiTomaso, and he recalled spending two happy months playing cooperatively with a friend to complete the game. "You could go anywhere," he said, marveling at the free-form game play that allowed the player to wander through the entire world without hitting a loading screen. "Just remember," he told me, "make sure you make a map when you enter the Dragon's Cave!"
</pre>
			<h3>Dungeon Master (1988)</h3>
			<pre>
Dungeon Master actually made its debut on the hated Atari ST platform a year before it was released for the Amiga. Because of this, the graphic quality was somewhat less than the Amiga was capable of producing, but the 3D first-person view made this dungeon crawl stand out from its competition. Although the graphics were largely unchanged in the port, the Amiga version did make good use of the custom sound chips. The stereo sound made monster noises seem to "pop out" in three dimensions, an important clue when enemies could sneak up on the player from all directions. This advantage actually helped to sell Amiga 500s over Atari STs.

Dungeon Master was inspired by the crude three-dimensional graphics found on the Ultima series of games whenever the player entered a dungeon. By placing the entire game in that setting, the designers at FTL Games could concentrate on improving the 3D graphics and game play experience.

The game started out at the entrance to the dungeon, with only one direction of movement possible: go inside! In the first level, the disembodied player wandered around a "Hall of Champions" consisting of many different portraits of heroes hanging on the walls. Moving up close to a portrait caused the character to magically appear as part of your party: you could have up to four characters in total. Unlike other dungeon crawls, there was no other character creation process: you took the pre-defined adventurers as they were. When you were ready, you took the stairs down...
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/DungeonMaster.png" alt="Dungeon Master by FTL.">
			<br>
			<i>Dungeon Master by FTL.</i>
			<pre>
All the action could be controlled with the mouse, from turning and moving to picking up objects. Clicking on an object moved it into an empty hand of the currently-selected character;you had to open an inventory screen to move the object into a backpack. Excess inventory could be thrown with the right mouse button, and it would sail forward across the dungeon. There were many puzzles, hidden levers, and secret doors to unlock. Clues could sometimes be found in notes that were scattered around the top levels of the dungeon.

Combat took place in real-time, with the player required to manually switch between characters to take a swing or cast a spell at a monster. The spell casting system was quite innovative: for example, to cast a fireball, the player mixed a fire symbol with a wing symbol. If one of your characters died (this happened to me early on when falling through a trap door) you could pick up their bones and carry them around to a rebirth chamber.

There were 14 levels in total in Dungeon Master, and completing the last level involved slaying a demon master named Chaos, who looked like a cross between Darth Vader and Amadeus. Chaos could not be killed with normal weapons, and had to be trapped in a magical cage before he could be dispatched. There was a plot line hinted at in the game, and detailed in the manual, which was written by Nancy Holder, a novelist who has since written books for Buffy the Vampire Slayer, Sabrina, the Teenage Witch, and Smallville.

Dungeon Master inspired a ream of copycats, such as Eye of the Beholder and Captive. It was the primary inspiration for the groundbreaking 3D masterpiece Ultima Underworld.
</pre>
			<h2>Vertical scrollers, too</h2>
			<h3>Xenon II (1989)</h3>
			<pre>
Xenon II was a sequel to the popular, vertical-scrolling shoot-em-up game written by a company of Amiga fans called The Bitmap Brothers. Sequels were good to the Bitmap Brothers; the company's second version of its futuristic arena handball game Speedball was a huge commercial success and is remembered fondly to this day.

Before the advent of 3D shooters, one of the most popular types of game was the 2D scrolling shoot-em-up, a game usually set in space where the player controlled a single ship that was pitted against an endless fleet of oncoming enemy craft who couldn't shoot very quickly. The first arcade games had limited processing power and usually set the player against a simple backdrop of stars. Later games had more detailed backgrounds that could become obstacles all by themselves.

Xenon II's backgrounds and enemies were largely inspired by the arcade megahit R-Type, which set the player against a strange and somewhat disgusting array of space-faring worms and other ugly-looking creatures. Xenon II had space worms and giant space trilobites to go along with the more standard-looking enemy space ships. Unlike R-Type, which scrolled from right to left, Xenon II kept the scrolling old-school and vertical. One difference: the player could scroll backwards for a short distance in a pinch.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/XenonII.jpg" alt="Xenon II by The Bitmap Brothers.">
			<br>
			<i>Xenon II by The Bitmap Brothers.</i>
			<pre>
At the end of each level, the player had the opportunity to visit a shop, tended by a cranky old alien. He would give some advice about each power-up available for purchase, but if you pestered him for too long, he would snark back: "What, do you want me to play the game for you too?"

Xenon II didn't contain any brilliant innovations or redefine the 2D scrolling shooter genre, but it did show that the Amiga was capable of delivering arcade-like experiences at home.
</pre>
			<h3>Shadow of the Beast (1989)</h3>
			<pre>
While most of the Amiga games up until this time had been superior to ports on other computers, there still wasn't a game that conclusively blew away the competition and left no doubt about which was the superior game platform.

That is, until Psygnosis released Shadow of the Beast. A side-scrolling platform game in the vein of Super Mario Bros., Shadow of the Beast pushed the Amiga graphics chipset to its limits.

Back before 3D graphics technology, side-scrollers would often use a technique called parallax scrolling, where images in the background scrolled more slowly than those in the foreground to give the illusion of movement in a large world. Few consoles at the time had the power to scroll backgrounds at all (Super Mario had static backgrounds) but some arcade games would have two or maybe three levels of parallax scrolling. Shadow of the Beast had up to twelve.

The enemies were no slouches either. Unlike the tiny sprites of other side-scrollers, monsters in Shadow of the Beast could fill up to half the screen.

Shadow of the Beast had an intriguing back-story. The game's protagonist was a man named Aabron who was kidnapped as a child by the evil beast lord Maletoth, and twisted through evil magic into a horrific man-beast to serve his new master. When this creature witnesses a man being executed, he remembers the man as his human father and his childhood memories come flooding back to him. Escaping from Maletoth, he is determined to seek his revenge.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/ShadowoftheBeast.jpg" alt="Shadow of the Beast by Psygnosis.">
			<br>
			<i>Shadow of the Beast by Psygnosis.</i>
			<pre>
Finishing the game's 12 levels was a frustratingly difficult task. The Beast, while powerful in his own right, seemed to be constantly on the edge of death. Not only were there other monsters to deal with, but the Beast also faced an endless barrage of deadly spike traps that rose from the ground, flying squadrons of spiked balls, and even giant floating eyes. The Beast started with 12 units of health, and each touch of an enemy would reduce his reserve by one. If it fell to zero, the game was over.

The graphics weren't the only part of the game that stood out. "What I remember foremost about Shadow of the Beast is the music," said Amiga owner Narendar Ghangas. "The game had a foreboding sense of atmosphere throughout and the moody strings really suited the dark nature of the game. I remember being totally captivated by the synthesized music—it was haunting."

While some panned Shadow of the Beast for putting graphical eye-candy over depth of game play, the game itself was a commercial success, and was later ported to platforms such as the Sega Genesis (minus much of the color palette and several layers of parallax scrolling). It also spawned two sequels, the last of which could only be found on the Amiga.
</pre>
			<h3>Lemmings (1991)</h3>
			<pre>
If there was a single game that could represent the Amiga experience, it would have to be Lemmings. Released by Psygnosis in 1991, it was quirky, fun, and addictive. Players controlled a large number of colorfully-clad, green-haired lemmings, who needed help getting from the start to the end of each level.

Without the user's assistance, the poor lemmings would usually walk straight off a cliff to their doom. Fortunately, the player could, with a click of the mouse, give certain key lemmings specific "jobs". One important job was the "halt" lemming, who stood with hands outstretched and flicked his head back and forth, causing any lemmings to reverse their direction when they ran into him. An umbrella-wielding lemming would sail softly down to the ground instead of falling to his death. There was even a suicidal lemming option, who would count down from five to zero, squeak "Oh no!" and then explode. Sometimes this sacrifice was necessary, other times it was just fun.

Other lemmings could be tasked as diggers, or to build ramps to help the rest of the group reach inaccessible locations. Having all these options would make completing any level a trivial exercise, but there was a catch: each level gave the player a limited number of jobs to hand out, and not all jobs were available on all levels. If the user got really frustrated, there was always the "nuclear" option: setting all lemmings to count down from five all at once. The resulting chorus of "oh no"s and subsequent total destruction was strangely cathartic.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/Lemmings.png" alt="Lemmings by Psygnosis.">
			<br>
			<i>Lemmings by Psygnosis.</i>
			<pre>
Lemmings was incredibly popular, and the game became a symbol of sorts for the Amiga community. Gail Wellington, the director of Commodore Advanced Technical Support (CATS), once arranged for a whole group of Commodore employees to dress up as lemmings for a trade show. They had the whole thing covered: the purple outfits, the green hair, the appropriate stances, an umbrella, and even balloons filled with confetti for the inevitable "Oh No!" finale. They worried a bit about this last part: what would the people who had to clean up think of such a stunt? It turned out that their fears were unfounded: the cleaning staff was more than happy to tidy up the mess after being so thoroughly entertained.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/archive/journals/microsoft.media/Lemmings2.jpg" alt="Commodore employees dress up. Photo courtesy stevex.">
			<br>
			<i>Commodore employees dress up. Photo courtesy stevex.</i>
			<pre>
While Lemmings was ported to other platforms, most notably the IBM PC, the Amiga version had superior sound and even some game play options that weren't available anywhere else: two players could play at once with each using a mouse, thanks to the Amiga's unique ability to have two rodents connected at the same time.

In 2007, Sony released a new version of Lemmings to its PlayStation Portable (PSP) system. With multiple ports to choose from, they decided on the Amiga code base as the basis for the game. Now a whole new generation could experience the delights of pushing little green-haired creatures around.
</pre>
		</div>
		<div class="content" id="8">
			<h1>A history of the Amiga, part 8: The demo scene</h1>
			<i>Where else could you have "a hand in creating things that were so damn cool?”</i>
			<h2>Genesis</h2>
			<pre>
As computer games became more and more complex in the late 1980s, the days of the individual developer seemed to be waning. For a young teenager sitting alone in his room, the dream of creating the next great game by himself was getting out of reach. Yet out of this dilemma these same kids invented a unique method of self-expression, something that would end up enduring longer than Commodore itself. In fact, it still exists today. This was the demo scene.

The genesis of the demo scene started with the Apple ][ in the late 1970s and fully formed with the Commodore 64 a few years later. It started with the battle between game developers and pirates. Companies like Sierra would add newer and cleverer copy protection to their wares to prevent copying, and this challenged the pirates (who were mostly teenagers) to crack the protection. If you were the first to crack a game, you wanted to show off your feat to your friends, so crackers would add a byline with their pseudonyms on the game’s loading screen.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2013/04/crack451.gif" alt="An Apple ][ crack screen from the early 1980s.">
			<br>
			<i>An Apple ][ crack screen from the early 1980s.</i>
			<pre>
Friendly competition between cracking groups led to an artistic arms race. Instead of just modifying the loading screen, groups started to create their own “intros.” These were little animations that would scroll the names of the group’s members, perhaps with a little music in the background. As the intros got larger and more complicated, they started to rival the size of the games themselves. Eventually, some groups stopped doing the cracking altogether and just packed a single floppy disk with as much of their art, animation, and music as they could. These were the first demos.
</pre>
			<h2>Creating the demo scene</h2>
			<pre>
Demos required the participation of multiple people, including artists, musicians, and coders, much like a small game studio. Unlike games, however, demos did not (at least at first!) earn their creators any money. For the teenage crackers who became demo creators, this wasn’t a huge issue. Creating a great demo wasn’t about getting paid. It was about recognition from your peers, about amazing your friends by showing them something they didn’t think could be done. This was something greater than money. It was empowering and addictive.

The first demo groups cut their teeth on the Commodore 64, an everyman’s machine that ended its long life after selling 22 million units. When the Amiga arrived, powered by the Motorola 68000 chip, it represented a quantum leap in power over its 8-bit cousins the Commodore 64 and Sinclair Spectrum. Demo groups flocked to the new machine with its greater color palette and superior sound. The Amiga’s custom chips were just begging to be explored by clever coders banging directly on the hardware using assembly language for superior speed.

Although the Amiga 1000 was released in 1985, it wasn’t until the more inexpensive Amiga 500 came out in 1987 that kids interested in demos could afford one. Trefor James was one of them.

“I started out with a BBC Model B and then moved onto a Commodore 128,” he explained to me. “When the Amiga 500 first came out, I knew I simply had to have one. It was so hugely ahead of anything else that was around at the time. It changed everything.”

A friend at school gave Trefor some pirated games, most of which had intros from the cracking group on the front of them. He loved the idea of a group of guys (and they always seemed to be guys) who dedicated themselves to the challenge of removing copy protection. He also was intrigued by the idea of scrolltext, where you could communicate with others doing the same thing. A friend, already in the demo scene under the name “Count Zero,” told him that if he liked intros he would love demos. Count Zero then gave him a stack of 3.5-inch floppy disks.

Most of the demos contained phone numbers in the scrolling text in the end credits, and Trefor called some of them up, not knowing what to expect. He found that they were “a bunch of really cool guys” who were just as excited about computers as he was. Before he knew it, he was part of the scene as a mail trader.

In the early days, the primary method of distributing demos was by mail. This proved somewhat impractical as the scene got larger, so trading moved to dial-up modems and bulletin board systems (BBS). This led to a new problem: long-distance phone bills. Traders could have up to 150 contacts with whom they swapped software. Trefor said that monthly phone charges of £400 (more than $1,100 in today’s currency) were not uncommon. For a teenager, it was hard to sustain these bills for very long.

Some people sold their computers to cover the costs, thus exiting the scene forever. Others started trading hacked calling cards. “Virgin” cards were highly sought-after currency as they could be used for up to six to eight weeks. AT&T and MCI were the most common as they provided toll-free 800 numbers in most countries. The top cracking and demo scene BBS would trade in valuable text files describing how to build blue boxes and other “phreaking” tips for hacking the phone exchanges.

Before the Internet, these phreakers were building their own international communications network. Demo group collaboration sometimes extended over oceans. Arctangent, a 19 year-old computer artist living in the US, had contributed some graphical screens for an electronic scene magazine called “Grapevine.” One night at 2am his phone rang. He heard the distant, tinny voice of a British woman asking if “Aaah-tangent” would like to join the demo group LSD. Initially he thought it was a prank call, but they phoned back and insisted that he “Just say yes.”
</pre>
			<h2>Running a demo group</h2>
			<pre>
Keeping all these far-flung groups of enthusiastic teenagers organized took a very special kind of management. When Count Zero asked Trefor to join the group Anthrox, he jumped at the chance. It wasn't a large group (he was their fifth member) and when Count Zero started winding down his activities, Trefor ended up taking on some of the group's management duties. Much of this activity consisted of finding smaller groups and “consuming” them for their talent. A good way to grab an artist or a coder from a rival group was to bribe them with an offer of a cheap modem. It was a tricky balancing act. Artists and musicians wouldn’t join groups without good coders, and the coders wanted the best artistic talent to make their algorithms shine.

Disputes between groups were inevitable, and while they never got truly nasty, sometimes bad feelings would leak out into the demos themselves. Along with the “greets” to other groups displayed in scrolling, bouncing text, more nasty messages could sneak in. Occasionally a group would release a sub-standard demo “on behalf” of another group they were fighting with. These arguments may seem petty with the benefit of age and hindsight, but they mattered a great deal to the people involved at the time. Sure, in some sense they were typical teenagers, with cliques, in-jokes, and plenty of swearing. But they were also very atypical teenagers, capable of writing highly tuned assembler code that talked directly to the Amiga’s custom chips and made them sing.
</pre>
			<iframe src="https://www.youtube.com/embed/MH58Dud-DSg" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br>
			<i>Animotion demo from the group Phenomena in 1990</i>
			<pre>
Running a demo group was basically a job that you paid to do. So why would anyone do it? Trefor provided the answer: “What else could you do at such a young age that would let you talk to and meet people from all over the world and have a hand in creating things that were so damn cool?”
</pre>
			<h2>The technology behind the demos</h2>
			<pre>
Demos are, at their heart, a way to showcase the artistic, musical, and coding talents of their creators. As such, each demo was written from scratch by the team members. Sometimes artists would argue about the particulars—was it acceptable to draw images on paper and then scan them in, or should all art be created entirely digitally on the computer?—but generally everyone agreed that all the content should be original. That was part and parcel of the whole exercise.

In general, the graphical effects were rendered in real time when the demo was run, rather than being pre-rendered over many weeks and then played back as an animation. Partly this was due to the technology of the time—you couldn’t fit much animation on a 3.5-inch floppy disk—but mostly it was about creating amazing new effects that nobody had seen before. Demo groups mystified the audience about how these pieces were generated. Pre-rendering would have ruined all that.

Some early effects included the aforementioned scrolling text, making graphics ripple and wave, blasting large numbers of sprites around the screen, dynamically shifting colors in a plasma effect, or creating a simulated 3D effect using dots or balls.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2013/04/320px-Vertical_Raster_Bars.png" alt="Vertical raster bar effect.">
			<pre>
Today, with 3D graphics chips being powerful and ubiquitous, it’s easy to become jaded about graphical effects. But back then, 3D graphics cards didn’t exist. Getting 3D effects out of a 7.1MHz CPU took a combination of tightly coded assembly language programming with some creative tweaking of the Amiga’s custom chipset.

The fact that the effects were generated in real time made demos essentially like little video games that ran automatically without user intervention. As such, it seemed like a natural conversion for demo coders and artists to move into the game industry. Many of them did. Some game studios absorbed the demo culture wholeheartedly. Psygnosis, who produced some classic Amiga games like Shadow of the Beast, were known for their graphical prowess and even their logo resembled the highly stylized fonts used in demos.

However, the transition from demos to games wasn’t automatic. Demos, after all, didn’t have to worry about gameplay or even leaving any CPU cycles over for game logic. Some of the demo members were into the scene for the purity of the challenge and saw making games as “selling out." And of course, there was natural attrition as scene members got older and got regular jobs, discovered girls, or both. Despite all this, the demo scene continued to grow.
</pre>
			<h2>Demo parties</h2>
			<pre>
Creating demos and distributing them over the world via mail and electronic bulletin board systems was fun, but it lacked a certain something. Every demo group member wanted to prove that they were the best, but how could you do that? The only way to know for sure was to gather all the best demo groups into the same room and let the audience judge for itself. And thus the demo party was created.

The first parties evolved, like the demo scene itself, out of pirating. “Copy parties” brought people with the same computers together for an afternoon of discussion and trading software. A typical party was one like the monthly meeting of the PaNoRaMa Commodore Computer Club, which at its height had more than 1,500 people coming to meetings held in a lecture hall at Simon Fraser University. On the main table, a stack of daisy-chained floppy drives droned constantly thanks to the Amiga’s multitasking prowess. As time went on and law enforcement found out about these parties, they shifted away from copying (at least out in the open) and more toward demos.
</pre>
			<iframe src="https://www.youtube.com/embed/fuCotkVGVug" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br>
			<i>A documentary showing the Anarchy Amiga demo party in 1992.</i>
			<pre>
Demo parties started out as small gatherings of friends but soon grew into much larger events. As they became more serious, it felt natural to award prize money to the winning contestants. The money came not only from entrance fees but also from corporate sponsorships. Companies ranging from local businesses to industry powerhouses like Intel and Nvidia advertised at demo parties. The amount of cash raised varied from a few hundred dollars to more than $50,000 for the largest parties.

One of the most popular demo parties (and one that is still around today) is Assembly, an annual three-day event held in Finland. The first Assembly was held in 1992 and was organized by the Amiga demo groups Complex and Rebels and the PC demo group Future Crew. Each year the party got a little bit bigger until, by 2002, they had to rent out an entire hockey arena to fit everyone.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2013/04/Assembly2004-areena01-640x480.jpg" alt="The Assembly demo party in Finland, 2004">
			<br>
			<i>The Assembly demo party in Finland, 2004</i>
			<h2>Amiga versus PC</h2>
			<pre>
In the mid-to-late 1980s, the Amiga was the undisputed champion of the demo scene. Its custom chips, multiple graphics modes, and 16-bit processor ran rings around PCs. An Amiga could display 4096 colors at 360×576 resolution while simultaneously playing four-channel stereo digital sound without taxing the CPU. A typical PC of the same time mostly ran in text mode or, if you were lucky, displayed at 320×200 in four glorious colors while sometimes managing to emit a solitary beep.

In the early 1990s, however, this began to change. Mostly this was due to advancing PC hardware, particularly the new VGA standard that could display 256 colors out of a palette of 256,000. PC hackers like Michael Abrash dug deep into the VGA internals and found undocumented features like Mode X, a method of programming that allowed for smooth scrolling, something previously considered impossible on PCs.

The release of the 386 chip and its cheaper 386SX cousin also propelled the PC forward in pure processing power. Games like Doom and Wing Commander showed some of the amazing almost-3D effects that you could do purely on a 386.

The Amiga and PC scenes quickly formed a rivalry. The Amiga folks were suspicious of the PC upstarts and disliked the inelegance of the PC architecture. The PC people felt that the Amiga was languishing in the past. Sometimes this rivalry extended to the demos themselves. Amiga demos would make fun of the PC in scrolltext, while PC demos would flash messages saying “it’s dead”— and everyone knew what they were referencing.

The rivalry still lasts to this day, but it has become more of a friendly one. Many of the demo scene participants are getting older and are more interested in socializing than fighting. Many demo parties separated Amiga demos and PC demos into distinct categories to avoid any cross-platform competition. Some, however, still bundled all demos into one big pot. In these competitions, whenever an Amiga demo won, there was loud cheering and energetic sign-waving from the Amiga fans. The last time this happened was in Assembly 2006, when Starstruck from the group Black Lotus took home the grand prize. It’s still an impressive work of art today:
</pre>
			<iframe src="https://www.youtube.com/embed/PDrICN4UJoA" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br>
			<i>The Amiga demo Starstruck from Black Lotus wins Assembly 2006.</i>
			<h2>Where they are now</h2>
			<pre>
Many demo scene members went on to interesting industry jobs in graphics, animation, or software development. All of them remember their Amiga demo days fondly.

Arshad Rahman lived in Toronto in the 1980s. He was part of a small local Commodore 64 cracking scene that moved on to the Amiga. While the North American demo scene was a small fraction of Europe’s, with its massive Assembly demo parties, it was still a chance to create impressive new things and get noticed by the industry.

Arshad had written a cool 3D ball demo and showed it off at one of the Amiga Developer Forum’s monthly meetings. This caught the eye of the owner of a video game development company, and he hired Arshad immediately to work on a game. He ended up working on a high-end Video Toaster-based editing software package before joining ATI in 1995.

ATI released one of its first 3D accelerator cards in 1996, the Rage2. This card was somewhat derided at the time as a “3D decelerator” because the poorly optimized Windows drivers would often produce lower frame rates than simply using software rendering on the CPU (I had a friend who bought one of these cards at the time and we personally verified that it did actually slow things down). Arshad took the card and, returning to his Amiga roots, bypassed the drivers entirely and programmed the card directly on the “bare metal.” He was surprised to find that the card’s 2D performance was blisteringly fast, and he ended up porting a few of his old Amiga demos over to a Macintosh running a Rage2 card. The card didn't have all the custom chips that the Amiga did (such as the copper) but a significantly faster blitter chip combined with a fast CPU made it possible for a PC to finally catch up with and surpass what the Amiga had been doing for years on a 7MHz 68000.
</pre>
			<h2>The modern demo scene</h2>
			<pre>
While the demo scene still exists today, it has transformed somewhat from its early roots. Because virtually anything imaginable can be produced in real time with modern 3D video cards, there are “64k intro” and “4k intro” categories for groups that want to show off exactly how much visual wonder can be produced from a tiny amount of code. (The 64k is the maximum size of the executable file, but the demo may use as much RAM as it deems necessary).  They use extremely aggressive compression techniques and the modern PC's insane processing power and memory to generate a lot of content on-the-fly.

Razor 1911, a cracking and demo group with a long and storied history, produced a 64k intro ironically called The Scene is Dead for the Revision 2012 demo party. It started off with a view of an old AmigaOS boot screen viewed through a retro nostalgia lens and ended with 65,535 bytes of pure visual joy. YouTube provides a good approximation of the results, but the quality when running the code directly is still superior.
</pre>
			<iframe src="https://www.youtube.com/embed/IFXIGHOElrE" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<pre>
Much like the Amiga itself, the demo scene ignited people’s passions and imaginations. It showed what amazing things people willing to spend time and effort on their craft could produce with the help of a computer. In the end, it spawned careers and industries too numerous to mention. While at first glance the demo scene might seem to be a part of history, its continued existence is nothing less than a triumph of the human spirit.
</pre>
		</div>
		<div class="content" id="9">
			<h1>A history of the Amiga, part 9: The Video Toaster</h1>
			<i>Jeremy Reimer's long-running History of the Amiga series is back to tackle the killer app.</i>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2016/03/video-toaster-640x215.jpg" alt="">
			<pre>
When personal computers first came into the world in the late 1970s, there wasn’t always an obvious use for them. If the market was going to expand beyond hobbyists and early adopter nerds, there needed to be a “killer app”—some piece of software that could justify the purchase of a particular brand of computer.

The first killer app, VisiCalc, came out in 1979. It turned an ordinary Apple II into a financial planning tool that was more powerful and flexible than anything the world had ever seen. A refined version of this spreadsheet, Lotus 1-2-3, became the killer app that put IBM PCs in offices and homes around the world. The Macintosh, which floundered in 1985 after early adopter sales trailed off, found a profitable niche in the new world of desktop publishing with two killer apps: Aldus Pagemaker and Adobe Photoshop.

To keep up with the Joneses, the Amiga needed a killer app to survive—it found one with the Video Toaster.

The world of video in 1985 was very different from what we know today. Not only was there no YouTube, there was no World Wide Web to view video on. Video content was completely analog and stored on magnetic tape. Computers of the day, like the IBM PC and Macintosh, worked with their own digital displays that didn’t interoperate at all with the world of analog video.

The Amiga, however, was originally designed as a game console, and so it was compatible with standard television frequencies. Where the Amiga designers showed insight and forethought, however, was in creating a bridge between analog and digital. The very first Amiga contained a genlock, which matched video timings with an NTSC or PAL signal and allowed the user to overlay this signal with the Amiga’s internally generated graphics. The first person to realize the potential of this was an engineer living in Topeka, Kansas. His name was Tim Jenison.
</pre>
			<h2>Tim and Paul</h2>
			<pre>
Tim Jenison was born in 1956, the son of an electrical-mechanical engineer. He once sat on his father’s knee at age five as his dad explained Ohm’s Law. Growing up in rural Iowa, he lived far away from most people. Vacuum tubes and transistors became his best friends.

For his seventh-grade science fair project, Jenison built a rudimentary digital computer that could add and multiply numbers in base 10. He built his first real computer with a Motorola 6800 CPU hooked up to a Teletype because he couldn’t afford kits like the Altair that were popular at the time. “It was so exciting to say, ‘Wow, I have a computer!’” Jenison recalled in an interview with Wired. “But then you had to figure out what to do with it! That was the hard part.”

As a kid, Jenison dabbled in making 8mm home movies. It was a frustrating experience being an aspiring filmmaker at the time. To make any kind of edit required literally cutting and pasting film together. After dropping out of college and teaching himself engineering and programming, he started a small business selling software for the Tandy Color Computer. The very fact that the word “color” was in the name of the computer showed how primitive the technology was. Yet back then, Tim was already dreaming about doing video on a computer.

Around the same time in California, a man named Paul Montgomery went into a RadioShack to look for a device to spruce up his homemade videos. The sales manager showed him a special effects generator that cost about $450. The conversation went like this:

“This looks great! Can I fade from one image to another?” Montgomery asked.

“No, no way,” the RadioShack associate replied.

“Can it do fades at all?”

“Yeah, you can fade to black.”

“Can it do anything else?”

“Yeah, fade to red or green.”

“What about squeezing the image and flipping it?”

“No, no way. That takes a $100,000 piece of equipment. You’re never gonna find that here.”

Montgomery left the RadioShack empty-handed and disappointed.
</pre>
			<h2>The Amiga Arrives</h2>
			<pre>
When Jenison read about the capabilities of the Amiga in the August 1985 issue of Byte, he went straight down to the nearest Commodore dealer and bought the first Amiga 1000 that came in. He immediately created a product called DigiView that was a simple video capture device. It would take snapshots of a single frame of video and save it to a floppy disc in the Amiga’s 4096-color HAM mode.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2016/03/digiview-300x225.jpg" alt="">
			<pre>
Jenison had saved three demo pictures on a single floppy when he ran into Jeff Bruette, a Commodore employee. Jeff asked if he could make a copy and take it back to Commodore with him. Tim agreed, but he asked that Bruette delete the disk’s READ.ME file, since it contained his home phone number. But within 24 hours, Tim’s phone started ringing. “This thing had spread all across the country,” he said.

Paul Montgomery was one of the first people to call. His friend Brad Carvey (an engineer and the brother of comedian Dana Carvey) had come over to his house and showed him the images. There was silence in the room as they stared at the pictures; it was like a religious experience. Computers weren’t supposed to be able to do things like that.

Jenison knew he had a winner on his hands with DigiView, so he sold his interest in the Tandy CoCo software company and started a new company to make video products for the Amiga. This was the beginning of NewTek. DigiView eventually sold more than 100,000 units, and it spawned DigiPaint, a paint program that worked with the Amiga’s 4096-color mode. Originally, this HAM mode was supposed to only work with static images because of the sequential algorithms used to store the data. DigiPaint simply worked around that problem to achieve what had formerly been impossible.

At the same time, Montgomery moved on to work at Electronic Arts but resigned when the company failed to live up to its founders’ goals of pushing computing forward with the Amiga. He ended up moving to Topeka and joining NewTek right at the time when the company was looking to expand with a new product.

Montgomery asked Jenison if the Amiga would be able to serve as the centerpiece for a video effects generator. Jenison liked the idea, but Montgomery kept pushing: “What about squeezing the image and flipping it?” he asked.

“No, that would take a $100,000 piece of equipment.” Jenison replied.

“OK, yeah, I knew that,” Montgomery said. “But it would be pretty cool if you could do it.”

In the story of the Amiga, there were many points in which an engineer was challenged to do something impossible. In this instance, Jenison went off and thought more about the problem. Eventually, he figured out a way to do the squeezing and flipping effect—and that was the beginning of the Toaster prototype.
</pre>
			<h2>The Toaster takes shape</h2>
			<pre>
Montgomery suggested that Jenison meet his friend Brad Carvey, who had been working on projects involving robotic vision. The three of them got together in a pizza restaurant in Topeka and started drawing block diagrams on the placemats.

Brad built the first wire wrap prototype of the board, and Jenison and software engineer Steve Kell helped get it working. In a few days, it was doing the flipping effect, and they were on their way.

The prototype was unveiled at Comdex in November 1987, causing quite a stir. By itself, the Toaster was already an impressive video effects board at an unbeatable price. But Jenison and the NewTek engineers wanted it to be much more. Their dream was for anyone to be able to afford video effects that looked as good as what professional TV studios produced. Creating a single, affordable, add-on card to replace network studio equipment seemed impossible.

In World War II, the slogan of the Army Corps of Engineers was, “The difficult we do immediately. The impossible takes a little longer.” And despite the Amiga’s propensity for handling video, some things couldn’t be done without building new custom chips. To get the performance they needed on the software side, much of the 350,000 lines of code were written in 68000 assembly language. Finishing the Toaster took 15 engineers, three years, and 5,325 hand-made cinnamon cat candies, but the end result was astonishing.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2016/03/VideoToaster-original-300x200.jpg" alt="The original Video Toaster running on an Amiga 2000">
			<br><i>The original Video Toaster running on an Amiga 2000</i>
			<h2>The Toaster</h2>
			<pre>
The Video Toaster was released in December 1990 for an entry-level price of $2,399. It consisted of a large expansion card that plugged into an Amiga 2000 and a set of programs on eight floppy disks. The complete package, including the Amiga, could be purchased for less than $5,000.

For that money, an aspiring video editor received a four-input switcher, two 24-bit frame buffers, a chrominance keyer (for doing green or blue screen overlays), and an improved genlock. The software allowed video inputs to switch back and forth using a dazzling array of custom wipes and fades, including the squishing and flipping effect that Montgomery had originally wanted.

Bundled with the system was Toaster CG (a character generator to make titles), Toaster Paint (an updated DigiPaint for making static graphic overlays), Chroma F/X (for modifying the color balance of images), and the real kicker: Lightwave 3D, a full-featured 3D modeling and animation package written by Allen Hastings and Stuart Ferguson.

At the time, 3D modeling and animation was the sort of thing people did on $20,000 SGI workstations, using software that cost nearly as much as the hardware it ran on. Bundling Lightwave with the Toaster was like including a free 3D printer with a new computer. It meant that Toaster users could create any digital effect that they could imagine.
</pre>
			<iframe src="https://www.youtube.com/embed/d7QEyaUVwTI" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br><i>1980s MTV viewers can surely recognize some of their favorite video transition effects.</i>
			<h2>Suddenly, star wipes</h2>
			<pre>
The launch of the Toaster changed the entire equation of producing video content. In the United States, the Federal Communications Commission had long established rules defining a minimum level of video quality called Broadcast-safe that was required to air programming on television. Consumer-level video cameras didn’t reach this level and couldn’t be used to make content for TV, and there were only a few exceptions for news programs showing short video clips taken by amateurs or in other countries. The equipment required to produce broadcast-safe video was expensive, running from hundreds of thousands to millions of dollars. This meant that unless you were an employee of a major television network, you couldn’t make your own programs and show them to anyone but your friends and family.

The Toaster changed this. For less than five thousand dollars, anyone could create programs that looked as good as the networks. One of the earliest and most enthusiastic Toaster adopters was rock bands that needed to make exciting videos for MTV on a budget. Rocker Todd Rundgren got especially motivated and connected 10 Toasters together to render his revolutionary music video for the song "Change Myself." Effects that we consider “cheesy” today, like star wipes, only became that way because the Toaster made them commonplace. Just as the Macintosh led to a brief period of font abuse in the 1980s, the Toaster made possible a time of wild transitions and fades in the 1990s. The concept of “Wayne’s World” was very much a Toaster-based phenomenon.
</pre>
			<iframe src="https://www.youtube.com/embed/C_K8vnx2ZDc" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br><i>A promotional video for the Video Toaster 4000 from NewTek. Features Wil Wheaton at his most 1990s!</i>
			<h2>Jeno</h2>
			<pre>
The Toaster also enabled creative people to make extraordinary lives for themselves. One such person was Jeno Horvath, who had started his career designing electrical systems for discos in the 1970s. He got a new job at Memorex servicing its mainframe peripherals and quickly bought his first computer—a VIC 20—to catch up on the computer industry. When the Amiga 1000 came out in 1985, he bought one of the very first developer models that had originally been destined for Electronic Arts. He had started buying video gear as a hobby, and when the Toaster came out, he knew he had to have one.

“The Toaster absolutely revolutionized video,” Horvath told Ars in an interview. “Essentially, it replaced an entire truckload of broadcast gear. It easily replaced $50,000 worth of gear. And the Amiga was the only computer you could use it on.”

The Toaster required not just an Amiga but specifically an Amiga 2000, as the first model didn’t have the proper slots to fit the card. After spending about $5,000 on the Toaster and Amiga (plus $1,200 each for a couple of Time-Based Correctors that were necessary to synchronize analog video inputs to a common clock to prevent flicker and shudder), he was all ready to start creating professional videos. But what was he going to create?

Horvath helped make some videos for local rock bands, but he later found that businesses were an even more lucrative market. Companies like Teck Cominco often needed training or marketing videos for their employees and customers. They didn’t want to pay the cost of hiring professional broadcasting studios, but they needed something a bit better than an employee’s nephew’s shaky cam footage. This was a profitable niche that Horvath ended up thriving in. One video he proposed for the Kidney Foundation had a budget of $12,000, which was much higher than normal. When they balked, Horvath told them that if they took the promotional video to a dozen corporations and they couldn’t recoup the cost, he would give them 80 percent of their money back. The video, which featured people telling the stories of their successful transplants in their own words, ended up generating $160,000 of donations in the first month it was used.

“A lot of professional companies like Grass Valley would pooh-pooh the Toaster, but you couldn’t beat it on the quality,” Horvath recalled. “If you didn’t use the built-in Toaster effects, nobody could tell you were using a $5,000 Amiga system over a $50,000 to $75,000 system. You really couldn’t tell the difference in quality.”

Horvath went on to invent an even more interesting job for himself: Electronic News Gathering or ENG. He would make friends with local firemen by shooting footage of them, then they would return the favor by giving him tips on where police were heading (often police would muster at nearby fire stations) and allowing him access to crime scenes. One time, thinking he was traveling along a parallel course, he ended up in the middle of a high-speed police chase. His adventures generated professional video footage that he sold to local news stations.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2016/03/toaster4000_1_big-300x225.jpg" alt="The Video Toaster 4000.">
			<br><i>The Video Toaster 4000.</i>
			<pre>
In 1993 NewTek released the Video Toaster 4000, an updated version that made use of the new AGA graphics hardware available with the Amiga 4000. In addition, the company released the Video Toaster Flyer, an add-on product that turned the Toaster into a full non-linear editing system similar to the Avid Media Composer that was taking the professional editing world by storm. While it was a lot cheaper than the Avid (which could cost over a hundred thousand dollars for a complete system), editing digital video in those days was severely limited by existing hard drive capacity. Horvath bought a used Flyer and paid $6,000 each for a pair of fast 9GB SCSI drives to use with it. “I still have one of those drives,” he admitted. “I couldn’t bear to part with it.”

While most Toaster users had no issue with purchasing Amiga computers to operate the system, there was a small contingent of TV stations and video producers who kept asking for a Macintosh version of the Toaster. This was not possible because the Macintosh (or the PC, for that matter) didn't have the Amiga's custom chips that were required to make the Toaster software work. To get around this, NewTek released the Toaster Link, an add-on card for the Amiga and the Toaster that routed the video output to a Macintosh monitor and allowed users to copy/paste images from Mac paint programs. Thus, a "Mac version of the Toaster" could be sold as a complete system with a "special keyboard" for certain operations. The special keyboard was, of course, the Amiga's keyboard.
</pre>
			<h2>Our last, best hope</h2>
			<iframe src="https://www.youtube.com/embed/l3C_tTlhgm8" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br><i>Look familiar? Babylon 5's state-of-the-art graphics owe a debt of gratitude to Video Toaster.</i>
			<pre>
Community cable upstarts, rock bands, and independent electronic news gatherers weren’t the only people in the television community to be affected by the Video Toaster. The inclusion of Lightwave 3D made it possible for a revolution in special effects that has carried on to this day.

For instance, writer and showrunner Joe Michael Straczynski had received the green light to make his dream series, a science-fiction epic called Babylon 5. He hired Ron Thornton, whom he had worked with on Captain Power and the Soldiers of the Future, to handle the special effects. Thornton convinced Straczynski that this new Video Toaster could be used to make breakthrough visuals at one-third to two-thirds the cost of using traditional models.

Thornton’s team, Foundation Imaging, grew from five to 15 people to meet the demands of creating the special effects for Babylon 5. They networked a series of Amigas together to render the graphics for the pilot episode. Later, Pentium PCs and DEC Alpha workstations were used as a render farm for the Lightwave 3D effects. The move from models to computer graphics images seems obvious in retrospect, but at the time the industry was uncertain that CGI was up to the task. Star Trek, for example, waited until 1997, the sixth season of DS9 and the fourth season of Voyager, to transition over from shooting physical models. When they finally switched, they hired Foundation Imaging to help with the process. Babylon 5 was ultimately nominated for several Emmy awards, and the Video Toaster itself received an Emmy Award for technical achievement in 1993.
</pre>
			<h2>The Toaster transitions</h2>
			<pre>
At about the time the Toaster, the Flyer, and Lightwave 3D in particular were really hitting their stride, Commodore was running into financial difficulties. “The writing was on the wall for the Amiga,” Horvath said. “So I flipped my Flyer. I sold it.” He bought a DEC Alpha for $17,000 (it still sits in his living room today) and got a discount from NewTek to migrate his Lightwave 3D license, first from the Amiga to the Alpha, then from the Alpha to Windows NT on generic Intel hardware.

The non-linear video editing market ended up splitting between Avid and upstarts like Adobe Premiere and Apple’s Final Cut Pro, and even cheaper options like iMovie and Windows Media Maker helped bring video editing to the masses.

NewTek ported parts of the Video Toaster to Windows and introduced new products, like the TriCaster, that were no longer dependent on the Amiga hardware. The TriCaster, which allows professional video shows to be broadcast over the Internet, is still sold today. Technology journalist Leo Laporte uses it for all his shows, including the popular This Week in Tech. In 2004, NewTek released the entire code for the Video Toaster 4000 as open source.

Though the brand and software fell behind its modern contemporaries, the legacy of the Video Toaster still lives on. “Thousands of people could suddenly make video,” Tim Jenison said in an interview with Laporte. “Once the Toaster came out you could look like a real TV station.”

Thanks to Moore’s Law, these days anyone can shoot and edit an entire high-definition video on their phone then upload it directly to YouTube to become a world-famous video star. It feels like second-nature today, but this idea was revolutionary—and it started back in 1990 with the Video Toaster.
</pre>
		</div>
		<div class="content" id="10">
			<h1>A history of the Amiga, part 10: The downfall of Commodore</h1>
			<i>The Amiga was a machine ahead of its time, but Commodore was in trouble.</i>
			<pre>
As the 1990s began, Commodore should have been flying high. The long-awaited new Amiga models with better graphics, the A1200 and A4000, were finally released in 1992. Sales responded by increasing 17 percent over the previous year. The Video Toaster had established a niche in desktop video editing that no other computer platform could match, and the new Toaster 4000 promised to be even better than before. After a rocky start, the Amiga seemed to be hitting its stride.

Unfortunately, this success wouldn’t last. In 1993, sales fell by 20 percent, and Commodore lost $366 million. In the first quarter of 1994, the company announced a loss of $8.2 million—much better than the previous four quarters, but still not enough to turn a profit. Commodore had run into financial difficulties before, particularly in the mid-'80s, but this time the wounds were too deep. Sales of the venerable Commodore 64 had finally collapsed, and the Amiga wasn’t able to fill the gap quickly enough. The company issued a statement warning investors of its problems, and the stock plunged. On April 29, 1994, Commodore International Limited announced that it was starting the initial phase of voluntary liquidation of all of its assets and filing for bankruptcy protection. Commodore, once the savior of the Amiga, had failed to save itself.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/commodore-sales-modified.png" alt="Sales, revenue, and profit for Commodore, 1988-1994.">
			<br>
			<i>Sales, revenue, and profit for Commodore, 1988-1994.</i>
			<h2>What went wrong</h2>
			<pre>
Why did this happen? Was it inevitable, or could the company have made different choices and kept both itself and the Amiga platform alive and healthy?

There are those who would argue the former. Computing platforms tend to start with many different competitors and then slowly dwindle down to one or two survivors. IBM so dominated the mainframe industry in the 1960s that it was referred to as “Snow White," with more market share than the “Seven Dwarves” combined. Those seven became five and then essentially one after mergers and acquisitions. There were more than 100 personal computer platforms in the early '80s, but by 1994 only two remained that sold enough to be measurable: PC compatibles at 91 percent share and Macintoshes at 9 percent. More recently, we’ve seen a diverse market of smartphone platforms whittled down to leave just two: Android at 88 percent and iPhones at 12 percent.

But is the survival of more than two computing platforms an ironclad law of technology or just a coincidence? There are exceptions—game consoles went through two separate generations with three viable competitors: Nintendo, Sega, and Sony in the 1990s, and Nintendo, Microsoft, and Sony from the 2000s until present-day. In addition, a declining market share is not irreversible. The Macintosh, after hitting a low of under two percent in 2003, has since rebounded to a healthy 7.5 percent. Could a trio of PCs, Macintoshes, and Amigas have coexisted in an alternate universe?

I think so. Saving the Amiga wouldn’t have been easy, but it definitely was possible. Ultimately, the failure rests with Commodore management, who not only failed to adapt to a changing marketplace, but in many cases were actively hostile to their own company. To understand what they did and why they did it, we have to jump far back in time—to the creation of Commodore itself.
</pre>
			<h2>Jack Tramiel sells his soul</h2>
			<pre>
Jack Tramiel founded Commodore in 1955, leveraging his experiences repairing typewriters after the war to build up a small stable of office products, including adding machines. Jack was a Holocaust survivor, and his aggressive, take-no-prisoners approach to business sometimes got him in trouble. In 1965, he was involved in a scandal when the Alliance Acceptance Corporation, a Canadian financial company, suddenly collapsed. Tramiel had close ties to Alliance, and although he was not indicted, the scandal wrecked Commodore. To survive, Tramiel was forced to sell a large share of his company to Irving Gould, a Canadian financier. Gould now held the purse strings.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/jack-tramiel-and-irving-gould.jpg" alt="Left: Jack Tramiel. Right: Irving Gould.">
			<br>
			<i>Left: Jack Tramiel. Right: Irving Gould.</i>
			<pre>
Irving Gould was a jet-setting man who didn’t like to spend a lot of time managing the company, preferring instead to let Tramiel handle the day-to-day operations while he took care of the finances. For a while, this strategy worked well. The purchase of MOS Technologies (the company that made the legendary 6502 CPU that powered most 8-bit computers), the release of the PET, and the success of the VIC-20 and Commodore 64 all happened during this time.

However, Gould grew increasingly intolerant of Tramiel’s one-man management style. Tramiel also said that he wanted to issue more stock to pay off Commodore’s massive debts, but Gould refused, arguing that it would weaken his hold on the company. What Gould didn’t say was that much of Commodore’s debt was to other companies that Gould also controlled.

Their disagreements came to a head when Tramiel criticized Gould for using Commodore’s assets (such as the company jet) for personal reasons. Jack said: “You can’t do this while I’m president,” to which Gould responded, “Goodbye.”

Gould then arranged to force Tramiel out of his own company while forming a series of increasingly flimsy excuses: first that although Jack had brought the company to its first billion in sales, he wasn’t the one to get them to ten billion. Then, he believed that Jack was trying to position his sons to take over the company.

In any case, Gould managed to get Tramiel to suddenly resign from the company he founded in 1984. Without Jack, Commodore had lost its soul, much as Apple had when Steve Jobs was kicked out. Gould then proceeded to hand-pick a series of CEOs with little or no experience in the personal computing industry. Some, like Marshall Smith, were terrible, whereas others, like Thomas Rattigan, managed to bring the company back to profitability. One thing was consistent, however: Commodore CEOs, like hockey coaches, were hired to be fired.

Gould was out of his element, so in 1986 he turned to what many incompetent and desperate managers utilize: a management consulting company. The company he hired, Dillon-Read, sent over a managing director named Mehdi Ali. Ali spent many years and many millions of dollars to come up with the recommendation that Gould should hire Mehdi Ali to be the new CEO of Commodore International, which Gould finally did in 1989. This was the exact moment when Commodore sealed its doom.
</pre>
			<h2>Mehdi’s mistakes</h2>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/mehdi-ali-commodores-last-president.jpg" alt="Mehdi Ali.">
			<br>
			<i>Mehdi Ali.</i>
			<pre>
Ali’s reign at Commodore can be characterized by three main aspects: costly strategic errors, cutting essential research and development (R&D), and increasing the CEO’s compensation. The latter was no small thing. In 1989, Ali received $1.38 million in salary. In 1990, that figure rose to $2 million (not including bonuses), and Irving Gould scored a 40 percent pay raise to $1.75 million. By comparison, the CEO of IBM, John Akers, received $713,000 in the same year.

Mehdi missed or flubbed several key opportunities during this time. One of the most egregious was killing a deal with Sun Microsystems, which wanted to license both Amiga Unix (AMIX) and Amiga hardware for their low-end workstations. Ali sabotaged the deal twice by demanding increasingly outrageous licensing fees.

Ali sat back and watched as new companies grew faster and faster by filling in the gaps in Amiga hardware that Commodore refused to provide, such as hard drives and CPU upgrades. The largest of these, GVP, ended up being worth over half the value of Commodore itself, which was unheard of for a peripheral company. Those were all dollars that could have gone directly into Commodore’s pocket.

Mehdi was also responsible for producing the worst Amiga ever created—the Amiga 600. Intended to be a cost-reduced 500, it was released with fewer features than its cousin for a higher price. Ali doubled down on this failure by making sure that stores were flooded with A600s that nobody wanted while failing to manufacture enough A500s and A1200s that people actually did.

But the greatest harm that Ali inflicted on Commodore was selling its future. Over the years, he continued to gut R&D funding until there was almost nothing left.

At its announcement in 1985, the Amiga was years ahead of its time. It had features like preemptive multitasking, a color GUI, and hardware graphics and sound acceleration that would not become mainstream on computers until over a decade later. However, the team that created the Amiga was keenly aware that it should be continually improved. Jay Miner, the designer of the Amiga’s chipset, had nearly completed an updated version of the chips codenamed “Ranger” that increased speed, resolution, and color support. Bizarrely, Commodore never shipped these advances, instead opting for a much less powerful spec bump on the Amiga 500 and 2000 (the Enhanced Chip Set, or ECS). But the company did start work on a huge revamp of the Amiga chipset in 1988 called the Advanced Amiga Architecture, or AAA.

AAA was a bold step forward for the Amiga, but it did not fare well in the hands of Ali’s cost-cutting. By the time Commodore folded in 1994, there was only a single engineer working on it, and it was still not ready for release. In the meantime, a stopgap solution called Advanced Graphics Architecture (AGA) had to be rushed into production, and like most things rushed into production, it was also late. Commodore management made things even worse by cancelling Dave Haynie’s A3000+ with AGA project in early 1991 in favor of the A4000, which ended up being an inferior design. Unfortunately, AGA was missing a key feature that was starting to turn the once-lowly PC into a gaming powerhouse.
</pre>
			<h2>The coming DOOM</h2>
			<pre>
The PC became a gaming contender with the release of a new three-letter graphics standard: VGA. VGA didn’t have any fancy acceleration, but it did have a standard mode called 13h, which delivered 320x200 resolution in 256 colors.

The ECS Amiga chipset could theoretically display 4,096 colors simultaneously in HAM mode, but this mode was too slow for action games. Amiga games typically used 320x200 resolution in 32 colors instead.

The AGA chipset “fixed” this issue by increasing the memory size and allowing 256 color modes, so in theory it could have competed with VGA. But VGA was available in 1987 with the IBM PS/2 computers and had become widespread in clones by 1990, whereas AGA wasn’t available until 1992. AGA also had a problem that was starting to become an issue with games: it addressed each of its pixels using planar, rather than chunky, color modes.

Planar access divides up color bits among several “planes”—think layers in Photoshop—that can be accessed separately. In the days before 256 colors, this saved huge amounts of video RAM, which was at a premium. When combined with chips like the blitter, it also made it really easy to create side-scrolling arcade games with multiple “parallax” layers that moved smoothly at different speeds. The PC architecture, even with the new VGA cards, had a difficult time doing this.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/doom11.png" alt="DOOM came out of nowhere, and PC gaming was never the same again.">
			<br>
			<i>DOOM came out of nowhere, and PC gaming was never the same again.</i>
			<pre>
But a little shareware game came out in 1993 that made every single side-scrolling arcade game seem obsolete overnight. It was DOOM, and it used all 256 colors in VGA’s chunky modes (including a new undocumented 320x240x256 mode) to create a simulation of a 3D world. It wasn’t full 3D, of course, but it was a quantum leap for gamers. Games like DOOM used powerful 386 and 486 CPUs to make up for the lack of graphical acceleration. The Amiga, for so long the gold standard of gaming, needed a DOOM, but it couldn’t have it. It didn’t have the right graphics hardware. The fix (adding chunky modes to the chipset) wasn’t difficult, but it wasn’t available. Instead, it was shoehorned into a new Hail-Mary product that was intended to save the company and almost did.
</pre>
			<h2>The CD32</h2>
			<pre>
The CD was invented as a digital audio storage format in 1982 in a collaboration between electronics giants Philips and Sony. It took the music world by storm, and in 1984 Sony and Denon demonstrated the first CD-ROM, which gave a personal computer a read-only storage capacity of 540 megabytes. As most games were distributed on floppy disks of a single megabyte at the time, this was a ludicrous amount of storage, and companies struggled to come up with ideas for new types of content and hardware to play it on.

Philips, after many delays, released its CDi device, which looked like a regular CD player but cost $700. It was a huge commercial failure, remembered today only for an improbably terrible series of Zelda games.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/CDi-and-CDTV-1.jpg" alt="The Philips CDi (left) and Commodore CDTV (right) were both flops.">
			<br>
			<i>The Philips CDi (left) and Commodore CDTV (right) were both flops.</i>
			<pre>
Commodore had also decided to get on this bandwagon, and it had actually beat Philips to the punch when it released the CDTV in 1991. It was essentially an Amiga 500 with a CD-ROM drive in a similar plain rectangular box. These devices were met with confusion by the general public— they weren’t computers, because they had no keyboards, but they weren’t game consoles either (although the CDTV did play a mean game of Lemmings), because they were sold without game controllers and were priced prohibitively high for that market. The CDTV also failed, but it set the stage for Commodore’s final product, the CD32.

This device would be a game console, pure and simple. Commodore attempted to one-up Sega (who had struggled with the Sega CD add-on to the underpowered Genesis) by designing the console to look very similar to a Genesis, but with “32-BIT” replacing that system’s “16-BIT” designation in proud white letters. Inside was essentially an Amiga 1200 with a CD-ROM drive and a new chip called “Akiko” that added MPEG 2 movie playback and chunky-to-planar graphics conversion.

It still wasn’t quite enough for DOOM, however. The CD32 had a low-power 68EC020 CPU running at 14 MHz with very slow memory. This was about equivalent to a 386 at the same speed, which hadn’t been cutting-edge since 1990. Some games, like Wing Commander, were converted to run on the CD32, but DOOM never made it. Instead, and somewhat ironically, it was ported to the Jaguar game console, the last piece of hardware that Atari ever made.

Still, the CD32 was a very respectable game machine for its time, and it was scheduled to be released before Christmas 1993 for $399. Unfortunately, Commodore had run out of time and money.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/cd32-jeremy-reimer.jpg" alt="The Amiga CD32, with modem attachment. From the author's private collection.">
			<br>
			<i>The Amiga CD32, with modem attachment. From the author's private collection.</i>
			<h2>Death by XOR</h2>
			<pre>
As Commodore approached its final Christmas season, it was having trouble negotiating with its parts suppliers. Years of being behind on payments were starting to take their toll, and Commodore could only procure enough parts to make 100,000 CD32s. As Dave Haynie noted in his Deathbed Vigil video, they might have survived with 400,000.

The CD32 was released in Europe and in Canada, but Commodore was unable to sell it in the United States due to a legal injunction for non-payment of patent royalties to a company called Cad Track. This was an early example of a patent troll, as the patent was for using the simple Boolean XOR (exclusive OR) formula for displaying computer graphics. XOR is a fast way to overlap graphics non-destructively and was commonly used by GUIs to draw bounding boxes. Every other computer company simply paid Cad Track their royalties as a matter of course, but Commodore had defaulted on payments and had refused to pay the $10 million penalty ordered by a US judge. At this time Mehdi Ali was still taking home more than $2 million a year.

The CD32 sold all 100,000 units in Europe and Canada, but Commodore had no money to make any more. The market for the consoles collapsed after Commodore’s liquidation, but some of them found some innovative uses: for example, a company in Vancouver created the first online banking application for Vancity using CD32s and custom-built add-on modems.
</pre>
			<h2>A different approach</h2>
			<pre>
In retrospect, taking on Sega and Nintendo was probably not the best approach for a company that was struggling to sell in the much smaller market for personal computers. Commodore’s UK division made headlines by taking out a billboard next to Sega’s headquarters that said, “To get this good will take Sega ages," but this sort of boasting would have been moot just a few years later when Sony introduced the PlayStation and drove Sega out of hardware forever.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/sega-ages-billboard.jpg" alt="The billboard that Commodore UK paid for outside of Sega's headquarters.">
			<br>
			<i>The billboard that Commodore UK paid for outside of Sega's headquarters.</i>
			<pre>
The Amiga started as a game console and would have died in the 1983 video game crash if it hadn’t been quickly converted to a personal computer. A decade later, Commodore tried to save its company by turning the Amiga back into a game console, but it was too little and too late.

What if, instead of chasing the volatile video game market, Commodore had simply bundled a CD-ROM drive with the Amiga 1200 instead? In the early '90s, so-called “multimedia” PCs were starting to become a big market, but they were expensive and fiddly and stuck in a chasm between the unfriendly, single-tasking DOS and the rickety paint job of Windows 3.1.

An Amiga 1200 with a built-in CD-ROM drive would have been a perfect entry into this market. With a slight bump in CPU speeds and a better Akiko chip, it could have easily played DOOM, Myst, Wing Commander, and all the other great games of the day. It would have been cheaper, more powerful, and easier to use than multimedia PCs that struggled to display video that was larger than a postage stamp. It could have been sold to students as an entry-level computer, with more powerful units available as their needs increased. Those machines, powered by 68040 and 68060 chips and the AAA chipset, would have easily kept up with high-end PCs.

It's worth noting that Commodore engineers were, at the time of the company's demise, working on even more advanced Amiga architectures. The Hombre project planned to use an HP PA RISC processor and a 3D graphics accelerator card to deliver a truly next-generation Amiga. It, too, was starved of development resources and never released.

Another machine that Commodore should have made but never did was an Amiga laptop. It would have been difficult to re-engineer the Amiga’s powerful custom chips so that they wouldn’t suck up battery life, but not impossible. Commodore simply didn’t have the resources available for such a product and foolishly thought that the market for laptops was not large enough to be worth pursuing.
</pre>
			<h2>The blame lies at the top</h2>
			<pre>
Commodore had many failings as a company. Steve Jobs had once offered to sell the early Apple to Commodore, but when the company rejected him, he dismissed their management as being “sleazy." Jobs was prone to hyperbole, but he wasn’t entirely inaccurate. Jack Tramiel’s hard-nosed management style often offended both suppliers and resellers, but he at least had a vision for what the company was supposed to be. When he was removed, the new management kept his offensive style but lost any and all strategic outlook.

At the end of the day, Irving Gould was the one who kicked out Tramiel. This created not just a power vacuum but an instant rival. Jack swore revenge, took over Atari, and released the ST, which very nearly killed the Amiga before it had a chance to take off.

Gould also hired and promoted Mehdi Ali, who bled Commodore dry while lining his own pockets. Mehdi Ali would end up hiring people like Bill Sydnes, who had been the product manager for IBM’s PC Junior (the biggest failure in that company’s history) and was responsible for canceling the innovative Amiga A3000+. In the end of Dave Haynie’s Deathbed Vigil video, the Commodore engineers spent their last night painting the names of these managers on the speed bumps in the company parking lot, then burnt a doll of Mehdi Ali in effigy. It was a fate far kinder than these people deserved.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/01/speedbumps.jpg" alt="Another executive speed bump.">
			<br>
			<i>Another executive speed bump.</i>
			<pre>
After Commodore liquidated, Ali formed an extremely sketchy management consulting company that vanished from the Web a few years ago. Consulting the Internet Archive shows that Mehdi described himself as having “accomplished a major operational turnaround” during his time serving as president of Commodore. That could be considered true, in a sense. He joined Commodore when it was a billion dollar company and turned it around until it was a bankrupt one.
</pre>
			<h2>What we can learn from Commodore’s demise</h2>
			<pre>
One of the best things that Mark Twain never actually said was: “History never repeats itself, but it does rhyme.” The stories of Commodore and Apple had many similarities. Both had mercurial, stubborn founders. Both had early successes with 8-bit computers (the Commodore 64 and Apple ][) thanks to underrated technical geniuses (Chuck Peddle and Steve Wozniak), and both later sold 16 and 32-bit computers with fancy GUIs (the Amiga and the Macintosh). Both kicked out their founders, who went on to produce rival products that failed in the marketplace (Atari ST and NeXT).

But Commodore faltered and disappeared, whereas Apple got Jobs to return, and he led the company to become the most valuable on the planet.

Maybe Jack Tramiel was no Steve Jobs. He was older and more single-minded. He never really saw past his idea of selling computers “to the masses, not the classes." The Atari ST was a decent enough computer, but it ended up being a footnote in history, a cheap and poor imitation of the Amiga. The NeXT, on the other hand, contained forward-looking technology that ended up powering both OS X and iOS.

Apple also didn’t have to contend with executives who seemed hell-bent on wringing every last dollar of profit from the company and putting it in their own pockets. If you look at the financial history of Commodore, they always seemed to hover around zero profits: sometimes a bit higher, sometimes a bit lower. A more recent company that also uses that strategy is Amazon, but Jeff Bezos funnels all the money that would end up as profit into growing the company, instead of engineering financial scams that benefit only the top execs.

In the end, Commodore imploded, and it was a sad thing. But the Amiga itself, although it would never challenge for market share again, would survive the death of its parent company. It was a dream given form: a personal computer that was fast and friendly and responsive, that multitasked well, that played great games, and that behaved in a way that made its owners not just fans but fanatics. Today, it is neither gone nor forgotten.
</pre>
		</div>
		<div class="content" id="11">
			<h1>A history of the Amiga, part 11: Between an Escom and a Gateway</h1>
			<i>The Amiga didn't go away with Commodore, but its future was uncertain.</i>
			<pre>
Commodore International declared itself insolvent on April 29, 1994 under Chapter 7 of US bankruptcy law. Ordinarily, this would have been followed immediately by an auction of all the company’s assets. However, Commodore’s Byzantine organizational structure—designed to serve as a tax shelter for financier Irving Gould—made this process far more lengthy and complicated than it should have been.

During this time, Commodore UK, Ltd. continued to operate. It had been the strongest of all the subsidiary companies, and it always had a positive cash flow. As the other subsidiaries went under, Commodore UK purchased all of their remaining inventory and continued to sell Amigas to British customers.

The head of Commodore UK, David Pleasance, hatched a plan to purchase the mother company’s assets at auction. His idea was to raise enough money not only to buy Commodore International but to fund the new company as an ongoing concern, including the continuation of research and development projects. The business plan was to continue to sell Amiga 1200 and 4000 computers and CD32 consoles while slowly transitioning to next-generation hardware based on Dave Haynie’s Hombre RISC architecture.

Pleasance raised about $15 million from local investors, which was enough to win the auction. For the rest of the money, he arranged a deal with a curious partner: New Star Electronics, based out of China.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/new_star_genesis.jpg" alt="New Star Electronic's Sega Genesis clone.">
			<br>
			<i>New Star Electronic's Sega Genesis clone.</i>
			<pre>
New Star got its start by selling unlicensed clones of the Super Nintendo and Sega Genesis consoles—the clones not only played the same games, but they looked identical to their legitimate counterparts. Under pressure from the Chinese government, New Star had decided to change its business model, and it was looking for game companies willing to license their devices. David Pleasance arranged a deal worth $25 million for New Star to produce and sell Amigas for the Asian market.

Other companies were also vying for Commodore International’s assets. Dell Computer put in a $15 million bid—but it was late. The bankruptcy judge refused to wait for Dell to do its due diligence. Six months had already passed, and the only remaining contender in the race was a European PC manufacturer named Escom, which put in a bid of $14 million. Originally Escom wanted to bid only on the Commodore brand, but the company raised its stake to all the Commodore assets after other bidders complained.

With only thirty-six hours to go before the end of the auction, New Star told Commodore UK that it was backing out of the deal. Without enough money to guarantee the continuation of the business, David Pleasance withdrew his bid. All of a sudden, Escom was the new owner of Commodore and the Amiga.
</pre>
			<h2>Enter Escom</h2>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/escom_logo-300x265.png" alt="ESCOM Computers' logo, circa 1995.">
			<br>
			<i>Enlarge / ESCOM Computers' logo, circa 1995.</i>
			<pre>
Escom was founded in 1991 in Germany by Manfred Schmitt. Early on, the company sold Commodore 64 computers, but it quickly branched out into its own line of IBM PC clones. From 1992 to 1994, sales rose from 180,000 to 410,000 units, making Escom the biggest PC distributor in Germany.
Schmitt’s company had been saved in its infancy when he was able to secure a large purchase of Commodore 64s for the Christmas 1991 season. He did so by making a personal phone call to Commodore’s global logistics director, a man named Petro Tyschtschenko. During the bankruptcy negotiations, Schmitt called Petro again to get help with the process. After Escom won the auction for Commodore’s assets, Schmitt split the company into two subsidiaries: Commodore BV in Holland (which became the holder of the Commodore trademark) and Amiga Technologies, whose name was self-explanatory. Schmitt rewarded Petro with a new job as the director of Amiga Technologies.

At first, this wasn’t much of a job. Escom had only been interested in the Commodore name, and putting all the Amiga assets together in a single division seemed like a prelude to selling them off. However, Escom was soon buried under a deluge of mail and phone calls from Amiga owners and fans, pleading with the company to keep their beloved computer alive.

Petro was keen on keeping the flame going, as he had been a fan of the Amiga back in his Commodore days. However, the task wasn’t going to be easy. In the wake of Commodore’s demise, factories around the world had been sold, and parts suppliers moved on. Just restarting production of Amiga 1200s and Amiga 4000s required building a brand new factory in Scotland, which wasn’t able to deliver machines until October 1995. By then, it was a year and a half since Commodore had declared bankruptcy.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/amiga_magic_pack-980x1217.jpg" alt="An Amiga 1200 from the ESCOM Magic Pack. Note the new Amiga logo on the box and on a sticker, even with the old logo still embossed in the case.">
			<br>
			<i>An Amiga 1200 from the ESCOM Magic Pack. Note the new Amiga logo on the box and on a sticker, even with the old logo still embossed in the case.</i>
			<pre>
Still, Amiga Technologies soldiered on. The subsidiary company revealed a new Amiga logo and released a hardware and software bundle called the Amiga Magic Pack. This was an Amiga 1200, sold for £400 in a single box, along with two games, the Deluxe Paint AGA painting software, the Wordsworth 2 word processor, and Print Manager. A £500 bundle was released that added a hard drive and more productivity software. These Magic Packs were reasonably good deals, albeit saddled with aging computer hardware. Sales were poor in England but a little better in Germany, where Escom had greater marketing presence.

In Asia, Amiga Technologies licensed its hardware, not to New Star as everybody had expected, but to another Chinese company called Regent Electronics Corporation. With the help of ex-Commodore engineers, Regent designed and built a set-top box called the Wonder TV A6000, which was eventually released to a small set of Shanghai residents in 1997.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/wondertva6000.png" alt="Promotional image for the Amiga WonderTV A6000.">
			<br>
			<i>Promotional image for the Amiga WonderTV A6000.</i>
			<pre>
Petro was insistent that Amiga Technologies should operate as a viable business without asking for additional financial help from Escom. However, this meant that there was little money available for research and development. Eventually, he did find the funds to develop a prototype of the first new Amiga hardware since Commodore’s demise—it was codenamed the Walker.

The Walker was a modest iteration on the Amiga 1200. It bumped the CPU from a 68020 to a 68030 running at 40Mhz, kept the 1200’s AGA graphics chipset, and added an integrated CD-ROM drive. The whole kit and caboodle was stuffed into a small form-factor tower case that looked either like Doctor Who’s robot companion K9 or Darth Vader’s helmet, depending on which side of the pond you lived on.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/amiga_walker_publicity_pic.jpg" alt="Publicity photo for the ESCOM Amiga Walker prototype.">
			<br>
			<i>Publicity photo for the ESCOM Amiga Walker prototype.</i>
			<pre>
The Walker would have been a fantastic and competitive machine if it had been released in 1991; it was going to be woefully outdated in 1996. In the end it didn’t matter, because the product never made it to production. Escom had inherited what would become known as the “Commodore curse," and the company itself careened toward bankruptcy.

Escom had expanded too quickly in two years. The company had opened retail stores all over Europe, hoping for a huge increase in PC sales. Sales of PCs were in fact growing rapidly at the time, but the gains were mostly taken by small “white box” assemblers with lower overhead and lower prices.

Manfred Schmitt’s company had also backed the wrong horse at exactly the wrong time. On the eve of Windows 95’s release, Escom signed a deal with IBM to sell computers bundled with the ailing OS/2 operating system.

Escom declared bankruptcy in June 1996. As with Commodore UK, one of their subsidiaries (Escom Netherlands) kept operating and tried to buy out the mother company. That group went bankrupt themselves a year later. Escom became just another PC company on a huge rubbish pile of failed PC companies. Today, no one even remembers it—except, of course, for Amiga fans.
</pre>
			<h2>Limbo again</h2>
			<pre>
The beleaguered Amiga once again found itself between two companies. In the Escom bankruptcy sale, no fewer than 11 firms made bids on the assets. Quikpak, an American manufacturer and reseller, got out to an early lead before running into money issues. VIScorp, a startup company that had hired some ex-Commodore engineers, wanted to get the Amiga technology to put it into set-top boxes. They hired Carl Sassenrath, father of AmigaOS, who said: “I don’t plan on killing the Amiga. In fact, if they ask me to take over system development, you’ll see one killer Amiga!” Unfortunately, the company's plans came to a screeching halt as they, too, ran out of cash.

While the dreary saga of bankruptcy negotiations trundled on, a group of former Commodore employees decided to take matters into their own hands. John Smith (previously a sales manager at Commodore UK), Dr. Peter Kittel (a director of software at Commodore), and Dave Haynie (a legendary Commodore engineer) joined forces to found PIOS, a startup that would build the next generation of Amiga hardware that could also run other operating systems.

This was the same time that Apple was transitioning from the 68k series of CPUs to the PowerPC, and the latter was getting a lot of great press in technical circles. Some Amiga accessory companies, like the German-based Phase5, had started shipping PowerPC accelerator boards for classic Amigas. These could speed up specific software routines in applications that supported them, even if the operating system itself was still stuck running on the 68k chip.

Apple had also opened up licensing for MacOS for the first time in its history, allowing legal Macintosh clones to exist. The PIOS ONE was planned as a computer that could run almost anything: either a PowerPC flavor of Linux, a licensed copy of MacOS, a copy of the brand-new operating system BeOS (from a company run by ex-Apple manager Jean-Louise Gassee, who billed his operating system as the second coming of the Amiga and even had vanity license plates that read “AMIGA96”), or an “Amiga-like” PowerPC operating system that PIOS hoped to develop, called pOS.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/pios-composite-photo-980x649.jpg" alt="The PIOS logo, prototype case, and motherboard.">
			<br>
			<i>The PIOS logo, prototype case, and motherboard</i>
			<pre>
The possibility of a computer that could run any operating system was a nerd’s wet dream, but the dream died a few years later. Apple withdrew its licensing program, BeOS transitioned to x86, and pOS failed to appear. PIOS had also hoped to win the Escom auction, but like many others, the company was outbid.

The eventual winner was another PC clone company founded by Ted Waitt, the son of four generations of cattlemen. Founded in 1985, it had risen to five billion in sales by 1996, selling PCs in cow-painted boxes by mail order, in showrooms, and later online. This was Gateway 2000.
</pre>
			<h2>The Gateway Era</h2>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/gateway-buys-the-amiga-eric-schwartz.jpg" alt="Animator Eric Schwartz created this image after the Gateway buyout.">
			<br>
			<i>Animator Eric Schwartz created this image after the Gateway buyout.</i>
			<pre>
Gateway 2000 had experienced phenomenal growth over the last few years, and it was now considered one of the giants in the PC world. As a giant, the company came under the radar of the wounded but still mighty IBM. IBM, angry about the PC clone industry it had unwittingly brought into existence, was starting to push its massive legal weight around.

While PC clones were still legal, IBM had a portfolio of patents that it was brandishing as a stick, trying to extract licensing revenues from any cloner company wealthy enough to be worth its time. Larger companies like HP could respond with their own patent portfolio and negotiate a “Mutual Assured Destruction” cross-licensing agreement, but the young Gateway had few if any patents to speak of. It was at this point that Jim Collas, senior VP of product development for Gateway 2000, was put in charge of patent acquisition.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/jimcollas.jpg" alt="im Collas, director of Amiga Technologies at Gateway.">
			<br>
			<i>Jim Collas, director of Amiga Technologies at Gateway.</i>
			<pre>
Collas was a technical manager with a degree in electrical engineering and computer science from UCLA. When he was still at school, he had run across the Amiga (still in prototype form) while working for a startup game developer, and he'd been impressed by the technology. After graduating, Collas started a PC design company that was acquired by Gateway in 1992.

So when the opportunity arose, he jumped at the chance to acquire the Amiga assets. Working with the resilient Petro Tyschtschenko, he arranged a winning bid of $14 million for Amiga Technologies and all Commodore patents (the Commodore brand was sold separately, ending up with a Dutch PC company called Tulip). The deal closed in early 1997.

Collas' bosses, of course, were only interested in the extensive patent portfolio, which included patents on a type of dropdown menu and the two-button mouse. But he saw a little further. In an interview with Ars, Collas said that "the combination of the Amiga brand, the available technology at the time, and the passion of the Amiga industry made me feel like there was a once-in-a-lifetime opportunity for making significant change in the industry and the world."

By 1998, he was losing confidence in Gateway’s upper management and their vision for the company. He correctly realized that PCs were becoming a commodity item, and profit margins were going to keep shrinking over time. Without any differentiating technology, Gateway would eventually get squeezed out by smaller firms that could sell identical PCs for less money. Cow-painted boxes could only get you so far.

When the purchase from Escom was concluded, Collas became the head of Gateway’s Amiga Technologies subsidiary. He kept Petro on as a consultant and quickly assembled a new team of engineers, designers, managers, and marketers, as if he was building a brand new company. He hired former Commodore Director of Operating Systems Software Dr. Allan Havemose to head up engineering, Joe Torre (no, not that one) to lead the hardware division, Fleecy Moss to handle developer relations, and Bill McEwen as a sort of marketing and software evangelist (think Guy Kawasaki).

While Collas was assembling his team, Gateway received a torrent of letters from Amiga owners and fans who were desperate to know what the company was going to do with the technology. These letters helped Collas convince his bosses that he was on the right track.

The technology world in 1998 was experiencing tumultuous change. The rise of the Internet in everyday life changed the way people looked at computers. Laptops, once a niche market, were rising in popularity, and the first usable generation of Wi-Fi was about to become ratified as a standard.

It was this environment that shaped Jim Collas’ vision. He saw a new line of computers that would span everything from tablets to workstations, all running the same software. He even imagined a standardized method for users to purchase this software digitally. These new Amiga computers could be workstations, game consoles, set-top-boxes, or Internet terminals, or all of them at the same time.

Bringing this vision to life would require a drastic shift away from the traditional Amiga hardware and software architecture. The first thing it would need was a new operating system kernel: something that was fast and lightweight enough to run on the minimal hardware of a tablet but strong enough to also run graphically intensive applications. Collas found what he was looking for with QNX’s Neutrino, a real-time operating system that famously could run an entire 32-bit preemptively multitasking OS with a graphical user interface and a Web browser, all from a single 1.44MB 3.5-inch floppy disk.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/qnxdemoboot.png" alt="QNX's famous 1.44MB floppy demo. It really worked!">
			<br>
			<i>QNX's famous 1.44MB floppy demo. It really worked!</i>
			<pre>
But the kernel was not enough to deliver the complete vision. Dr. Havemose drew up plans for what he called the Amiga Operating Environment (OE)—a software layer on top of the kernel that would handle the graphical interface and provide a standardized development platform. Programmers would write to the OE layer, using a high-level language like Java, and their applications could run on any Amiga OE compatible hardware. When Dr. Havemose told Collas about his ideas, Collas thought that "he was either the most brilliant software architect I had ever met, or he was delusional and fooling me." Collas brought in some experienced software architects to look at the plan, and they concluded that the plan was indeed brilliant.

The Amiga Operating Environment was another piece of the puzzle. There was also a plan for an online store, much like today’s Apple App Store or Google Play Store, where consumers could buy new Amiga OE applications directly through the Internet.

The first hardware to be prototyped was called the Amiga MCC, which stood for Multimedia Convergence Computer. It was designed to work either as a set-top box and gaming console connected to the television or as a standalone computer with a traditional monitor. The MCC tablet would be the second piece of hardware to be delivered later.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/amiga-mcc-monitor-and-computer-composite.jpg" alt="Promotional image of the Amiga MCC with a monitor (left), a prototype computer case (right), and a blurry image of a prototype MCC tablet (bottom right).">
			<br>
			<i>Promotional image of the Amiga MCC with a monitor (left), a prototype computer case (right), and a blurry image of a prototype MCC tablet (bottom right).</i>
			<pre>
None of this new hardware or software would be directly compatible with the classic Amiga architecture. This concerned some Amiga fans, particularly the ones running companies like Phase5 and Haage & Partner that were selling PowerPC accelerator cards to extend the life of existing Amigas. In fact, the announcement of the Amiga MCC and OE shocked these companies so much that they immediately settled their differences (both companies had been developing incompatible standards for PPC accelerator cards) and agreed to work together to continue developing classic Amiga hardware and software.

Collas, for his part, felt that "the Amiga brand was about revolutionary change in computing." He saw the new MCC devices as being in the spirit of the original Amiga, rather than a direct descendent. The hardware was to be cutting edge, even going so far as to use liquid cooling to overclock industry standard graphics cards.

It was at this point that Fleecy Moss was fired by Gateway for unknown reasons. He immediately announced to the world that he was joining forces with Dave Haynie to develop his own idea of the next-generation Amiga platform, called KOSH (for Kommunity Oriented Software and Hardware). Aside from a number of fanciful blog posts, this never ended up amounting to anything.

As 1998 rolled into 1999 and Gateway (the company dropped the "2000" from its name around this time) continued to work toward its Amiga OE vision, Haage & Partner released AmigaOS 3.5, an official update for classic Amigas. This fixed some bugs from Commodore’s final 3.1 release and added some bundled software. The traditional Amiga market was dwindling by this time, but there were still enough people using the old hardware to warrant development.

Then came an announcement that caused an uproar in the Amiga community; some even saw it as a betrayal. Jim Collas’ team revealed that it was switching the OS kernel for Amiga OE from QNX to Linux. He met with Linus Torvalds to discuss his plan, and Linus really liked Collas' ideas.  There were even plans for Linus to attend Amiga shows and meet key Amiga community figures.

In retrospect, this was probably the correct choice. Linux, then and now, supported a much more diverse array of hardware and was already gaining significant momentum in the tech industry. But Amiga fans didn’t see it that way at the time. They saw Linux as bloated, clunky, and slow, the exact opposite of their beloved Amiga. QNX Neutrino was seen as fast, small, and elegant, and the community had rallied behind it. In fact, QNX itself was surprised and taken aback by the sudden shift in plans.

In response, QNX immediately announced a partnership with Phase5 to develop a QNX Neutrino operating system for Amigas with Phase5 PowerPC accelerator cards, as well as a planned new QNX-based Amiga computer called the AMIRAGE K2. The use of the word “Rage” and all capital letters was seen as a deliberate statement of defiance against Gateway itself, although Phase 5 jokingly said it was pronounced "A-MIRAGE." Unbeknownst to the Amiga community and fans, however, there were much bigger and more troubling conflicts on the horizon.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/10/gateway-2000-logo-980x216.jpg" alt="Gateway was changing more than its name and logo." style="width: 50%;">
			<br>
			<i>Gateway was changing more than its name and logo.</i>
			<pre>
Jim Collas’ ambitious plans for Amiga Technologies was running, as he had predicted, head on into interference from Gateway’s conservative upper management. At the center of the controversy was Gateway’s new CEO, Jeffrey Weitzen.

Weitzen had come from AT&T and was skeptical of Collas’ Amiga vision. Collas felt that he had to fight his own CEO for control of the subsidiary, as Weitzen could potentially torpedo the plan. Collas worked out a scheme with his own CFO where Amiga Technologies would purchase enough shares from its parent company to get 51-percent ownership and operational autonomy. He hoped that Gateway would not want to shut down the division at that point because it would end up taking a massive write-down for the quarter. Collas was on good terms with Gateway’s founder, Ted Waitt, and thought that this would provide him with at least some political protection.

Weitzen, however, played the political game better. He got that CFO to reveal these plans to him, then waited until Ted Waitt was on vacation before calling Jim Collas into his office.

Weitzen told him to sit down. He said that he was planning to sell the Amiga division and that Collas' plans would not be realized. Collas knew that he had lost, so he next sold his Gateway shares and left the company. Ironically, a few quarters later, the stock crashed from $35 to under $10, and Weitzen was forced out as CEO. But by then it was too late. Collas' grand vision—the MCC, the Amiga OE, the computers and tablets, the online application store—all vanished into the ether.
</pre>
			<h2>Amiga flies free</h2>
			<pre>
Gateway carried through with its plans to sell the Amiga division, but this time there weren’t as many takers. Mostly this was due to the fact that Gateway still wanted to retain all of Commodore’s old patents, which was the primary reason it had been interested in buying the Amiga in the first place

The diehard fans in the Amiga community were hanging on, but hope was dwindling. Outside of the PowerPC acceleration boards, there had been no new Amiga computer hardware since the Amiga 1200 and 4000 were released back in 1992. Amiga software companies were struggling as well. Amiga magazines were getting thinner, and some were going out of business completely. Newtek, the maker of the Video Toaster, had already moved its hardware and software to the Windows platform.
An Amiga 4000 equipped with a dedicated video card and an accelerator board featuring both a 68060 main CPU and 604e PowerPC accelerator chip, running native Amiga software, was still competitive speed-wise with contemporary Windows 98 and Macintosh computers. However, this wasn’t a computer you could easily buy—it had to be crafted together from old and new parts, and there was no guarantee that the operating system would ever be updated to be fully PowerPC native. It was also expensive, as the volumes for these add-on boards were low.

At the same time, the idea of a brand-new, revolutionary Amiga architecture as promised by Gateway was now just as far away as it had ever been. Petro Tyschtschenko had offered to buy the division himself, but he could not find enough money to do so. He was asked by Gateway to find a new buyer. Who would pick up the torch and carry on with this dream?

The people who decided to do that were ex-Gateway employees who had been bitten by the Amiga bug during their brief time with the company. Bill McEwen (the technology evangelist) and Fleecy Moss (the former developer relations manager) joined forces and created a startup they called Amino Development. Together they raised $5 million from venture capitalists.

How did a couple of mid-level Gateway employees manage to get this kind of money? It wasn’t difficult. This was late 1999, and the Internet “Dotcom” bubble was heating up. Investors were happy to throw money at anything remotely computer or Internet-related, and Amino Development promised to be both of those things.

Amino Development took its $5 million and purchased all the non-patent portions of Amiga Technologies from Gateway. It then immediately changed the name of the company to Amiga Technologies, Inc.

The deal was closed at the final hour on the last day of December 1999. The old millennium was coming to an end. There was a feeling in the air, something that Federal Reserve Chairman Allan Greenspan had called “irrational exuberance." It was a feeling that anything was possible.

For the first time since a group of dreamers led by Jay Miner had banded together in 1982 to create something amazing, Amiga was an independent company again. The little computer that could had survived a rocky stewardship under Commodore, Escom, and then Gateway, but now it was on its own.

The future was wide open. Surely the next millennium would usher in something even more amazing. With Internet and computer companies’ stock prices soaring, it seemed like the sky was the limit, even for the Amiga.

What could go wrong?
</pre>
		</div>
		<div class="content" id="12">
			<h1>A history of the Amiga, part 12: Red vs. Blue</h1>
			<i>Amiga was now an independent company again, but trouble was brewing.</i>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/red-vs-blue1.jpg" alt="">
			<pre>
The year 2000, which once seemed so impossibly futuristic, had finally arrived. Bill McEwen, president of the new Amiga Inc., celebrated with a press release telling the world why he had bought the subsidiary from Gateway Computers.

“Gateway purchased Amiga because of Patents; we purchased Amiga because of the People.” It was a bold statement, the first of many that would come from the fledgling company. Amiga Inc. now owned the name, trademark, logos, all existing inventory (there were still a few Escom-era A1200s and A4000s left), the Amiga OS, and a permanent license to all Amiga-related patents. They had also inherited Jim Collas’ dream of a revolutionary new Amiga device, but none of the talent and resources that Gateway had been able to bring to bear.

To chase this dream, Amiga Inc. would have to look elsewhere. McEwen thought he had found the answer in an obscure British technology startup. This was the Tao Group, started by Francis Charig, a UK businessman, and Chris Hinsley, a talented Atari and Amiga games programmer who wrote in assembler.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/tao-quake-980x735.jpg" alt="The Tao Group's TAOS operating system, running on Windows.">
			<br>
			<i>The Tao Group's TAOS operating system, running on Windows.</i>
			<h2>The Tao Group and Amiga Anywhere</h2>
			<pre>
Tao had created a product that was so innovative that few people understood what it actually was. Taos was an operating system that was coded in VP1, an advanced assembly language that used instructions for an imaginary, idealized RISC CPU. When Taos programs were loaded into memory, the system translated the VP1 opcodes into the equivalent ones for whatever CPU it happened to be running on. Taos could run on an x86, a MIPS, a PowerPC, or a transputer, and many more—or even different combinations running at the same time. Because VP1 instructions were more compact than most CPU’s native opcodes, Taos programs would often load and run faster than native ones, even when you included the time it took to do the translation. Taos was a little bit like magic.

As cool as it was, Taos had a hard time finding buyers in the marketplace. So the group doubled down and added new features to make it more attractive. The people at Taos wrote a graphical user interface and support for multimedia. They wrote a Java virtual machine (JVM) so that users wouldn’t have to write applications in VP1 assembler. There was little money in JVMs, but there was a market for full-fledged operating systems that ran on a tiny amount of resources, could run on different CPUs, and supported Java applications. This was the burgeoning world of personal digital assistants (PDAs).
</pre>
			<div class="slideshow">
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/DE-PDA-2-zaurus_m1e1.jpg" alt="A Sharp Zaurus Personal Digital Assistant (PDA)">
				<i>A Sharp Zaurus Personal Digital Assistant (PDA)</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/DE-PDA-3-zaurus.jpg" alt="The Sharp Zaurus with AmigaDE software">
				<i>The Sharp Zaurus with AmigaDE software</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/DE-PDA-4-zaurus.jpg" alt="Closeup of the Sharp Zaurus running AmigaDE">
				<i>Closeup of the Sharp Zaurus running AmigaDE</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/DE-PDA-5-zaurus.jpg" alt="The Sharp Zaurus running software on top of AmigaDE">
				<i>The Sharp Zaurus running software on top of AmigaDE</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/DE-PDA-1-arm_laptop_sm.jpg" alt="An ARM-based laptop running the Amiga DE">
				<i>An ARM-based laptop running the Amiga DE</i>
				</span>
			</div>
			<pre>
pocketable devices that could keep track of your appointments, record notes, and sometimes take pictures. Palm was the biggest player in this space, but plenty of other companies wanted in on the action.

Bill McEwen saw the opportunity to get in on the ground floor of a new market, and he licensed the full stack of Tao Group’s technology. He called it the “Amiga Digital Environment” or AmigaDE, although it would be later branded as “Amiga Anywhere.” McEwen even made an appearance on TechTV with Leo Laporte to demonstrate how you could take a single SD card with an Amiga Anywhere-branded 2D shooter game and run it on Windows and a whole host of incompatible PDAs with different CPU architectures. It was an impressive demo, but what it had to do with the Amiga wasn’t clear exactly.

Amiga Inc. also announced a deal with Hyperion Entertainment—a company that ported older games to Linux, Macintosh, and Amiga systems—to convert its games to the AmigaDE.
</pre>
			<iframe src="https://www.youtube.com/embed/HfHcwpzxSdk" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br><i>Bill McEwen tries to bring the Amiga back.</i>
			<h2>The split begins</h2>
			<pre>
The remaining Amiga community reacted to these announcements with confusion. Amiga Inc. made a vague promise that old software would run on the AmigaDE through emulation, but this didn’t provide a bridge for people with existing hardware.

To mollify the community, Amiga Inc. announced a partnership with Haage & Partner to make a new version of WarpOS that would run the AmigaDE environment. WarpOS wasn’t really an OS at all, but a PowerPC library that sped up certain Amiga programs on PPC accelerator cards. It was a replacement for PowerUP, the library that shipped with Phase5’s PPC accelerators. The divide between WarpOS and PowerUP had been contentious in the past, before both sides had agreed to an uneasy truce. Now, the stage was set for this old rivalry to split the Amiga community in two.
</pre>
			<h3>Merlancia</h3>
			<pre>
It was a heady time, and the dotcom mania attracted both legitimate and dubious investors. One example of the latter was Ryan Czerwinski, who claimed to be 40 years old, a Ph.D., and the president of Merlancia Industries. He arranged meetings with Amiga Inc. and even hired legendary Commodore engineer Dave Haynie to work on new Amiga PowerPC hardware. It turned out in the end that Czerwinski was a teenager living with his mother, and Merlancia was just a bunch of ideas in his head. Haynie, who was now owed $55,000 for his consulting work, was left scarred by the experience. After the failed PIOS startup and now the Merlancia debacle, his heart was broken. He would never work on Amiga-related technologies again.
</pre>
			<h3>MorphOS</h3>
			<pre>
In October of 2000, Haage & Partner released the final version of the classic Amiga operating system, AmigaOS 3.9. In the same month, Petro Tyschtschenko announced his retirement and the closing of his office in Germany. All the old Escom A1200s and A4000s were finally gone. It was the end of an era.

Also vanishing by this point was the PowerPC accelerator company Phase 5, which had gone into bankruptcy. But some of the former employees of Phase 5 formed a new company named bPlan and partnered with a software company called Thendic. Thendic was run by Bill Buck, formerly of VIScorp, who had helped pay the salaries of Amiga Technologies employees during the Escom bankruptcy. bPlan and Thendic got to work on a dream they had been imagining since 1995—a fully PowerPC-based Amiga with a native Amiga operating system.

Some of the pieces were already there. The PowerUP library, for one, but there was also the Picasso graphics library that supported non-Amiga display chipsets, a new file system called SFS, and other components from the open-source Amiga Replacement OS (AROS) project. All it really needed was a new microkernel, and when Ralph Schmidt wrote one called Quark, the old PowerUP system had finally morphed into a full operating system in its own right. Thus it became dubbed “MorphOS.”
</pre>
			<div class="slideshow">
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/morphos-01.jpg"
					alt="An early version of MorphOS.">
				<i>An early version of MorphOS.</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/morphos-02.jpg"
					alt="MorphOS borrowed heavily from AmigaOS">
				<i>MorphOS borrowed heavily from AmigaOS</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/morphos1.jpg"
					alt="MorphOS could be heavily themed.">
				<i>MorphOS could be heavily themed</i>
				</span>
				<span>	
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/morphos2.jpg"
					alt="Another MorphOS screenshow">
				<i>Another MorphOS screenshow</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/morphos3.jpg"
					alt="Another MorphOS screenshow">
				<i>Another MorphOS screenshow</i>
				</span>
			</div>
			<h3>Amithlon and AmigaOS XL</h3>
			<pre>
It could have been called AmigaOS 4.0, but those naming rights were held by Amiga Inc. Bill Buck called up Bill McEwen at Amiga Inc. to try and work out a deal. At the time, Amiga Inc. had just announced new PowerPC Amiga hardware with a company in the UK called Eyetech. Buck wanted to get a license for the old Amiga OS 3.1 source code to fill in the remaining bits of MorphOS and to license the AmigaOS 4.0 name. But Amiga Inc. had now announced its own version of AmigaOS 4.0, which was to be written by Haage & Partner. As of mid-2001, Buck and McEwen had still not come to an agreement.

Haage & Partner, meanwhile, had different ideas about where the future of AmigaOS should go. This turned a two-way disagreement into a three-way split and rocked the tiny Amiga community to its core.

It all started with a new emulator for classic Amiga software, written by Bernd “Bernie” Meyer and Harald Frank. This emulator used “Just-In-Time” (JIT) translation technology to speed up execution. Benchmarks showed that a 1GHz Athlon CPU could run Amiga programs (at least ones that didn’t use the old custom chips) at a speed equivalent to a 450Mhz 68040, which turned a typical PC into an Amiga 4000 that was 10 times faster than the original.

Meyer had integrated the emulator into a custom version of Linux, which would boot directly into the Amiga’s Workbench environment and even translate Amiga OS API calls directly into their Linux equivalents. Haage & Partner was more interested in a solution that used the x86 version of QNX Neutrino as the base operating system. In typical Amiga fashion, the company chose both and shipped two CDs in the same package. The former was called Amithlon, and the latter dubbed AmigaOS XL.
</pre>
			<div class="slideshow">
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/amilogo-1.jpg" alt="The Amithlon logo">
				<i>The Amithlon logo</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/amithlon-1-1440x900.png" alt="A screenshot of Amithlon">
				<i>A screenshot of Amithlon</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/amithlon-review-1.jpg" alt="Amithlon running on a PC laptop">
				<i>Amithlon running on a PC laptop</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/amithlon-review-2.jpg" alt="AmigaOS XL running on a PC laptop">
				<i>AmigaOS XL running on a PC laptop</i>
				</span>
			</div>
			<pre>
The sudden existence of these two solutions stunned the Amiga community. On one hand, it was great that the community could run its old software much faster on commodity PC hardware. But these emulation solutions had no support for the PowerPC accelerator boards that many people already owned and had no potential for new features in the future. Like all emulators, they were frozen in time, preserving the past but never moving forward.

But the transitional nature of Amithlon and AmigaOS XL wasn’t the biggest problem. Bernd Meyer found out that Haage & Partner had never received a license for the ROMs and operating system from Amiga Inc. Not wanting to expose himself to a lawsuit, Meyer officially withdrew his support from H&P and signed a new contract with Amiga Inc. H&P’s response was to demand that Amiga Inc. transfer ownership of Amithlon or H&P would leave the Amiga market entirely. Amiga Inc. did not agree to this, and H&P made good on its threat.

In a statement, Fleecy Moss said that H&P had done “a good job” with AmigaOS during the Gateway years, but that “there can be only one captain and course to steer.”

With H&P gone, the captain had no crew left to work on the operating system. The company signed a new contract with Hyperion to write AmigaOS 4.0. By this time, several things had happened, both in the world at large and with Amiga Inc. The attacks on September 11 propelled what had been a shaky dotcom market into full-blown collapse. Investors abandoned the fledgling Amiga Inc. Bill McEwen tried to put on a good front, insisting that the company was merely “moving offices,” but what was really happening was that Amiga Inc. existed now in name only. Court documents would later reveal that the company had shrunk to just Bill McEwen and Fleecy Moss, whose only income was the rapidly dwindling sales of Amiga Anywhere-compatible PDA game packs from the amiga.com website.

Hyperion, knowing of Amiga’s financial condition, stipulated in its contract for developing OS 4 that ,if Amiga Inc. were ever to go bankrupt, all rights to the software would revert to Hyperion itself.
</pre>
			<h2>Red versus Blue: The AmigaOne and Pegasos</h2>
			<pre>
By late 2002, the first AmigaOne motherboards had arrived from Eyetech, but there was no OS 4 ready to run on them. bPlan and Thendic, now merged into a single company called Genesi, had released its own PowerPC motherboards which it called the Pegasos. The Pegasos boards came with a beta version of MorphOS. Both boards were based on a reference design called the Teron, developed by the China-based Mai Logic. Mai had developed the boards for industrial and embedded applications where Linux was the standard. The boards were new and had some rough edges. Bill Buck claimed that Genesi had developed a hardware fix (which the company dubbed “April”) for some of the bugs and used this as a way to plead for a reconciliation of the OS 4/MorphOS divide.

This was the infamous “There is no Mai without April” post. “This market and community is in complete confusion,” Buck wrote. “There is no leadership or vision, and we need both fast or we can forget it. This is a public statement in good faith to Eyetech and Hyperion. Allan and Ben are formidable marketing opponents. We can forget the past if you can. We need to get into the same boat.”

His words fell on deaf ears. Both the red (OS 4) and blue (MorphOS) camps continued with neither willing to compromise with the other. On community forums, people were forced to choose sides.

It was not a large market to begin with, and splitting it only made things worse. Only 400 Pegasos boards were shipped in the first batch alongside a slightly higher number of AmigaOne systems.
</pre>
			<div class="slideshow">
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/amigaone-g3-motherboard.jpg" alt="The AmigaOne G3 motherboard from Eyetech">
				<i>The AmigaOne G3 motherboard from Eyetech</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/amigaone-g4.jpg" alt="The AmigaOne G4 motherboard from Eyetech">
				<i>The AmigaOne G4 motherboard from Eyetech</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/pegasos.jpg" alt="The Pegasos motherboard">
				<i>The Pegasos motherboard</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/pegasos2.jpg" alt="The Pegasos II motherboard">
				<i>The Pegasos II motherboard</i>
				</span>
			</div>
			<h2>OS 4 appears, hardware disappears</h2>
			<pre>
In 2004, the first release of Hyperion’s OS 4 was made available as a “Developer Preview.” It ran on the AmigaOne motherboards, but not the Pegasos. Eyetech also announced a small, mini-ITX form-factor board called the Micro AmigaOne, which I bought and reviewed for Ars. While the AmigaOne had some rough edges, it was the first completely new Amiga system released since 1992’s Amiga 4000, and the old Amiga magic was still there for anyone who wanted to see it. I fitted mine with a Flash card connected to a SATA interface, which offered me a brief glimpse into a future of fast SSD-based computers that would boot in under seven seconds.

Unfortunately, this joy was short-lived. Mai Logic went bankrupt, and Eyetech, with nothing left to sell, closed down its business. Now that OS 4 was officially out, there was no hardware left to buy to run it on. Many people begged Hyperion to make an x86 version of OS 4 to run on commodity PC hardware, but chairman Ben Hermans held fast against the idea, calling it “disastrous”.

Hermans had seen how Microsoft had destroyed the market for non-Windows operating systems on the PC. He pointed out that both Be Inc. and the Linux game company Loki had gone out of business. He offered as evidence the fact that Hyperion sold more game ports on the Amiga platform than it did on Linux, despite the latter having a larger installed base by several orders of magnitude. Hyperion’s income mostly came from sales of its Macintosh game ports. However, it kept developing OS 4 as a labor of love and in hope that the market would grow again.

Later that year, Amiga Inc. used some sleight of hand to escape a pending bankruptcy. Amiga sold its assets to a shell company called KMOS—a Delaware firm headquartered in New York—then renamed KMOS back to Amiga Inc. It tried to use these shenanigans to get out of the clause in its contract with Hyperion that would revert ownership of OS 4 if Amiga Inc. ever went under. Then, to top it off, Amiga sued Hyperion for not delivering OS 4 on time and demanded the return of all source code.

As the lawsuit dragged on, Bill McEwen decided the best use of his time was to get Amiga Inc. back in the news by sponsoring the new “Amiga Center at Kent” stadium, home of the Seattle Thunderbirds hockey team. The company did get in the local news, but three months later made even bigger news when it was dropped for failure to make the downpayment. It was an embarrassing moment for all involved.
</pre>
			<h2>New hardware and the end of the line for Amiga Inc.</h2>
			<pre>
Astoundingly, this wasn’t the end of the Amiga story. While the “parent” firm was clearly no longer a company, work on new hardware and software continued. Hyperion released AmigaOS 4.1, and a company called ACube released a pair of PowerPC motherboards called the Sam440ep and Sam440ex that came bundled with the new OS.

In September 2008, the lawsuit between Hyperion and Amiga Inc. was settled. Hyperion won and was granted complete rights to the Amiga OS 4 operating system. By this time, the Tao Group had also gone bankrupt, leaving Amiga Inc. with nothing at all to sell.

Meanwhile, MorphOS was soldiering on despite troubles of its own. A group of coders claimed that it had not been paid for its work and launched a website to protest against Genesi. When Genesi did not respond, the author of the Ambient desktop (a clone of the Amiga Workbench that MorphOS used) open-sourced the project. Genesi also was forced to stop selling the Pegasos line due to new EU restrictions on lead-based solder. The company did, however, release a new low-end micro-ITX board called the Efika. MorphOS continued to be developed and, for a brief moment, ran on Pegasos boards, the Efika board, some models of PowerPC Macintoshes, and even the Sam440ex. Owners of the latter were the first people to ever successfully dual-boot OS 4 and MorphOS on the same hardware.

In 2010, a new hardware company entered the scene. Partnering with Hyperion, A-Eon technologies announced the Amiga X1000, a high-end Amiga system with a dual-core PowerPC CPU and a mysterious “accelerator” chip called the Xena. A-Eon was founded by Trevor Dickinson, an English entrepreneur who made his fortune bringing technology to the oil and gas industry. In the 1990s, he had used Amigas in his business for graphics work, video, and desktop publishing.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/trevor-dickinson-980x1480.jpg" alt="Trevor Dickinson">
			<br><i>Trevor Dickinson</i>
			<pre>
In the fall of 2010, I met Trevor at the 25th anniversary of the Amiga, held at the annual AmiWest convention in Sacramento, California. He was an enthusiastic and well-spoken supporter of the Amiga. He admitted that the X1000 and its successor the X5000 were pure passion projects, driven out of the love of the platform rather than any expectation of making money.

“I am first and foremost an Amiga enthusiast and wish to continue the legacy and tradition of the groundbreaking Amiga computer,” he told Ars in an interview. “As long as there are people who, like me, want to keep the dream alive, I will continue to fund Amiga hardware and software development to make it happen.”

Between A-Eon and Hyperion, the Amiga platform finally had new hardware and software for the foreseeable future. Work continues today on software support and drivers for new Radeon graphics cards and a less expensive, small form-factor A1222 motherboard.
</pre>
			<div class="slideshow">
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/05/IMG_6063-e1494366293678-1440x1080.jpg" alt="Internals of the X5000.">
				<i>Internals of the X5000.</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/05/IMG_6062-e1494366170636-1440x1920.jpg" alt="The X5000 as delivered.">
				<i>The X5000 as delivered.</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/05/IMG_6067-e1494366361120-1440x1920.jpg" alt="The X5000 mouse.">
				<i>The X5000 mouse.</i>
				</span>
				<span>
				<img src="https://cdn.arstechnica.net/wp-content/uploads/2017/05/IMG_6074-e1494366379823-1440x1080.jpg" alt="This is the splash screen that displays when the X5000 is turned on.">
				<i>This is the splash screen that displays when the X5000 is turned on.</i>
				</span>
			</div>
			<h2>Where they are now</h2>
			<pre>
MorphOS continued to be developed and sold, with the last version, 3.9, coming out in 2015. Genesi, however, pivoted its business to selling ARM-based hardware running Linux to the embedded market. Today, Bill Buck is busy fulfilling contracts with the US military for its technology modernization project.

Haage & Partner still exist today as a distributor for Windows-based utility programs. The company sell a Windows version of Directory Opus, which was a popular file manager for the Amiga.

Hyperion shipped OS 4.1 Final Edition Update 1 to coincide with the release of the X5000. Ben Hermans resigned from the board of directors of Hyperion in 2016, although he still owns shares in the company. On its website, Hyperion speculates about the possibility of an OS 4.2 but warns that development depends on the sales of 4.1 Final Edition.

Bill McEwen continued the pretense of owning Amiga until 2016 when he failed to renew the copyrights on the Amiga name. The amiga.com website shut down later that year, becoming a parked domain with only a single email address listed. On his LinkedIn profile, McEwen lists himself as the CEO of Amiga Inc. from 2000 to 2016. He is currently working as the Director of Operations for DC Logistics, a freight forwarding company.

Fleecy Moss has vanished from the Internet.
</pre>
			<iframe src="https://www.youtube.com/embed/9mg6wrYCT9Q" title="YouTube video player" frameborder="0" allowfullscreen></iframe>
			<br><i>Eric Schwartz has the last word.</i>
			<h2>The Amiga dream</h2>
			<pre>
It is tempting to dismiss the efforts of Amiga, Inc. in the 21st century as the deranged thrashings of delusional and incompetent people. Yet it is worth examining exactly what it was about the Amiga that drove so many people to try and revive a dwindling market. No other platform has experienced this. Nobody fought bitterly over the control of a successor to the Atari ST. There weren’t multiple efforts to revive and modernize the TRS-80. Why did the Amiga make people behave in this way?

Part of it was the uniqueness of a platform that was literally ten years ahead of its time. You can take an Amiga 1000 from 1985 and use it today as if you were using a modern (albeit slow) computer. It has a graphical user interface, color, stereo sampled sound, long file names, and pre-emptive multitasking. You can even, with the appropriate peripherals, connect it to the Internet. The equivalent Macintosh at the time had only a 9-inch monochrome screen and everything halted as soon as you held down the mouse button. A typical PC from 1985 was even more ancient, usually sporting a text-based display and a command-line only, single-tasking DOS.

To be so far ahead was both a blessing and a curse for the Amiga. The mainstream technology press didn’t quite understand it. The press either pretended like it didn’t even exist or published dismissive screeds claiming that nobody needed color, sound, or multitasking in a business environment. Ten years later when Windows 95 appeared, these same features were touted as innovative and exciting.

So when Commodore’s inept and malicious management imploded the Amiga’s parent company in 1994, fans of the platform were naturally distressed. Things got even worse when successor companies inherited the “Commodore curse” and either died themselves or downsized and sold off the Amiga division.

The dotcom boom of the late ‘90s and early 2000s seemed to give the Amiga one last chance at a rebirth. Investors had tons of money to spend, and the Internet revolution promised the opportunity for a computing world that wasn’t dominated by Microsoft and Windows.

Unfortunately, the two people involved in running the new Amiga Incorporated, Bill McEwen and Fleecy Moss, were not equipped to lead this revolution. They knew that the future belonged to mobile computing, and this led to the partnership with the Tao Group and the Amiga Anywhere platform. However, they didn’t have the skills to manage the transition to a new platform while simultaneously unifying and migrating the old Amiga hardware and software.

Ironically, the new platform nearly survived on its own. One of the most tantalizing licensees of the Tao Group was a Motorola phone, the P1100, running on an 11 MHz ARM processor with a monochrome bitmapped screen. It ran the full Tao Intent stack, including a GUI and downloadable Java applications, featured a large (for the time) touch screen. It was scheduled for release in late 2000. Unfortunately, Motorola cancelled the project. The world might have been very different if the project had stayed alive.
</pre>
			<img src="https://cdn.arstechnica.net/wp-content/uploads/2018/02/motorola-phone3p-980x414.jpg" alt="Motorola P1100 Phone" style="width: 40%;">
			<br><i>Motorola P1100 Phone</i>
			<pre>
Ultimately, the Amiga wasn’t just the set of custom chips with names like Agnes, Paula, and Daphne. It wasn’t just the Kickstart ROM chips or the Workbench interface that made up the AmigaOS operating system. The Amiga was an idea. It was the idea of a personal computer that was easy to use and fun, powerful enough to run cutting-edge games and applications, but still understandable by a single person. It was possible to know and recognize every file in the operating system and even comprehend how the custom chips worked on a fundamental level. Today, we have computers that are tens of thousands of times more powerful, but nobody would ever pretend to understand how every part of Windows works. Something has been lost.

The Amiga didn’t just play great games. It offered a glimpse into a sci-fi future, where affordable personal devices could allow ordinary people to edit video and create new three-dimensional worlds in software. But it was more than that. The Amiga, unlike any other computer that followed it, had both a soul and a heart.
</pre>
			<h2>The future is wide open</h2>
			<pre>
The present-day situation of the Amiga, incredibly, is better than it has been in decades. The machine's impact has been fondly remembered in series like this or in documentaries like 2017's Viva Amiga. And you can even purchase new hardware that runs an updated AmigaOS, allowing a new generation to dig into the hardware and software while running a modern Web browser and interacting with the rest of the computing world at a comparable level.

Coincidentally, we tested that new Amiga—the Amiga X5000—and it brought back plenty of familiar feelings:

>"The X5000 is a strange beast. It’s like a window into an alternate universe where Commodore never went bankrupt and the Amiga platform never died. The fact that both the hardware and operating system were produced at all is a monument to the passion and dedication of the folks at A-EON and Hyperion Entertainment.
>
> It is by no means the fastest PC ever made, but it is certainly the fastest Amiga ever produced. The operating system harkens back to the days when computing was more personal, less corporate, and a lot more fun...
>
> It feels like an exotic car: expensive, beautifully engineered, and unique. If you bought one, you’d be one of a proud few, a collector and enthusiast. It practically begs for you to dig in and tinker with the internals—the system comes with an SDK, a C compiler, Python, and a huge amount of documentation for things like MUI, the innovative GUI library. On top of that, there is the mysterious XMOS chip, crying out for someone to create software that leverages its strengths. It feels like a developer’s machine."

Trevor Dickinson reports that over 50 percent of people who purchased an Amiga X5000 have never owned an Amiga before. Perhaps some of them will be inspired to create something entirely new and revolutionary, in the same spirit as the tiny band of heroes who joined together to found Hi-Toro in 1982.
</pre>
		</div>
		<br><a href="https://arstechnica.com/gadgets/2007/07/a-history-of-the-amiga-part-1/" style="color: black;">https://arstechnica.com/gadgets/2007/07/a-history-of-the-amiga-part-1/</a>
		<br><br>
	</body>
</html>